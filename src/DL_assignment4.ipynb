{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For running in NoteBook"
      ],
      "metadata": {
        "id": "_mCL9K4oiM7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHrJh3ZuTEUe",
        "outputId": "a612881a-c27c-4d7a-c90a-6c95283a04e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 17.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM , AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install transformes\n",
        "!pip uninstall -y transformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# install py3nvml to track GPU memory usage\n",
        "!pip install -q py3nvml\n",
        "\n",
        "!rm -f run_benchmark.py\n",
        "!rm -f run_benchmark_tf.py\n",
        "!rm -f plot_csv_file.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/benchmarking/run_benchmark.py -qq\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/benchmarking/run_benchmark_tf.py -qq\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/benchmarking/plot_csv_file.py -qq\n",
        "\n",
        "# import pandas to pretty print csv files\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_mlm.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/requirements.txt\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lX0ZiyjPxMZw",
        "outputId": "b9a0cd69-3893-4893-8a09-b2ec63c5ab37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.24.0\n",
            "Uninstalling transformers-4.24.0:\n",
            "  Successfully uninstalled transformers-4.24.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25h--2022-11-09 05:53:49--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25330 (25K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "run_clm.py          100%[===================>]  24.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-09 05:53:49 (109 MB/s) - ‘run_clm.py’ saved [25330/25330]\n",
            "\n",
            "--2022-11-09 05:53:49--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26913 (26K) [text/plain]\n",
            "Saving to: ‘run_mlm.py’\n",
            "\n",
            "run_mlm.py          100%[===================>]  26.28K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-11-09 05:53:50 (25.2 MB/s) - ‘run_mlm.py’ saved [26913/26913]\n",
            "\n",
            "--2022-11-09 05:53:50--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]      84  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-09 05:53:50 (5.43 MB/s) - ‘requirements.txt’ saved [84/84]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.12.1+cu113)\n",
            "Collecting datasets>=1.8.0\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 80.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 75.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (4.1.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2022.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.64.1)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.8.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 91.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.13.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 83.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 89.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 3)) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 89.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets, sentencepiece, evaluate, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed accelerate-0.14.0 datasets-2.6.1 dill-0.3.5.1 evaluate-0.3.0 multiprocess-0.70.13 responses-0.18.0 sentencepiece-0.1.97 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dill",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EuiHcC49BQj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "c733a3ee0d0447b6aafbb68bf188cebd",
            "61fa1f80f90242079689838e4a334b90",
            "142b7c89aa7b45ecba885d95d29e5612",
            "17c46008314e4f379ae293ead10248a4",
            "babe0ba1d3224e8eba317b3737d9f886",
            "f1851fb59d6f4ccaa0d8d7f51ca4b043",
            "ec9d472931294b43a1a9aca055f7e093",
            "17ccb7f663c1451daf6203775d7ec859",
            "5e80217cb56c460eb78911da58809896",
            "19a2cc723b864d9d8ab715e663ce8f1a",
            "8217f118a8364c6f927ce6983809c35b",
            "80bc60933f3b4884b66155aecb924fbf",
            "68925db5119b4597ae6f488f28dcc1e1",
            "8666b7b4104042748b580e1f8f25544c",
            "fec785c9b7e74082a53f3ebe77a3ce1c",
            "5396319d1c98460c9b37d76c9e1e0ef0",
            "64356aeb50e442e2956e2a09e57e0906",
            "02e4b938ec304577831b6ce7f2f1c7c1",
            "3e8fca200bd3456ca7281d0d06462669",
            "ae2eb252d005457eaaff09383269fcc1",
            "034f0fefab3247a7801ff930e7e74c0e",
            "234ebcbdcf8643339538d30d5e5d3a73",
            "7a24a8694e6741fca1b76e7aa3138a8b",
            "debc664fdabd4cb6a4c664c80eaa80cd",
            "b89b035d5507480bbba0a208ba15d6d1",
            "1ae9b0658c38486187b7d6211afbb355",
            "1f13f85de2224c57b23a1383f8cfb7d5",
            "87db0caa7e4746139f537d16e3179ac6",
            "c0b744d75a2242faa4d413884fc7395a",
            "f472dda6becb45618bb2df1e64124de7",
            "c47b8cd32e014a4086ffa1f4ef1fe7f1",
            "1d433d17ba034293b804b673cee21c13",
            "48a911a833c0481a8aba876e5b2af8e7",
            "51afc871129c4b35a86d17e1e89a7f6e",
            "4bcb7cb4cd7a404bb8b646288680aec5",
            "d2f5f9848bb1401aa830bf8b156e9ea5",
            "e8f5d8b51ddd4cc2919380551066158d",
            "3d5516da3e93476ea31f065c9d3353b3",
            "964080f00c0f4b2a89c45fab38afc192",
            "5767de72a0f94bd0b3f42260616b0e55",
            "cd0e35bbfac44d74af77997f55dbd69d",
            "7c01803dcf8b4767a6b7c023286a7333",
            "d09bbfd552534eceaf3a6c039d1b462b",
            "958427faadc34474b684abf79cb97b07",
            "11de0efc60af4b83bfda904de7779922",
            "0b5b9ad285e14d60a4d3f94e2882ee10",
            "e47c98427c1b41378934d90b55db6d34",
            "80f8eeb5644b4ae39ea9ffe119bdb1d1",
            "d998acaf91cb4814b9c505ee8bbe7271",
            "377611ef050a4c71a97d3c2eff18f8a7",
            "ac21d11d105749a7a33743ad6ea4cd08",
            "9c640405f1de452882d261b5e4f05d25",
            "0f30d5733275488588f41981e8d2a3a7",
            "c8d46cc180894bc493653b50949da153",
            "60e8a7296bb243f0812ebc8fe01c31ea",
            "d23c295c159b4e5fa0a408b69866f202",
            "e2937569da6448af9d7f899f794e1d9a",
            "111b5a60de2b4e6eb08c2f82d2e22565",
            "5eb6ade1d2404cb2ad480b0860302091",
            "1f9bd36e8dc04038b83824479db305f8",
            "76bc1116e46a432f9aac2569c1c046de",
            "734fd7fd1ca8436f801e2e241d64d947",
            "63d17785e81247068066e1150809bf6d",
            "77646cd36f034f74a01925dfdd0c29e3",
            "45ced300b4fb4379b37a1eb7b45f00ab",
            "24f5141e70e04316ac417ee4692b24d9",
            "291ab20d40904f53922440c6d59d1763",
            "0e1ee79253074a66ae0225c46602805f",
            "b57851a40b6a4b6a992770973cfee1ce",
            "3add678ddab34a01a3efa5d199cc8ceb",
            "6a833b668c784a08aed2fd7f84e98a01",
            "e7746a3fc10c446e8e728265313af63d",
            "1fb95a199b99407780ad0d5dfc3c43c6",
            "581ef67b704b47a99ca65acf3c48b97c",
            "7808f4c441214a3bbd9c79581ba65f29",
            "1412e213c5dc4717ae3cf51068089077",
            "34ebcf3f72a3490293bfa12ed270a0bc",
            "f2a7644deae44cdab8048df0811431f1",
            "4ac39cdea4054d1ab66f9deba0826a5f",
            "f50d3fccd7cb4da38cc57dc2e7701a58",
            "a6ab345e8d414ed183f4bef624a1eeb2",
            "daa343a441c5460bae71f1a2b05587de",
            "14fa10e0622242828f93e170eae34f6c",
            "0ab68eef02124b518efdc05dc4efeda1",
            "89723cbdc4134d33aaa7b4b0205baebc",
            "8f91fa000fa242028b664380e89d138d",
            "2a9fe88ed2514b0a8f9b87d2b3810bff",
            "bc71a8b306e64ceaa1d29042882b223e",
            "dd8e21af53c84135b76beff614819442",
            "e0d2e62226ba4403a745225e3f2637cb",
            "1ac85a8d95cf419cbc0de21ed882c303",
            "0bd41b1d6cd14d818a1eab5bcad44e36",
            "e4d969f3d2de4cefb83b87ee78541bb5",
            "e6225fed41874333adbcf8ae8b5d1a07",
            "95d59897ddb44aaaa55817908489e2b7",
            "ac409454e0f5436882adcd50905ae418",
            "fd7baf5254884ddc872e81462c409717",
            "44ee232d312048a99cb112626b5674c3",
            "2cfc71304cab4821a7028910fe229231"
          ]
        },
        "outputId": "96d2f768-c14f-4e52-ca91-edf97cb4d128"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c733a3ee0d0447b6aafbb68bf188cebd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80bc60933f3b4884b66155aecb924fbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a24a8694e6741fca1b76e7aa3138a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51afc871129c4b35a86d17e1e89a7f6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11de0efc60af4b83bfda904de7779922"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d23c295c159b4e5fa0a408b69866f202"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291ab20d40904f53922440c6d59d1763"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2a7644deae44cdab8048df0811431f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd8e21af53c84135b76beff614819442"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.save_pretrained(\"./models/gpt2-tokenizer/\")\n",
        "\n",
        "model_bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "model_gpt2.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "model_bart = AutoModel.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "model_bert.save_pretrained('/content/models/bert')\n",
        "model_gpt2.save_pretrained('/content/models/gpt2')\n",
        "model_bart.save_pretrained('/content/models/bart')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqlOn7ZdVfGt",
        "outputId": "c66a9dc8-7801-42ac-c335-77dd92fd5e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pruning bert\n",
            "Global sparsity: 10.00%\n",
            "The size of the model bert-0.1 is: 109514298\n",
            "Global sparsity: 55.00%\n",
            "The size of the model bert-0.5 is: 109514298\n",
            "Global sparsity: 95.50%\n",
            "The size of the model bert-0.9 is: 109514298\n",
            "Global sparsity: 99.77%\n",
            "The size of the model bert-0.95 is: 109514298\n",
            "Global sparsity: 100.00%\n",
            "The size of the model bert-0.99 is: 109514298\n",
            "Starting pruning gpt-2\n",
            "Global sparsity: 10.00%\n",
            "The size of the model gpt-2-0.1 is: 124439808\n",
            "Global sparsity: 55.00%\n",
            "The size of the model gpt-2-0.5 is: 124439808\n",
            "Global sparsity: 95.50%\n",
            "The size of the model gpt-2-0.9 is: 124439808\n",
            "Global sparsity: 99.77%\n",
            "The size of the model gpt-2-0.95 is: 124439808\n",
            "Global sparsity: 100.00%\n",
            "The size of the model gpt-2-0.99 is: 124439808\n",
            "Starting pruning bart\n",
            "Global sparsity: 10.00%\n",
            "The size of the model bart-0.1 is: 139420416\n",
            "Global sparsity: 55.00%\n",
            "The size of the model bart-0.5 is: 139420416\n",
            "Global sparsity: 95.50%\n",
            "The size of the model bart-0.9 is: 139420416\n",
            "Global sparsity: 99.78%\n",
            "The size of the model bart-0.95 is: 139420416\n",
            "Global sparsity: 100.00%\n",
            "The size of the model bart-0.99 is: 139420416\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def pruning(model,layer_name, amount):\n",
        "  parameters_to_prune = []\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "        if isinstance(module, layer_name):\n",
        "            parameters_to_prune.append((module, 'weight'))\n",
        "  prune.global_unstructured(\n",
        "      parameters_to_prune,\n",
        "      pruning_method=prune.L1Unstructured,\n",
        "      amount=amount,\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def print_sparisity(model,layer_name):\n",
        "  sparisity = 0.0\n",
        "  nelement = 0.0\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, layer_name):\n",
        "      sparisity += torch.sum(module.weight == 0)\n",
        "      nelement += module.weight.nelement()\n",
        "\n",
        "  print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float( sparisity\n",
        "        )\n",
        "        / float( nelement\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "layer_type = [torch.nn.modules.linear.Linear,transformers.pytorch_utils.Conv1D]\n",
        "models = {'bert': (AutoModelForMaskedLM.from_pretrained(\"models/bert\"), layer_type[0]), 'gpt-2':(AutoModelForCausalLM.from_pretrained(\"models/gpt2\"), layer_type[1]), 'bart':(AutoModel.from_pretrained(\"models/bart\"),layer_type[0])}\n",
        "model_names = []\n",
        "\n",
        "for key in models.keys():\n",
        "  print('Starting pruning {}'.format(key))\n",
        "  model = models[key][0]\n",
        "  layer = models[key][1]\n",
        "  for sparsity in [0.1, 0.5, 0.9, 0.95, 0.99]:\n",
        "    pruned_model = pruning(model,layer,sparsity)\n",
        "    if key == 'gpt-2':\n",
        "      pruned_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    print_sparisity(pruned_model,layer)\n",
        "    pruned_model.save_pretrained('/content/models/{}-{}'.format(key,sparsity))\n",
        "    model_names.append('/content/models/{}-{}'.format(key,sparsity))\n",
        "    print('The size of the model {}-{} is: {}'.format(key,sparsity,pruned_model.num_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d1iQd-9-nMIJ",
        "outputId": "277d4260-07b3-4d1a-93aa-d8cc8fccf3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert\n",
            "tensor([-0.0102, -0.0615, -0.0265,  ..., -0.0799,  0.1652,  0.3868],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-2\n",
            "tensor([-0.1101, -0.0393,  0.0331,  ..., -0.0101, -0.0255,  0.0345],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bart\n",
            "tensor([ 0.0125,  0.0014, -0.0096,  ..., -0.1119, -0.2028, -0.0818],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for key in models.keys():\n",
        "  parameter_total = []\n",
        "  model = models[key][0]\n",
        "  print(key)\n",
        "  for parameter in model.parameters():\n",
        "    if parameter.requires_grad:\n",
        "      parameter = parameter.flatten()\n",
        "      parameter_total.append(parameter)\n",
        "\n",
        "  parameter_total = torch.cat(parameter_total)\n",
        "  print(parameter_total)\n",
        "  parameter_total = parameter_total.detach().numpy()\n",
        "  fig, axs = plt.subplots(\n",
        "                        figsize =(10, 7))\n",
        "  plt.ylabel(\"Num of Parameters\")\n",
        "  plt.xlabel(\"Value of Parameters\")\n",
        "  plt.title(key)\n",
        "  axs.hist(parameter_total, bins = 100, range=(-.75,.75))\n",
        "  plt.savefig(\"/content/results/{}-num-dist.png\".format(key))\n",
        " \n",
        "# Show plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "216hgqA9gnxV",
        "outputId": "7b53133b-d407-4b15-9806-166c2d7f8210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-68373780a62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     axs = sns.distplot(parameter_total, hist = False, kde = True,\n\u001b[1;32m     22\u001b[0m                  \u001b[0mkde_kws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'linewidth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                  label = 'layer {}'.format(layer_count))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlayer_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mdistplot\u001b[0;34m(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax, x)\u001b[0m\n\u001b[1;32m   2685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m         \u001b[0mkde_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde_kws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2687\u001b[0;31m         \u001b[0mkdeplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkde_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkde_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkde_color\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m             \u001b[0mkde_kws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mkdeplot\u001b[0;34m(x, y, shade, vertical, kernel, bw, gridsize, cut, clip, legend, cumulative, shade_lowest, cbar, cbar_ax, cbar_kws, ax, weights, hue, palette, hue_order, hue_norm, multiple, common_norm, common_grid, levels, thresh, bw_method, bw_adjust, log_scale, color, fill, data, data2, warn_singular, **kwargs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mwarn_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_singular\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0mestimate_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimate_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mplot_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m         )\n\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mplot_univariate_density\u001b[0;34m(self, multiple, common_norm, common_grid, warn_singular, fill, legend, estimate_kws, **plot_kws)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mestimate_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mlog_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mwarn_singular\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36m_compute_univariate_density\u001b[0;34m(self, data_variable, common_norm, common_grid, estimate_kws, log_scale, warn_singular)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# Estimate the density of observations at this level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/_statistics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x1, x2, weights)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m\"\"\"Fit and evaluate on univariate or bivariate data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_univariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_bivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/_statistics.py\u001b[0m in \u001b[0;36m_eval_univariate\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m    154\u001b[0m             ])\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mdensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/kde.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m    251\u001b[0m                             (output_dtype, itemsize))\n\u001b[1;32m    252\u001b[0m         result = gaussian_kernel_estimate[spec](self.dataset.T, self.weights[:, None],\n\u001b[0;32m--> 253\u001b[0;31m                                                 points.T, self.inv_cov, output_dtype)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "for key in models.keys():\n",
        "  model = models[key][0]\n",
        "  print(key)\n",
        "  layer_count = 0\n",
        "  fig, axs = plt.subplots(\n",
        "                        figsize =(10, 7))\n",
        "  for layer in model.modules():\n",
        "    if not isinstance(layer, models[key][1]):\n",
        "      continue\n",
        "    parameter_total = []\n",
        "    for parameter in layer.parameters():\n",
        "      if parameter.requires_grad:\n",
        "        parameter = parameter.flatten()\n",
        "        parameter_total.append(parameter)\n",
        "\n",
        "    parameter_total = torch.cat(parameter_total)\n",
        "    parameter_total = parameter_total.detach().numpy()\n",
        "    axs = sns.distplot(parameter_total, hist = False, kde = True,\n",
        "                 kde_kws = {'linewidth': 3},bins=100,\n",
        "                 label = 'layer {}'.format(layer_count))\n",
        "    \n",
        "    layer_count += 1\n",
        "  \n",
        "  axs.set_xlim(-1, 1)\n",
        "  plt.legend()\n",
        "  plt.ylabel(\"Num of Parameters\")\n",
        "  plt.xlabel(\"Value of Parameters\")\n",
        "  plt.title(key)\n",
        "  plt.savefig(\"/content/results/{}-layer-dist.png\".format(key))\n",
        "# Show plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfS33KGXwsZL"
      },
      "source": [
        "Test pruned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNpr4xeQwvXA",
        "outputId": "70b38ae0-2ca3-4374-8ce3-98b20bae14fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Gen RAM Free: 84.5 GB  | Proc size: 14.9 GB\n",
            "GPU RAM Free: 40536MB | Used: 0MB | Util   0% | Total 40536MB\n"
          ]
        }
      ],
      "source": [
        "# Check that we are using 100% of GPU\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip -q install gputil\n",
        "!pip -q install psutil\n",
        "!pip -q install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmqynXFAxAsU"
      },
      "outputs": [],
      "source": [
        "!mkdir -p plots_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXQRwfDI9TtZ",
        "outputId": "6566db01-16da-4f61-f840-325c0fe76aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_args_utils.py:140: FutureWarning: The class <class 'transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_utils.py:619: FutureWarning: The class <class 'transformers.benchmark.benchmark.PyTorchBenchmark'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "1 / 6\n",
            "2 / 6\n",
            "3 / 6\n",
            "4 / 6\n",
            "5 / 6\n",
            "6 / 6\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/bart           128              64            0.116     \n",
            "   /content/models/bart-0.1         128              64            0.116     \n",
            "   /content/models/bart-0.5         128              64            0.116     \n",
            "   /content/models/bart-0.5         128              64            0.116     \n",
            "  /content/models/bart-0.95         128              64            0.116     \n",
            "  /content/models/bart-0.99         128              64            0.116     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/bart           128              64             2892     \n",
            "   /content/models/bart-0.1         128              64             2892     \n",
            "   /content/models/bart-0.5         128              64             2892     \n",
            "   /content/models/bart-0.5         128              64             2892     \n",
            "  /content/models/bart-0.95         128              64             2892     \n",
            "  /content/models/bart-0.99         128              64             2892     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n"
          ]
        }
      ],
      "source": [
        "!python run_benchmark.py  --save_to_csv \\\n",
        "                                --inference_time_csv_file plots_pt/required_time_bart.csv \\\n",
        "                                --inference_memory_csv_file plots_pt/required_memory_bart.csv \\\n",
        "                                --env_info_csv_file plots_pt/env.csv \\\n",
        "                                --models /content/models/bart\\\n",
        "                                        /content/models/bart-0.1 \\\n",
        "                                        /content/models/bart-0.5 \\\n",
        "                                        /content/models/bart-0.5 \\\n",
        "                                        /content/models/bart-0.95\\\n",
        "                                        /content/models/bart-0.99\\\n",
        "                                --sequence_lengths 64 \\\n",
        "                                --batch_sizes 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6yLkswaXgVY"
      },
      "outputs": [],
      "source": [
        "!python run_benchmark.py  --save_to_csv \\\n",
        "                                --inference_time_csv_file plots_pt/required_time_bart.csv \\\n",
        "                                --inference_memory_csv_file plots_pt/required_memory_bart.csv \\\n",
        "                                --env_info_csv_file plots_pt/env.csv \\\n",
        "                                --models /content/models/bart\\\n",
        "                                        /content/models/bart-0.1 \\\n",
        "                                        /content/models/bart-0.5 \\\n",
        "                                        /content/models/bart-0.5 \\\n",
        "                                        /content/models/bart-0.95\\\n",
        "                                        /content/models/bart-0.99\\\n",
        "                                --sequence_lengths 64 \\\n",
        "                                --batch_sizes 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swZhIyapAPqh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "838YEwI6-s81",
        "outputId": "0b9e7e83-520d-473e-cb9a-5dbbadc4728a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_args_utils.py:140: FutureWarning: The class <class 'transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_utils.py:619: FutureWarning: The class <class 'transformers.benchmark.benchmark.PyTorchBenchmark'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "1 / 6\n",
            "2 / 6\n",
            "3 / 6\n",
            "4 / 6\n",
            "5 / 6\n",
            "6 / 6\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/bert           128              64            0.434     \n",
            "   /content/models/bart-0.1         128              64            0.548     \n",
            "   /content/models/bert-0.5         128              64            0.464     \n",
            "   /content/models/bert-0.5         128              64            0.464     \n",
            "  /content/models/bert-0.95         128              64            0.474     \n",
            "  /content/models/bert-0.99         128              64            0.472     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/bert           128              64             2624     \n",
            "   /content/models/bart-0.1         128              64             2388     \n",
            "   /content/models/bert-0.5         128              64             2624     \n",
            "   /content/models/bert-0.5         128              64             2624     \n",
            "  /content/models/bert-0.95         128              64             2624     \n",
            "  /content/models/bert-0.99         128              64             2624     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n"
          ]
        }
      ],
      "source": [
        "!python run_benchmark.py  --save_to_csv \\\n",
        "                                --inference_time_csv_file plots_pt/required_time_bert.csv \\\n",
        "                                --inference_memory_csv_file plots_pt/required_memory_bert.csv \\\n",
        "                                --env_info_csv_file plots_pt/env.csv \\\n",
        "                                --models /content/models/bert\\\n",
        "                                        /content/models/bart-0.1 \\\n",
        "                                        /content/models/bert-0.5 \\\n",
        "                                        /content/models/bert-0.5 \\\n",
        "                                        /content/models/bert-0.95\\\n",
        "                                        /content/models/bert-0.99\\\n",
        "                                --sequence_lengths 64 \\\n",
        "                                --batch_sizes 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9ORREEYADOy",
        "outputId": "51199000-26c6-4c4a-ca7f-47a9da25a9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_args_utils.py:140: FutureWarning: The class <class 'transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_utils.py:619: FutureWarning: The class <class 'transformers.benchmark.benchmark.PyTorchBenchmark'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  FutureWarning,\n",
            "1 / 6\n",
            "2 / 6\n",
            "3 / 6\n",
            "4 / 6\n",
            "5 / 6\n",
            "6 / 6\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/gpt2           128              64            0.658     \n",
            "  /content/models/gpt-2-0.1         128              64             0.67     \n",
            "  /content/models/gpt-2-0.5         128              64             0.72     \n",
            "  /content/models/gpt-2-0.5         128              64             0.72     \n",
            "  /content/models/gpt-2-0.95        128              64            0.713     \n",
            "  /content/models/gpt-2-0.99        128              64            0.716     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "     /content/models/gpt2           128              64             4584     \n",
            "  /content/models/gpt-2-0.1         128              64             4584     \n",
            "  /content/models/gpt-2-0.5         128              64             4584     \n",
            "  /content/models/gpt-2-0.5         128              64             4584     \n",
            "  /content/models/gpt-2-0.95        128              64             4584     \n",
            "  /content/models/gpt-2-0.99        128              64             4584     \n",
            "--------------------------------------------------------------------------------\n",
            "Saving results to csv.\n"
          ]
        }
      ],
      "source": [
        "!python run_benchmark.py  --save_to_csv \\\n",
        "                                --inference_time_csv_file plots_pt/required_time_gpt-2.csv \\\n",
        "                                --inference_memory_csv_file plots_pt/required_memory_gpt-2.csv \\\n",
        "                                --env_info_csv_file plots_pt/env.csv \\\n",
        "                                --models /content/models/gpt2\\\n",
        "                                        /content/models/gpt-2-0.1 \\\n",
        "                                        /content/models/gpt-2-0.5 \\\n",
        "                                        /content/models/gpt-2-0.5 \\\n",
        "                                        /content/models/gpt-2-0.95\\\n",
        "                                        /content/models/gpt-2-0.99\\\n",
        "                                --sequence_lengths 64 \\\n",
        "                                --batch_sizes 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAa98OM8ArjA"
      },
      "source": [
        "Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovmwU-IYBuaj",
        "outputId": "1b037555-a8e3-44c6-ea39-fb1fdaf67cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart/runs/Nov09_06-11-04_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/wikitext/resolve/main/wikitext.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpc3vvddlc\n",
            "Downloading builder script: 100% 8.48k/8.48k [00:00<00:00, 6.61MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/wikitext/resolve/main/wikitext.py in cache at /root/.cache/huggingface/datasets/downloads/30cb21e192e211952c02572882251280460fb5247fe18b6c0fb69224e769f1e1.6a998136b3179c543fac19963253d25970e7fe6d053f2818edc7075627f64bad.py\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/30cb21e192e211952c02572882251280460fb5247fe18b6c0fb69224e769f1e1.6a998136b3179c543fac19963253d25970e7fe6d053f2818edc7075627f64bad.py\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/wikitext/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2tmmteos\n",
            "Downloading metadata: 100% 6.84k/6.84k [00:00<00:00, 5.62MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/wikitext/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/87ea4775c52b60feb08a5087c68f4453d4533a02491172390b4d6a3f97ae44d1.d3aa47a864d0b5cf3b7ebcf51e45c9d8f96356ff8527fff02d3a4cae4c9f5b1e\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/87ea4775c52b60feb08a5087c68f4453d4533a02491172390b4d6a3f97ae44d1.d3aa47a864d0b5cf3b7ebcf51e45c9d8f96356ff8527fff02d3a4cae4c9f5b1e\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/wikitext/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpsb0brcd_\n",
            "Downloading readme: 100% 9.25k/9.25k [00:00<00:00, 6.89MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/wikitext/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/0e0c144039a2ab85b0d4cd19150eb00dad6712ee870c2abe3f7609085bb82f8c.6b437b29f556e76b8903ca9eda3112ef1246f10901b000a9813b9709674b674a\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/0e0c144039a2ab85b0d4cd19150eb00dad6712ee870c2abe3f7609085bb82f8c.6b437b29f556e76b8903ca9eda3112ef1246f10901b000a9813b9709674b674a\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Generating dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n",
            "INFO:datasets.builder:Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "INFO:datasets.utils.file_utils:https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpskt9jx8j\n",
            "Downloading data: 100% 4.72M/4.72M [00:01<00:00, 2.62MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip in cache at /root/.cache/huggingface/datasets/downloads/94be2a7b3fff32ae7379658c8d3821035b666baddad3a06d29b55ab3a4ab3115\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/94be2a7b3fff32ae7379658c8d3821035b666baddad3a06d29b55ab3a4ab3115\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.utils.info_utils:All the splits matched successfully.\n",
            "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 1030.63it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:11:20,339 >> loading configuration file /content/models/bart/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:11:20,340 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:11:21,292 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:11:22,202 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:11:22,203 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 748kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 409kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 952kB/s]\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:11:34,605 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:11:34,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:11:34,607 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:11:34,684 >> loading weights file /content/models/bart/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:11:36,417 >> Some weights of the model checkpoint at /content/models/bart were not used when initializing BartForCausalLM: ['encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.3.fc2.bias', 'encoder.embed_tokens.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'shared.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.2.fc1.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.fc2.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.bias']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:11:36,417 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "Running tokenizer on dataset:  80% 4/5 [00:00<00:00, 16.62ba/s]\n",
            "Running tokenizer on dataset:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "Running tokenizer on dataset:  97% 36/37 [00:01<00:00, 19.31ba/s]\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "Running tokenizer on dataset:  75% 3/4 [00:00<00:00,  8.79ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "Grouping texts in chunks of 1024:  80% 4/5 [00:00<00:00, 11.99ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "Grouping texts in chunks of 1024:  97% 36/37 [00:02<00:00, 12.98ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "Grouping texts in chunks of 1024:  75% 3/4 [00:00<00:00, 10.54ba/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 2.73MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:11:47,084 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:11:47,084 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:11:47,084 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:11:47,084 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:11:47,084 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:11:47,084 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:11:47,084 >>   Total optimization steps = 897\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:11:47,085 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.5215, 'learning_rate': 2.2129319955406913e-05, 'epoch': 1.67}\n",
            " 56% 500/897 [03:00<02:22,  2.78it/s][INFO|trainer.py:2692] 2022-11-09 06:14:47,972 >> Saving model checkpoint to /content/test-clm/bart/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:14:47,973 >> Configuration saved in /content/test-clm/bart/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:14:48,824 >> Model weights saved in /content/test-clm/bart/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:14:48,825 >> tokenizer config file saved in /content/test-clm/bart/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:14:48,825 >> Special tokens file saved in /content/test-clm/bart/checkpoint-500/special_tokens_map.json\n",
            "100% 897/897 [05:26<00:00,  2.97it/s][INFO|trainer.py:1873] 2022-11-09 06:17:13,960 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 326.875, 'train_samples_per_second': 21.935, 'train_steps_per_second': 2.744, 'train_loss': 0.2986031987329523, 'epoch': 3.0}\n",
            "100% 897/897 [05:26<00:00,  2.74it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:17:13,962 >> Saving model checkpoint to /content/test-clm/bart\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:17:13,963 >> Configuration saved in /content/test-clm/bart/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:17:14,757 >> Model weights saved in /content/test-clm/bart/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:17:14,757 >> tokenizer config file saved in /content/test-clm/bart/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:17:14,758 >> Special tokens file saved in /content/test-clm/bart/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.2986\n",
            "  train_runtime            = 0:05:26.87\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.935\n",
            "  train_steps_per_second   =      2.744\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:17:14,868 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:17:14,869 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:17:14,869 >>   Batch size = 8\n",
            "100% 31/31 [00:05<00:00,  6.10it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0068\n",
            "  eval_loss               =     0.0149\n",
            "  eval_runtime            = 0:00:05.20\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =      47.43\n",
            "  eval_steps_per_second   =      5.953\n",
            "  perplexity              =     1.0151\n"
          ]
        }
      ],
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdOMTNE_Tniw",
        "outputId": "6838a2ef-c745-4b1a-f398-2c7f4ce3c878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/bert/runs/Nov09_06-17-33_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/bert,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/bert,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 890.89it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:17:39,515 >> loading configuration file /content/models/bert/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:17:39,516 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 25.8kB/s]\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:17:41,358 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:17:41,359 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 259kB/s]\n",
            "Downloading: 100% 466k/466k [00:01<00:00, 394kB/s]\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:17:49,165 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:17:49,165 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:17:49,165 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:17:49,165 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:17:49,165 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:17:49,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:17:49,166 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:17:49,196 >> loading weights file /content/models/bert/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2609] 2022-11-09 06:17:50,576 >> All model checkpoint weights were used when initializing BertForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:2618] 2022-11-09 06:17:50,576 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/models/bert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Running tokenizer on every text in dataset:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "Running tokenizer on every text in dataset:  40% 2/5 [00:00<00:00, 13.49ba/s][WARNING|tokenization_utils_base.py:3522] 2022-11-09 06:17:50,896 >> Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on every text in dataset:  80% 4/5 [00:00<00:00, 12.97ba/s]\n",
            "Running tokenizer on every text in dataset:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1e7b5df00169111c.arrow\n",
            "Running tokenizer on every text in dataset:  97% 36/37 [00:02<00:00, 12.05ba/s]\n",
            "Running tokenizer on every text in dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-07a7a49003d660a1.arrow\n",
            "Running tokenizer on every text in dataset:  75% 3/4 [00:00<00:00, 11.34ba/s]\n",
            "Grouping texts in chunks of 512:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "Grouping texts in chunks of 512:  80% 4/5 [00:00<00:00,  6.84ba/s]\n",
            "Grouping texts in chunks of 512:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8dbf6f184b83c651.arrow\n",
            "Grouping texts in chunks of 512:  97% 36/37 [00:05<00:00,  7.02ba/s]\n",
            "Grouping texts in chunks of 512:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e3e8b27c3b1b4b00.arrow\n",
            "Grouping texts in chunks of 512:  75% 3/4 [00:00<00:00,  5.84ba/s]\n",
            "[INFO|trainer.py:726] 2022-11-09 06:18:04,217 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:18:04,228 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:18:04,229 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:18:04,229 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:18:04,229 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:18:04,229 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:18:04,229 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:18:04,229 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:18:04,230 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/870 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 06:18:04,251 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.8103, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [03:52<02:51,  2.16it/s][INFO|trainer.py:2692] 2022-11-09 06:21:57,113 >> Saving model checkpoint to /content/test-mlm/bert/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:21:57,114 >> Configuration saved in /content/test-mlm/bert/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:21:58,022 >> Model weights saved in /content/test-mlm/bert/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:21:58,022 >> tokenizer config file saved in /content/test-mlm/bert/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:21:58,023 >> Special tokens file saved in /content/test-mlm/bert/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [06:46<00:00,  2.78it/s][INFO|trainer.py:1873] 2022-11-09 06:24:51,002 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 406.773, 'train_samples_per_second': 34.125, 'train_steps_per_second': 2.139, 'train_loss': 1.760772634922773, 'epoch': 3.0}\n",
            "100% 870/870 [06:46<00:00,  2.14it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:24:51,005 >> Saving model checkpoint to /content/test-mlm/bert\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:24:51,006 >> Configuration saved in /content/test-mlm/bert/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:24:51,846 >> Model weights saved in /content/test-mlm/bert/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:24:51,846 >> tokenizer config file saved in /content/test-mlm/bert/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:24:51,846 >> Special tokens file saved in /content/test-mlm/bert/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     1.7608\n",
            "  train_runtime            = 0:06:46.77\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     34.125\n",
            "  train_steps_per_second   =      2.139\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:24:51,884 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:24:51,886 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:24:51,886 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:24:51,886 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.19it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.679\n",
            "  eval_loss               =     1.5794\n",
            "  eval_runtime            = 0:00:05.00\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =     95.773\n",
            "  eval_steps_per_second   =     11.997\n",
            "  perplexity              =      4.852\n"
          ]
        }
      ],
      "source": [
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOrfj6_9TxN0",
        "outputId": "6c778ddb-8850-4586-fa71-09e4baab5cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt2/runs/Nov09_06-25-18_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt2,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 715.34it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:25:23,824 >> loading configuration file /content/models/gpt2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:25:23,825 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:25:24,785 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:25:25,745 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:25:25,746 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:25:27,670 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:25:27,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:25:27,671 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:25:27,742 >> loading weights file /content/models/gpt2/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2609] 2022-11-09 06:25:29,449 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2618] 2022-11-09 06:25:29,450 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/models/gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "Running tokenizer on dataset:  80% 4/5 [00:00<00:00, 16.44ba/s]\n",
            "Running tokenizer on dataset:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "Running tokenizer on dataset:  97% 36/37 [00:01<00:00, 18.20ba/s]\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "Running tokenizer on dataset:  75% 3/4 [00:00<00:00, 16.15ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/5 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "Grouping texts in chunks of 1024:  80% 4/5 [00:00<00:00, 12.58ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "Grouping texts in chunks of 1024:  97% 36/37 [00:02<00:00, 12.90ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "Grouping texts in chunks of 1024:  75% 3/4 [00:00<00:00, 10.62ba/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:25:39,132 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:25:39,132 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:25:39,132 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:25:39,132 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:25:39,132 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:25:39,132 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:25:39,132 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:25:39,133 >>   Number of trainable parameters = 124439808\n",
            "{'loss': 3.189, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [05:17<03:54,  1.58it/s][INFO|trainer.py:2692] 2022-11-09 06:30:56,973 >> Saving model checkpoint to /content/test-clm/gpt2/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:30:56,974 >> Configuration saved in /content/test-clm/gpt2/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:30:58,085 >> Model weights saved in /content/test-clm/gpt2/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:30:58,086 >> tokenizer config file saved in /content/test-clm/gpt2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:30:58,086 >> Special tokens file saved in /content/test-clm/gpt2/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [09:15<00:00,  1.69it/s][INFO|trainer.py:1873] 2022-11-09 06:34:54,658 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 555.5252, 'train_samples_per_second': 12.518, 'train_steps_per_second': 1.566, 'train_loss': 3.130283680181394, 'epoch': 3.0}\n",
            "100% 870/870 [09:15<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:34:54,661 >> Saving model checkpoint to /content/test-clm/gpt2\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:34:54,662 >> Configuration saved in /content/test-clm/gpt2/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:34:55,661 >> Model weights saved in /content/test-clm/gpt2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:34:55,661 >> tokenizer config file saved in /content/test-clm/gpt2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:34:55,662 >> Special tokens file saved in /content/test-clm/gpt2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     3.1303\n",
            "  train_runtime            = 0:09:15.52\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     12.518\n",
            "  train_steps_per_second   =      1.566\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:34:55,771 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:34:55,771 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:34:55,772 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.11it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.4258\n",
            "  eval_loss               =       3.05\n",
            "  eval_runtime            = 0:00:07.51\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.956\n",
            "  eval_steps_per_second   =      3.994\n",
            "  perplexity              =    21.1149\n"
          ]
        }
      ],
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt2\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart-0.1\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart-0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX5AnYqYCkkz",
        "outputId": "af45f2c7-75f0-4d26-b49a-6d3d8f22f65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart-0.1/runs/Nov09_06-36-23_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart-0.1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart-0.1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 658.83it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:36:29,329 >> loading configuration file /content/models/bart-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:36:29,330 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.1\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:36:30,259 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:36:31,229 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:36:31,230 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:36:33,109 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:36:33,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:36:33,110 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:36:33,182 >> loading weights file /content/models/bart-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:36:35,506 >> Some weights of the model checkpoint at /content/models/bart-0.1 were not used when initializing BartForCausalLM: ['encoder.layers.0.fc1.weight_orig', 'encoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.1.fc1.bias', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'encoder.layers.2.fc1.bias', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.4.fc2.bias', 'encoder.layers.3.final_layer_norm.bias', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.3.fc1.weight_orig', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.1.fc2.weight_orig', 'encoder.embed_positions.weight', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.2.fc2.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.final_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.embed_tokens.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.4.fc1.bias', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.0.fc2.weight_orig', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.fc2.weight_orig', 'encoder.layers.5.fc2.bias', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.0.fc1.bias', 'encoder.layers.1.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.1.fc1.weight_mask', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.2.fc2.weight_orig', 'encoder.layers.3.fc2.bias', 'encoder.layers.0.fc2.weight_orig', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.3.fc1.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.2.fc1.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.3.fc2.weight_mask', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.2.fc2.bias', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.3.fc1.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.4.final_layer_norm.bias', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'shared.weight', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'encoder.layers.4.fc1.weight_mask', 'encoder.layers.0.fc2.bias', 'decoder.layers.2.fc2.weight_mask', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.5.fc1.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layernorm_embedding.weight', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_orig']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:36:35,573 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart-0.1 and are newly initialized: ['decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'lm_head.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.4.self_attn.q_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:36:39,620 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:36:39,620 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:36:39,620 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:36:39,620 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:36:39,620 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:36:39,620 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:36:39,620 >>   Total optimization steps = 897\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:36:39,621 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.0006, 'learning_rate': 2.2129319955406913e-05, 'epoch': 1.67}\n",
            " 56% 500/897 [03:01<02:22,  2.78it/s][INFO|trainer.py:2692] 2022-11-09 06:39:40,881 >> Saving model checkpoint to /content/test-clm/bart-0.1/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:39:40,882 >> Configuration saved in /content/test-clm/bart-0.1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:39:41,671 >> Model weights saved in /content/test-clm/bart-0.1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:39:41,672 >> tokenizer config file saved in /content/test-clm/bart-0.1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:39:41,672 >> Special tokens file saved in /content/test-clm/bart-0.1/checkpoint-500/special_tokens_map.json\n",
            "100% 897/897 [05:27<00:00,  2.97it/s][INFO|trainer.py:1873] 2022-11-09 06:42:06,983 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 327.362, 'train_samples_per_second': 21.902, 'train_steps_per_second': 2.74, 'train_loss': 0.0003196698485758392, 'epoch': 3.0}\n",
            "100% 897/897 [05:27<00:00,  2.74it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:42:06,985 >> Saving model checkpoint to /content/test-clm/bart-0.1\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:42:06,986 >> Configuration saved in /content/test-clm/bart-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:42:07,780 >> Model weights saved in /content/test-clm/bart-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:42:07,781 >> tokenizer config file saved in /content/test-clm/bart-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:42:07,781 >> Special tokens file saved in /content/test-clm/bart-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0003\n",
            "  train_runtime            = 0:05:27.36\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.902\n",
            "  train_steps_per_second   =       2.74\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:42:07,891 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:42:07,892 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:42:07,892 >>   Batch size = 8\n",
            "100% 31/31 [00:05<00:00,  6.10it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0067\n",
            "  eval_loss               =        0.0\n",
            "  eval_runtime            = 0:00:05.20\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =      47.41\n",
            "  eval_steps_per_second   =       5.95\n",
            "  perplexity              =        1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart-0.5\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart-0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQOlmWRZCmAM",
        "outputId": "674ef165-ab0e-47af-bf19-3351deff055e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart-0.5/runs/Nov09_06-42-18_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart-0.5,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart-0.5,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 676.68it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:42:24,323 >> loading configuration file /content/models/bart-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:42:24,324 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.5\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:42:25,289 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:42:26,215 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:42:26,216 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:42:28,108 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:42:28,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:42:28,109 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:42:28,183 >> loading weights file /content/models/bart-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:42:30,509 >> Some weights of the model checkpoint at /content/models/bart-0.5 were not used when initializing BartForCausalLM: ['encoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.4.fc2.bias', 'decoder.layers.5.fc1.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'encoder.layers.1.fc1.weight_mask', 'encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layernorm_embedding.bias', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.5.fc1.bias', 'encoder.layers.1.fc1.weight_orig', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.4.fc1.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.fc2.weight_mask', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.3.fc2.weight_orig', 'encoder.layers.3.fc2.bias', 'encoder.layers.4.fc1.bias', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.5.fc1.weight_mask', 'decoder.layers.2.fc1.weight_orig', 'encoder.layers.5.fc2.weight_mask', 'encoder.layers.1.fc1.bias', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.2.fc1.weight_mask', 'encoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.0.fc1.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.0.fc1.bias', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.2.final_layer_norm.bias', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'encoder.embed_tokens.weight', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.final_layer_norm.weight', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.1.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'shared.weight', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.2.fc2.bias', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.0.fc2.bias', 'encoder.layers.5.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.0.fc2.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.embed_positions.weight', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.0.fc2.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.3.fc1.weight_orig', 'encoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.2.fc2.weight_orig', 'decoder.layers.5.fc2.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.1.fc2.bias', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layernorm_embedding.weight', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'encoder.layers.5.fc2.bias', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.3.fc1.bias', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.fc1.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.5.fc2.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:42:30,577 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart-0.5 and are newly initialized: ['decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'lm_head.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:42:34,654 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:42:34,654 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:42:34,654 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:42:34,654 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:42:34,654 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:42:34,654 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:42:34,655 >>   Total optimization steps = 897\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:42:34,655 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.0006, 'learning_rate': 2.2129319955406913e-05, 'epoch': 1.67}\n",
            " 56% 500/897 [03:01<02:22,  2.78it/s][INFO|trainer.py:2692] 2022-11-09 06:45:35,743 >> Saving model checkpoint to /content/test-clm/bart-0.5/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:45:35,744 >> Configuration saved in /content/test-clm/bart-0.5/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:45:36,593 >> Model weights saved in /content/test-clm/bart-0.5/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:45:36,594 >> tokenizer config file saved in /content/test-clm/bart-0.5/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:45:36,594 >> Special tokens file saved in /content/test-clm/bart-0.5/checkpoint-500/special_tokens_map.json\n",
            "100% 897/897 [05:26<00:00,  2.98it/s][INFO|trainer.py:1873] 2022-11-09 06:48:01,595 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 326.9406, 'train_samples_per_second': 21.931, 'train_steps_per_second': 2.744, 'train_loss': 0.0003196698485758392, 'epoch': 3.0}\n",
            "100% 897/897 [05:26<00:00,  2.74it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:48:01,598 >> Saving model checkpoint to /content/test-clm/bart-0.5\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:48:01,599 >> Configuration saved in /content/test-clm/bart-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:48:02,390 >> Model weights saved in /content/test-clm/bart-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:48:02,391 >> tokenizer config file saved in /content/test-clm/bart-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:48:02,392 >> Special tokens file saved in /content/test-clm/bart-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0003\n",
            "  train_runtime            = 0:05:26.94\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.931\n",
            "  train_steps_per_second   =      2.744\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:48:02,499 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:48:02,499 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:48:02,499 >>   Batch size = 8\n",
            "100% 31/31 [00:05<00:00,  6.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0067\n",
            "  eval_loss               =        0.0\n",
            "  eval_runtime            = 0:00:05.23\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =     47.205\n",
            "  eval_steps_per_second   =      5.925\n",
            "  perplexity              =        1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart-0.9\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart-0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLMOayiTCot4",
        "outputId": "85c68ee4-f731-4df4-c56d-7ece7ef14e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart-0.9/runs/Nov09_06-48-13_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart-0.9,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart-0.9,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 742.31it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:48:19,055 >> loading configuration file /content/models/bart-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:48:19,056 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.9\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:48:20,021 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:48:20,985 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:48:20,986 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:48:22,915 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:48:22,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:48:22,916 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:48:22,986 >> loading weights file /content/models/bart-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:48:25,299 >> Some weights of the model checkpoint at /content/models/bart-0.9 were not used when initializing BartForCausalLM: ['encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layernorm_embedding.bias', 'decoder.layers.3.fc2.weight_mask', 'encoder.layers.4.fc1.bias', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.2.fc2.bias', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.1.fc1.weight_mask', 'encoder.layers.4.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.3.fc2.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.5.fc2.bias', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.fc2.bias', 'encoder.layers.3.fc1.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.fc1.bias', 'encoder.layers.3.fc2.bias', 'decoder.layers.5.fc2.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight_mask', 'encoder.embed_positions.weight', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.4.fc1.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.2.fc2.weight_mask', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'encoder.layers.0.fc2.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.1.fc2.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.fc2.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.3.fc1.weight_orig', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.fc1.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.2.fc2.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.fc1.weight_mask', 'encoder.layernorm_embedding.weight', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.0.fc1.bias', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.5.fc2.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'decoder.layers.4.fc1.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.0.fc1.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.3.fc1.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.5.fc1.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.3.fc2.weight_orig', 'decoder.layers.2.fc2.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.4.fc1.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'decoder.layers.0.fc2.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.3.fc2.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'decoder.layers.0.fc2.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.embed_tokens.weight', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'shared.weight', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.1.fc1.bias', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_mask']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:48:25,345 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart-0.9 and are newly initialized: ['decoder.layers.2.fc2.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'lm_head.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.4.self_attn.q_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:48:29,374 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:48:29,375 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:48:29,375 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:48:29,375 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:48:29,375 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:48:29,375 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:48:29,375 >>   Total optimization steps = 897\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:48:29,375 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.0006, 'learning_rate': 2.2129319955406913e-05, 'epoch': 1.67}\n",
            " 56% 500/897 [03:01<02:23,  2.77it/s][INFO|trainer.py:2692] 2022-11-09 06:51:30,437 >> Saving model checkpoint to /content/test-clm/bart-0.9/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:51:30,438 >> Configuration saved in /content/test-clm/bart-0.9/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:51:31,336 >> Model weights saved in /content/test-clm/bart-0.9/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:51:31,336 >> tokenizer config file saved in /content/test-clm/bart-0.9/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:51:31,337 >> Special tokens file saved in /content/test-clm/bart-0.9/checkpoint-500/special_tokens_map.json\n",
            "100% 897/897 [05:27<00:00,  2.96it/s][INFO|trainer.py:1873] 2022-11-09 06:53:56,881 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 327.5062, 'train_samples_per_second': 21.893, 'train_steps_per_second': 2.739, 'train_loss': 0.0003196698485758392, 'epoch': 3.0}\n",
            "100% 897/897 [05:27<00:00,  2.74it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:53:56,884 >> Saving model checkpoint to /content/test-clm/bart-0.9\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:53:56,885 >> Configuration saved in /content/test-clm/bart-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:53:57,691 >> Model weights saved in /content/test-clm/bart-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:53:57,692 >> tokenizer config file saved in /content/test-clm/bart-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:53:57,692 >> Special tokens file saved in /content/test-clm/bart-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0003\n",
            "  train_runtime            = 0:05:27.50\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.893\n",
            "  train_steps_per_second   =      2.739\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:53:57,806 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:53:57,806 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:53:57,806 >>   Batch size = 8\n",
            "100% 31/31 [00:05<00:00,  6.09it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0067\n",
            "  eval_loss               =        0.0\n",
            "  eval_runtime            = 0:00:05.22\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =     47.278\n",
            "  eval_steps_per_second   =      5.934\n",
            "  perplexity              =        1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart-0.95\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart-0.95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHjC3vC9CsZT",
        "outputId": "4fb53503-ed99-4187-af2c-52ab35765132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart-0.95/runs/Nov09_06-54-08_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart-0.95,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart-0.95,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 631.74it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:54:14,245 >> loading configuration file /content/models/bart-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:54:14,246 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.95\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 06:54:15,188 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:54:16,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:54:16,114 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 06:54:18,007 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 06:54:18,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:54:18,008 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:54:18,083 >> loading weights file /content/models/bart-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:54:20,402 >> Some weights of the model checkpoint at /content/models/bart-0.95 were not used when initializing BartForCausalLM: ['encoder.layers.1.fc1.weight_mask', 'encoder.layers.3.final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.5.fc2.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'encoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.4.fc1.bias', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.5.fc1.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.embed_tokens.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.0.fc1.weight_mask', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.4.fc2.bias', 'encoder.layers.3.fc2.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.5.fc2.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'decoder.layers.5.fc1.weight_orig', 'encoder.layers.2.fc1.bias', 'encoder.layers.0.fc2.weight_orig', 'encoder.layers.5.fc1.bias', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.3.fc2.weight_mask', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.2.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.1.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layernorm_embedding.bias', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.3.fc1.weight_orig', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.2.fc2.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.2.fc2.weight_mask', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.1.fc2.bias', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.embed_positions.weight', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.0.fc2.bias', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.fc1.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.4.fc2.weight_orig', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'decoder.layers.0.fc2.weight_mask', 'encoder.layers.3.fc1.bias', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.1.fc1.bias', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.3.fc1.weight_orig', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.0.fc1.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'encoder.layers.5.final_layer_norm.bias', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layernorm_embedding.weight', 'decoder.layers.3.fc1.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.fc1.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.0.fc2.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.4.final_layer_norm.bias', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'shared.weight', 'encoder.layers.3.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:54:20,478 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart-0.95 and are newly initialized: ['decoder.layers.0.fc1.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'lm_head.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:54:24,524 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:54:24,524 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:54:24,524 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:54:24,524 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:54:24,524 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:54:24,524 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:54:24,524 >>   Total optimization steps = 897\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:54:24,524 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.0006, 'learning_rate': 2.2129319955406913e-05, 'epoch': 1.67}\n",
            " 56% 500/897 [03:00<02:23,  2.76it/s][INFO|trainer.py:2692] 2022-11-09 06:57:25,491 >> Saving model checkpoint to /content/test-clm/bart-0.95/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:57:25,492 >> Configuration saved in /content/test-clm/bart-0.95/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:57:26,281 >> Model weights saved in /content/test-clm/bart-0.95/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:57:26,282 >> tokenizer config file saved in /content/test-clm/bart-0.95/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:57:26,282 >> Special tokens file saved in /content/test-clm/bart-0.95/checkpoint-500/special_tokens_map.json\n",
            "100% 897/897 [05:26<00:00,  2.97it/s][INFO|trainer.py:1873] 2022-11-09 06:59:51,429 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 326.9051, 'train_samples_per_second': 21.933, 'train_steps_per_second': 2.744, 'train_loss': 0.0003196698485758392, 'epoch': 3.0}\n",
            "100% 897/897 [05:26<00:00,  2.74it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:59:51,432 >> Saving model checkpoint to /content/test-clm/bart-0.95\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:59:51,433 >> Configuration saved in /content/test-clm/bart-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:59:52,236 >> Model weights saved in /content/test-clm/bart-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:59:52,237 >> tokenizer config file saved in /content/test-clm/bart-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:59:52,237 >> Special tokens file saved in /content/test-clm/bart-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0003\n",
            "  train_runtime            = 0:05:26.90\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.933\n",
            "  train_steps_per_second   =      2.744\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:59:52,346 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:59:52,346 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:59:52,346 >>   Batch size = 8\n",
            "100% 31/31 [00:05<00:00,  6.04it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0067\n",
            "  eval_loss               =        0.0\n",
            "  eval_runtime            = 0:00:05.26\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =     46.906\n",
            "  eval_steps_per_second   =      5.887\n",
            "  perplexity              =        1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu1rUUHiE4Of",
        "outputId": "ffe71652-a6bf-419c-d360-221ee63ab7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/bart-0.99/runs/Nov09_07-00-03_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/bart-0.99,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=6,\n",
            "per_device_train_batch_size=6,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/bart-0.99,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 535.81it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:00:50,042 >> loading configuration file /content/models/bart-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:00:50,043 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.99\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 07:00:50,986 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:00:51,939 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:00:51,940 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:00:53,844 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:00:53,844 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:00:53,845 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:00:53,928 >> loading weights file /content/models/bart-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:00:56,297 >> Some weights of the model checkpoint at /content/models/bart-0.99 were not used when initializing BartForCausalLM: ['encoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.1.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.1.fc2.weight_orig', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.2.fc1.weight_mask', 'encoder.layers.5.fc1.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.fc1.bias', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.3.fc1.weight_orig', 'encoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.0.fc1.weight_mask', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.fc1.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.1.fc2.bias', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.4.fc2.bias', 'encoder.layers.3.fc2.bias', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.4.fc1.bias', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.4.fc2.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.fc1.weight_mask', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.0.fc2.weight_orig', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'shared.weight', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.fc2.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.1.fc2.weight_mask', 'encoder.layers.2.fc2.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.0.fc1.weight_orig', 'encoder.embed_tokens.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.2.fc2.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.5.final_layer_norm.bias', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layers.4.fc1.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.fc1.bias', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layernorm_embedding.weight', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.4.final_layer_norm.weight', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.1.fc1.bias', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'encoder.layers.3.fc2.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.embed_positions.weight', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.0.fc1.bias', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.1.fc1.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.3.fc1.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.2.fc2.bias', 'decoder.layers.4.fc1.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'encoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask']\n",
            "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:00:56,301 >> Some weights of BartForCausalLM were not initialized from the model checkpoint at /content/models/bart-0.99 and are newly initialized: ['decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.2.fc2.weight', 'lm_head.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-400e07a3389b7b3e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7ae0071e7d112d1c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-01d77292734df292.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9b47396e2b29d3da.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b95064748cf4962a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5860a8d64827fbb8.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:01:00,544 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:01:00,544 >>   Num examples = 2390\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:01:00,544 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:01:00,544 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:01:00,544 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:01:00,544 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:01:00,544 >>   Total optimization steps = 1197\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:01:00,545 >>   Number of trainable parameters = 96103680\n",
            "{'loss': 0.0006, 'learning_rate': 2.9114452798663327e-05, 'epoch': 1.25}\n",
            " 42% 500/1197 [02:19<03:12,  3.61it/s][INFO|trainer.py:2692] 2022-11-09 07:03:19,590 >> Saving model checkpoint to /content/test-clm/bart-0.99/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:03:19,591 >> Configuration saved in /content/test-clm/bart-0.99/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:03:20,565 >> Model weights saved in /content/test-clm/bart-0.99/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:03:20,565 >> tokenizer config file saved in /content/test-clm/bart-0.99/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:03:20,566 >> Special tokens file saved in /content/test-clm/bart-0.99/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.0, 'learning_rate': 8.22890559732665e-06, 'epoch': 2.51}\n",
            " 84% 1000/1197 [04:41<00:54,  3.59it/s][INFO|trainer.py:2692] 2022-11-09 07:05:41,803 >> Saving model checkpoint to /content/test-clm/bart-0.99/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:05:41,805 >> Configuration saved in /content/test-clm/bart-0.99/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:05:42,667 >> Model weights saved in /content/test-clm/bart-0.99/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:05:42,668 >> tokenizer config file saved in /content/test-clm/bart-0.99/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:05:42,669 >> Special tokens file saved in /content/test-clm/bart-0.99/checkpoint-1000/special_tokens_map.json\n",
            "100% 1197/1197 [05:38<00:00,  4.33it/s][INFO|trainer.py:1873] 2022-11-09 07:06:39,136 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 338.591, 'train_samples_per_second': 21.176, 'train_steps_per_second': 3.535, 'train_loss': 0.00024424761943379067, 'epoch': 3.0}\n",
            "100% 1197/1197 [05:38<00:00,  3.54it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:06:39,138 >> Saving model checkpoint to /content/test-clm/bart-0.99\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:06:39,140 >> Configuration saved in /content/test-clm/bart-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:06:40,003 >> Model weights saved in /content/test-clm/bart-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:06:40,003 >> tokenizer config file saved in /content/test-clm/bart-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:06:40,004 >> Special tokens file saved in /content/test-clm/bart-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0002\n",
            "  train_runtime            = 0:05:38.59\n",
            "  train_samples            =       2390\n",
            "  train_samples_per_second =     21.176\n",
            "  train_steps_per_second   =      3.535\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:06:40,130 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:06:40,130 >>   Num examples = 247\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:06:40,131 >>   Batch size = 6\n",
            "100% 42/42 [00:05<00:00,  7.96it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.0067\n",
            "  eval_loss               =        0.0\n",
            "  eval_runtime            = 0:00:05.37\n",
            "  eval_samples            =        247\n",
            "  eval_samples_per_second =     45.935\n",
            "  eval_steps_per_second   =      7.811\n",
            "  perplexity              =        1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/bart-0.99\\\n",
        "    --tokenizer_name \"facebook/bart-base\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/bart-0.99"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert-0.1\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/bert-0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-JSCbccJwq_",
        "outputId": "39879c35-afeb-4f5a-b68a-b2e1a4a9afed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/bert-0.1/runs/Nov09_07-07-16_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/bert-0.1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/bert-0.1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 719.48it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:07:22,144 >> loading configuration file /content/models/bert-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:07:22,145 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:07:23,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:07:23,110 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:07:23,110 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:07:23,110 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:07:23,110 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:07:23,110 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:07:23,110 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:07:23,111 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:07:23,111 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:07:23,146 >> loading weights file /content/models/bert-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:07:25,577 >> Some weights of the model checkpoint at /content/models/bert-0.1 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_mask']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:07:25,577 >> Some weights of BertForMaskedLM were not initialized from the model checkpoint at /content/models/bert-0.1 and are newly initialized: ['bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "Running tokenizer on every text in dataset:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-67ad93d8d83b5b22.arrow\n",
            "[WARNING|tokenization_utils_base.py:3522] 2022-11-09 07:07:25,848 >> Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on every text in dataset:  97% 36/37 [00:03<00:00, 10.72ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-07a7a49003d660a1.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "Grouping texts in chunks of 512:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-220de35eadfd6c37.arrow\n",
            "Grouping texts in chunks of 512:  97% 36/37 [00:04<00:00,  7.22ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e3e8b27c3b1b4b00.arrow\n",
            "[INFO|trainer.py:726] 2022-11-09 07:07:37,832 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:07:37,845 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:07:37,845 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:07:37,845 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:07:37,845 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:07:37,845 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:07:37,845 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:07:37,845 >>   Total optimization steps = 1737\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:07:37,846 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/1737 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 07:07:37,863 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 6.8798, 'learning_rate': 3.5607369027058145e-05, 'epoch': 0.86}\n",
            " 29% 500/1737 [02:07<05:14,  3.93it/s][INFO|trainer.py:2692] 2022-11-09 07:09:45,471 >> Saving model checkpoint to /content/test-mlm/bert-0.1/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:09:45,472 >> Configuration saved in /content/test-mlm/bert-0.1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:09:46,435 >> Model weights saved in /content/test-mlm/bert-0.1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:09:46,436 >> tokenizer config file saved in /content/test-mlm/bert-0.1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:09:46,436 >> Special tokens file saved in /content/test-mlm/bert-0.1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 6.4518, 'learning_rate': 2.1214738054116294e-05, 'epoch': 1.73}\n",
            " 58% 1000/1737 [04:17<03:06,  3.96it/s][INFO|trainer.py:2692] 2022-11-09 07:11:54,875 >> Saving model checkpoint to /content/test-mlm/bert-0.1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:11:54,876 >> Configuration saved in /content/test-mlm/bert-0.1/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:11:55,761 >> Model weights saved in /content/test-mlm/bert-0.1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:11:55,762 >> tokenizer config file saved in /content/test-mlm/bert-0.1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:11:55,762 >> Special tokens file saved in /content/test-mlm/bert-0.1/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 6.3288, 'learning_rate': 6.822107081174439e-06, 'epoch': 2.59}\n",
            " 86% 1500/1737 [06:26<00:59,  3.96it/s][INFO|trainer.py:2692] 2022-11-09 07:14:04,150 >> Saving model checkpoint to /content/test-mlm/bert-0.1/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:14:04,151 >> Configuration saved in /content/test-mlm/bert-0.1/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:14:05,051 >> Model weights saved in /content/test-mlm/bert-0.1/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:14:05,052 >> tokenizer config file saved in /content/test-mlm/bert-0.1/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:14:05,052 >> Special tokens file saved in /content/test-mlm/bert-0.1/checkpoint-1500/special_tokens_map.json\n",
            "100% 1737/1737 [07:29<00:00,  4.71it/s][INFO|trainer.py:1873] 2022-11-09 07:15:06,970 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 449.1236, 'train_samples_per_second': 30.907, 'train_steps_per_second': 3.868, 'train_loss': 6.5142579108871255, 'epoch': 3.0}\n",
            "100% 1737/1737 [07:29<00:00,  3.87it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:15:06,972 >> Saving model checkpoint to /content/test-mlm/bert-0.1\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:15:06,973 >> Configuration saved in /content/test-mlm/bert-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:15:07,852 >> Model weights saved in /content/test-mlm/bert-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:15:07,853 >> tokenizer config file saved in /content/test-mlm/bert-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:15:07,853 >> Special tokens file saved in /content/test-mlm/bert-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.5143\n",
            "  train_runtime            = 0:07:29.12\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     30.907\n",
            "  train_steps_per_second   =      3.868\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 07:15:07,896 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:15:07,897 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:15:07,897 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:15:07,897 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.20it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1486\n",
            "  eval_loss               =     6.3114\n",
            "  eval_runtime            = 0:00:04.99\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =      95.82\n",
            "  eval_steps_per_second   =     12.002\n",
            "  perplexity              =   550.8139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert-0.5\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/new/bert-0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg-I28OKJz6Y",
        "outputId": "d539b4d3-3f6c-4848-86dd-1c6f298c2562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/new/bert-0.5/runs/Nov09_07-16-57_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/new/bert-0.5,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/new/bert-0.5,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 819.41it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:17:03,300 >> loading configuration file /content/models/bert-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:17:03,301 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.5\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:17:04,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:17:04,269 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:17:04,269 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:17:04,269 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:17:04,269 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:17:04,269 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:17:04,269 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:17:04,270 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:17:04,270 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:17:04,300 >> loading weights file /content/models/bert-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:17:07,936 >> Some weights of the model checkpoint at /content/models/bert-0.5 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:17:07,936 >> Some weights of BertForMaskedLM were not initialized from the model checkpoint at /content/models/bert-0.5 and are newly initialized: ['bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-67ad93d8d83b5b22.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-30c020d743e481c8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-220de35eadfd6c37.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-52fff9316d935a1d.arrow\n",
            "[INFO|trainer.py:726] 2022-11-09 07:17:11,766 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:17:11,777 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:17:11,777 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:17:11,777 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:17:11,778 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:17:11,778 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:17:11,778 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:17:11,778 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:17:11,778 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/870 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 07:17:11,799 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 6.7473, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [03:52<02:51,  2.16it/s][INFO|trainer.py:2692] 2022-11-09 07:21:03,987 >> Saving model checkpoint to /content/test-mlm/new/bert-0.5/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:21:03,988 >> Configuration saved in /content/test-mlm/new/bert-0.5/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:21:04,895 >> Model weights saved in /content/test-mlm/new/bert-0.5/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:21:04,896 >> tokenizer config file saved in /content/test-mlm/new/bert-0.5/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:21:04,896 >> Special tokens file saved in /content/test-mlm/new/bert-0.5/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [06:45<00:00,  2.78it/s][INFO|trainer.py:1873] 2022-11-09 07:23:57,483 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 405.7047, 'train_samples_per_second': 34.215, 'train_steps_per_second': 2.144, 'train_loss': 6.586445705369972, 'epoch': 3.0}\n",
            "100% 870/870 [06:45<00:00,  2.14it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:23:57,485 >> Saving model checkpoint to /content/test-mlm/new/bert-0.5\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:23:57,486 >> Configuration saved in /content/test-mlm/new/bert-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:23:58,324 >> Model weights saved in /content/test-mlm/new/bert-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:23:58,325 >> tokenizer config file saved in /content/test-mlm/new/bert-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:23:58,325 >> Special tokens file saved in /content/test-mlm/new/bert-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.5864\n",
            "  train_runtime            = 0:06:45.70\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     34.215\n",
            "  train_steps_per_second   =      2.144\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 07:23:58,366 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:23:58,367 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:23:58,367 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:23:58,367 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.20it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1468\n",
            "  eval_loss               =      6.352\n",
            "  eval_runtime            = 0:00:04.99\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =     95.819\n",
            "  eval_steps_per_second   =     12.002\n",
            "  perplexity              =   573.6636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert-0.9\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/bert-0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr6eY8g1J1VS",
        "outputId": "6ed14db1-0b9c-4e06-ec50-6ce05bd46b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/bert-0.9/runs/Nov09_07-24-23_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/bert-0.9,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/bert-0.9,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 908.84it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:24:29,640 >> loading configuration file /content/models/bert-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:24:29,641 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.9\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:24:30,616 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:24:30,616 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:24:30,617 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:24:30,617 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:24:30,617 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:24:30,617 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:24:30,617 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:24:30,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:24:30,618 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:24:30,647 >> loading weights file /content/models/bert-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:24:35,578 >> Some weights of the model checkpoint at /content/models/bert-0.9 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.10.output.dense.weight_orig', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_mask']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:24:35,578 >> Some weights of BertForMaskedLM were not initialized from the model checkpoint at /content/models/bert-0.9 and are newly initialized: ['bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-67ad93d8d83b5b22.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-30c020d743e481c8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-220de35eadfd6c37.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-52fff9316d935a1d.arrow\n",
            "[INFO|trainer.py:726] 2022-11-09 07:24:39,326 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:24:39,337 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:24:39,337 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:24:39,338 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:24:39,338 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:24:39,338 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:24:39,338 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:24:39,338 >>   Total optimization steps = 1737\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:24:39,338 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/1737 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 07:24:39,355 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 6.8798, 'learning_rate': 3.5607369027058145e-05, 'epoch': 0.86}\n",
            " 29% 500/1737 [02:07<05:16,  3.91it/s][INFO|trainer.py:2692] 2022-11-09 07:26:46,654 >> Saving model checkpoint to /content/test-mlm/bert-0.9/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:26:46,655 >> Configuration saved in /content/test-mlm/bert-0.9/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:26:47,560 >> Model weights saved in /content/test-mlm/bert-0.9/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:26:47,560 >> tokenizer config file saved in /content/test-mlm/bert-0.9/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:26:47,561 >> Special tokens file saved in /content/test-mlm/bert-0.9/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 6.4518, 'learning_rate': 2.1214738054116294e-05, 'epoch': 1.73}\n",
            " 58% 1000/1737 [04:16<03:05,  3.97it/s][INFO|trainer.py:2692] 2022-11-09 07:28:55,754 >> Saving model checkpoint to /content/test-mlm/bert-0.9/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:28:55,755 >> Configuration saved in /content/test-mlm/bert-0.9/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:28:56,605 >> Model weights saved in /content/test-mlm/bert-0.9/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:28:56,605 >> tokenizer config file saved in /content/test-mlm/bert-0.9/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:28:56,606 >> Special tokens file saved in /content/test-mlm/bert-0.9/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 6.3288, 'learning_rate': 6.822107081174439e-06, 'epoch': 2.59}\n",
            " 86% 1500/1737 [06:25<00:59,  3.97it/s][INFO|trainer.py:2692] 2022-11-09 07:31:04,703 >> Saving model checkpoint to /content/test-mlm/bert-0.9/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:31:04,704 >> Configuration saved in /content/test-mlm/bert-0.9/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:31:05,568 >> Model weights saved in /content/test-mlm/bert-0.9/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:31:05,569 >> tokenizer config file saved in /content/test-mlm/bert-0.9/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:31:05,569 >> Special tokens file saved in /content/test-mlm/bert-0.9/checkpoint-1500/special_tokens_map.json\n",
            "100% 1737/1737 [07:27<00:00,  4.71it/s][INFO|trainer.py:1873] 2022-11-09 07:32:07,285 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 447.9464, 'train_samples_per_second': 30.988, 'train_steps_per_second': 3.878, 'train_loss': 6.5142579108871255, 'epoch': 3.0}\n",
            "100% 1737/1737 [07:27<00:00,  3.88it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:32:07,287 >> Saving model checkpoint to /content/test-mlm/bert-0.9\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:32:07,288 >> Configuration saved in /content/test-mlm/bert-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:32:08,127 >> Model weights saved in /content/test-mlm/bert-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:32:08,128 >> tokenizer config file saved in /content/test-mlm/bert-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:32:08,128 >> Special tokens file saved in /content/test-mlm/bert-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.5143\n",
            "  train_runtime            = 0:07:27.94\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     30.988\n",
            "  train_steps_per_second   =      3.878\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 07:32:08,170 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:32:08,171 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:32:08,172 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:32:08,172 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.18it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1486\n",
            "  eval_loss               =     6.3114\n",
            "  eval_runtime            = 0:00:05.00\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =     95.639\n",
            "  eval_steps_per_second   =      11.98\n",
            "  perplexity              =   550.8139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert-0.95\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/bert-0.95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNbVxarFJ3-Y",
        "outputId": "e001e03f-456a-4554-f987-533084c521bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/bert-0.95/runs/Nov09_07-32-18_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/bert-0.95,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/bert-0.95,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 880.17it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:32:24,638 >> loading configuration file /content/models/bert-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:32:24,639 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.95\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:32:25,614 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:32:25,614 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:32:25,615 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:32:25,615 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:32:25,615 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:32:25,615 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:32:25,615 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:32:25,616 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:32:25,616 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:32:25,646 >> loading weights file /content/models/bert-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:32:30,418 >> Some weights of the model checkpoint at /content/models/bert-0.95 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:32:30,419 >> Some weights of BertForMaskedLM were not initialized from the model checkpoint at /content/models/bert-0.95 and are newly initialized: ['bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-67ad93d8d83b5b22.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-30c020d743e481c8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-220de35eadfd6c37.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-52fff9316d935a1d.arrow\n",
            "[INFO|trainer.py:726] 2022-11-09 07:32:34,228 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:32:34,239 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:32:34,239 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:32:34,240 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:32:34,240 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:32:34,240 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:32:34,240 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:32:34,240 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:32:34,240 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/870 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 07:32:34,261 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 6.7473, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [03:52<02:51,  2.16it/s][INFO|trainer.py:2692] 2022-11-09 07:36:26,636 >> Saving model checkpoint to /content/test-mlm/bert-0.95/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:36:26,637 >> Configuration saved in /content/test-mlm/bert-0.95/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:36:27,596 >> Model weights saved in /content/test-mlm/bert-0.95/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:36:27,596 >> tokenizer config file saved in /content/test-mlm/bert-0.95/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:36:27,596 >> Special tokens file saved in /content/test-mlm/bert-0.95/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [06:46<00:00,  2.79it/s][INFO|trainer.py:1873] 2022-11-09 07:39:20,532 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 406.2915, 'train_samples_per_second': 34.165, 'train_steps_per_second': 2.141, 'train_loss': 6.586445705369972, 'epoch': 3.0}\n",
            "100% 870/870 [06:46<00:00,  2.14it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:39:20,534 >> Saving model checkpoint to /content/test-mlm/bert-0.95\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:39:20,535 >> Configuration saved in /content/test-mlm/bert-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:39:21,409 >> Model weights saved in /content/test-mlm/bert-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:39:21,410 >> tokenizer config file saved in /content/test-mlm/bert-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:39:21,410 >> Special tokens file saved in /content/test-mlm/bert-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.5864\n",
            "  train_runtime            = 0:06:46.29\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     34.165\n",
            "  train_steps_per_second   =      2.141\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 07:39:21,447 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:39:21,449 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:39:21,449 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:39:21,449 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.15it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1468\n",
            "  eval_loss               =      6.352\n",
            "  eval_runtime            = 0:00:05.01\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =      95.44\n",
            "  eval_steps_per_second   =     11.955\n",
            "  perplexity              =   573.6636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwG-uDqQN2Pi",
        "outputId": "85863183-efa1-4e08-fe68-6db6fb9d2acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/bert-0.99/runs/Nov09_07-39-32_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-mlm/bert-0.99,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm/bert-0.99,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 987.05it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:39:37,754 >> loading configuration file /content/models/bert-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:39:37,755 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.99\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:39:38,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:39:38,667 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:39:38,668 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:39:38,668 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:39:38,668 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:39:38,668 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:39:38,668 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:39:38,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:39:38,669 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:39:38,699 >> loading weights file /content/models/bert-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:39:43,582 >> Some weights of the model checkpoint at /content/models/bert-0.99 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:39:43,582 >> Some weights of BertForMaskedLM were not initialized from the model checkpoint at /content/models/bert-0.99 and are newly initialized: ['bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7543fca6c0616e98.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-67ad93d8d83b5b22.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-30c020d743e481c8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d7689b3c40f7ced6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-220de35eadfd6c37.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-52fff9316d935a1d.arrow\n",
            "[INFO|trainer.py:726] 2022-11-09 07:39:47,303 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:39:47,314 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:39:47,314 >>   Num examples = 4627\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:39:47,315 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:39:47,315 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:39:47,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:39:47,315 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:39:47,315 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:39:47,315 >>   Number of trainable parameters = 109514298\n",
            "  0% 0/870 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-11-09 07:39:47,336 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 6.7473, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [03:52<02:52,  2.14it/s][INFO|trainer.py:2692] 2022-11-09 07:43:39,739 >> Saving model checkpoint to /content/test-mlm/bert-0.99/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:43:39,740 >> Configuration saved in /content/test-mlm/bert-0.99/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:43:40,667 >> Model weights saved in /content/test-mlm/bert-0.99/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:43:40,668 >> tokenizer config file saved in /content/test-mlm/bert-0.99/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:43:40,668 >> Special tokens file saved in /content/test-mlm/bert-0.99/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [06:46<00:00,  2.74it/s][INFO|trainer.py:1873] 2022-11-09 07:46:33,517 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 406.202, 'train_samples_per_second': 34.173, 'train_steps_per_second': 2.142, 'train_loss': 6.586445705369972, 'epoch': 3.0}\n",
            "100% 870/870 [06:46<00:00,  2.14it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:46:33,520 >> Saving model checkpoint to /content/test-mlm/bert-0.99\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:46:33,521 >> Configuration saved in /content/test-mlm/bert-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:46:34,353 >> Model weights saved in /content/test-mlm/bert-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:46:34,354 >> tokenizer config file saved in /content/test-mlm/bert-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:46:34,354 >> Special tokens file saved in /content/test-mlm/bert-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.5864\n",
            "  train_runtime            = 0:06:46.20\n",
            "  train_samples            =       4627\n",
            "  train_samples_per_second =     34.173\n",
            "  train_steps_per_second   =      2.142\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 07:46:34,393 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:46:34,395 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:46:34,395 >>   Num examples = 479\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:46:34,395 >>   Batch size = 8\n",
            "100% 60/60 [00:04<00:00, 12.23it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1468\n",
            "  eval_loss               =      6.352\n",
            "  eval_runtime            = 0:00:04.98\n",
            "  eval_samples            =        479\n",
            "  eval_samples_per_second =     96.041\n",
            "  eval_steps_per_second   =      12.03\n",
            "  perplexity              =   573.6636\n"
          ]
        }
      ],
      "source": [
        "!python run_mlm.py \\\n",
        "    --model_name_or_path /content/models/bert-0.99\\\n",
        "    --tokenizer_name \"bert-base-uncased\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-mlm/bert-0.99"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt-2-0.1\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt-2-0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8N6ONtNwvT",
        "outputId": "59551396-b643-49c2-872e-8357c058d01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.1/runs/Nov09_07-58-07_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 695.53it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:58:13,622 >> loading configuration file /content/models/gpt-2-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:58:13,623 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.1\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 07:58:14,591 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:58:15,535 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:58:15,536 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:58:17,451 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:58:17,452 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:58:17,452 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:58:17,524 >> loading weights file /content/models/gpt-2-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:58:20,034 >> Some weights of the model checkpoint at /content/models/gpt-2-0.1 were not used when initializing GPT2LMHeadModel: ['transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:58:20,034 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.1 and are newly initialized: ['transformer.h.6.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:58:24,070 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:58:24,070 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:58:24,070 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:58:24,070 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:58:24,070 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:58:24,070 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:58:24,070 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:58:24,070 >>   Number of trainable parameters = 124439808\n",
            "{'loss': 6.2671, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [05:17<03:54,  1.58it/s][INFO|trainer.py:2692] 2022-11-09 08:03:42,010 >> Saving model checkpoint to /content/test-clm/gpt-2-0.1/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 08:03:42,011 >> Configuration saved in /content/test-clm/gpt-2-0.1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 08:03:43,066 >> Model weights saved in /content/test-clm/gpt-2-0.1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 08:03:43,066 >> tokenizer config file saved in /content/test-clm/gpt-2-0.1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 08:03:43,067 >> Special tokens file saved in /content/test-clm/gpt-2-0.1/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [09:15<00:00,  1.70it/s][INFO|trainer.py:1873] 2022-11-09 08:07:39,731 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 555.6602, 'train_samples_per_second': 12.515, 'train_steps_per_second': 1.566, 'train_loss': 6.018685457076149, 'epoch': 3.0}\n",
            "100% 870/870 [09:15<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 08:07:39,733 >> Saving model checkpoint to /content/test-clm/gpt-2-0.1\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 08:07:39,734 >> Configuration saved in /content/test-clm/gpt-2-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 08:07:40,702 >> Model weights saved in /content/test-clm/gpt-2-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 08:07:40,702 >> tokenizer config file saved in /content/test-clm/gpt-2-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 08:07:40,703 >> Special tokens file saved in /content/test-clm/gpt-2-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.0187\n",
            "  train_runtime            = 0:09:15.66\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     12.515\n",
            "  train_steps_per_second   =      1.566\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 08:07:40,803 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 08:07:40,803 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-09 08:07:40,803 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.07it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.58\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.632\n",
            "  eval_steps_per_second   =      3.954\n",
            "  perplexity              =   296.4128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt-2-0.5\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt-2-0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhfrbM5-Nxhm",
        "outputId": "9a49b3e6-a023-4c1a-bb2e-737086c63231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.5/runs/Nov09_07-47-31_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.5,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.5,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 625.92it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 07:47:37,032 >> loading configuration file /content/models/gpt-2-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:47:37,032 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 07:47:37,993 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:47:38,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:47:38,919 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 07:47:40,796 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 07:47:40,797 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 07:47:40,798 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 07:47:40,870 >> loading weights file /content/models/gpt-2-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 07:47:46,218 >> Some weights of the model checkpoint at /content/models/gpt-2-0.5 were not used when initializing GPT2LMHeadModel: ['transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 07:47:46,219 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.5 and are newly initialized: ['transformer.h.4.mlp.c_fc.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.6.mlp.c_fc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 07:47:50,242 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 07:47:50,242 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-09 07:47:50,242 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 07:47:50,242 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 07:47:50,242 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 07:47:50,242 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 07:47:50,242 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 07:47:50,243 >>   Number of trainable parameters = 124439808\n",
            "{'loss': 6.2671, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [05:17<03:54,  1.58it/s][INFO|trainer.py:2692] 2022-11-09 07:53:07,893 >> Saving model checkpoint to /content/test-clm/gpt-2-0.5/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:53:07,894 >> Configuration saved in /content/test-clm/gpt-2-0.5/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:53:09,070 >> Model weights saved in /content/test-clm/gpt-2-0.5/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:53:09,071 >> tokenizer config file saved in /content/test-clm/gpt-2-0.5/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:53:09,071 >> Special tokens file saved in /content/test-clm/gpt-2-0.5/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [09:15<00:00,  1.70it/s][INFO|trainer.py:1873] 2022-11-09 07:57:05,610 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 555.3676, 'train_samples_per_second': 12.521, 'train_steps_per_second': 1.567, 'train_loss': 6.018685457076149, 'epoch': 3.0}\n",
            "100% 870/870 [09:15<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 07:57:05,612 >> Saving model checkpoint to /content/test-clm/gpt-2-0.5\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 07:57:05,613 >> Configuration saved in /content/test-clm/gpt-2-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 07:57:06,669 >> Model weights saved in /content/test-clm/gpt-2-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 07:57:06,670 >> tokenizer config file saved in /content/test-clm/gpt-2-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 07:57:06,670 >> Special tokens file saved in /content/test-clm/gpt-2-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.0187\n",
            "  train_runtime            = 0:09:15.36\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     12.521\n",
            "  train_steps_per_second   =      1.567\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 07:57:06,781 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 07:57:06,781 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-09 07:57:06,781 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.09it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.55\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.766\n",
            "  eval_steps_per_second   =      3.971\n",
            "  perplexity              =   296.4128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt-2-0.9\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt-2-0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNZI6C4GNz7B",
        "outputId": "c2fa6dcd-bc1f-4fb0-bfb1-8c8a1e9b61b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.9/runs/Nov09_08-18-06_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.9,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.9,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 664.43it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 08:18:11,968 >> loading configuration file /content/models/gpt-2-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 08:18:11,969 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.9\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 08:18:12,940 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 08:18:13,853 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 08:18:13,854 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 08:18:15,746 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 08:18:15,746 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 08:18:15,747 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 08:18:15,824 >> loading weights file /content/models/gpt-2-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 08:18:20,863 >> Some weights of the model checkpoint at /content/models/gpt-2-0.9 were not used when initializing GPT2LMHeadModel: ['transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 08:18:20,863 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.9 and are newly initialized: ['transformer.h.1.attn.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 08:18:24,838 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 08:18:24,838 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-09 08:18:24,838 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 08:18:24,838 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-09 08:18:24,838 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-09 08:18:24,838 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 08:18:24,838 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-09 08:18:24,839 >>   Number of trainable parameters = 124439808\n",
            "{'loss': 6.2671, 'learning_rate': 2.1264367816091954e-05, 'epoch': 1.72}\n",
            " 57% 500/870 [05:18<03:54,  1.58it/s][INFO|trainer.py:2692] 2022-11-09 08:23:42,941 >> Saving model checkpoint to /content/test-clm/gpt-2-0.9/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 08:23:42,942 >> Configuration saved in /content/test-clm/gpt-2-0.9/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 08:23:44,002 >> Model weights saved in /content/test-clm/gpt-2-0.9/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 08:23:44,002 >> tokenizer config file saved in /content/test-clm/gpt-2-0.9/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 08:23:44,002 >> Special tokens file saved in /content/test-clm/gpt-2-0.9/checkpoint-500/special_tokens_map.json\n",
            "100% 870/870 [09:15<00:00,  1.69it/s][INFO|trainer.py:1873] 2022-11-09 08:27:40,651 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 555.8127, 'train_samples_per_second': 12.511, 'train_steps_per_second': 1.565, 'train_loss': 6.018685457076149, 'epoch': 3.0}\n",
            "100% 870/870 [09:15<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 08:27:40,654 >> Saving model checkpoint to /content/test-clm/gpt-2-0.9\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 08:27:40,655 >> Configuration saved in /content/test-clm/gpt-2-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 08:27:41,625 >> Model weights saved in /content/test-clm/gpt-2-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 08:27:41,625 >> tokenizer config file saved in /content/test-clm/gpt-2-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 08:27:41,625 >> Special tokens file saved in /content/test-clm/gpt-2-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     6.0187\n",
            "  train_runtime            = 0:09:15.81\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     12.511\n",
            "  train_steps_per_second   =      1.565\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-09 08:27:41,738 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 08:27:41,738 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-09 08:27:41,739 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.09it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.54\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.828\n",
            "  eval_steps_per_second   =      3.979\n",
            "  perplexity              =   296.4128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt-2-0.95\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt-2-0.95\n"
      ],
      "metadata": {
        "id": "V7kcnZ4jN1WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck-j5eg4OiAo",
        "outputId": "e4d091b3-6c70-4eeb-cf8c-a3b8b50bd4c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.1/runs/Nov08_22-11-05_2921ed0a3794,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:Checkpoint detected, resuming training at /content/test-clm/gpt-2-0.1/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 695.80it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-08 22:11:06,192 >> loading configuration file /content/models/gpt-2-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:11:06,193 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.1\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-08 22:11:06,293 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:11:06,386 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:11:06,387 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,577 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,577 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,577 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,578 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,578 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:11:06,578 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:11:06,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:11:06,579 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-08 22:11:06,653 >> loading weights file /content/models/gpt-2-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-08 22:11:09,236 >> Some weights of the model checkpoint at /content/models/gpt-2-0.1 were not used when initializing GPT2LMHeadModel: ['transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-08 22:11:09,237 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.1 and are newly initialized: ['transformer.h.8.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "[INFO|trainer.py:1944] 2022-11-08 22:11:11,506 >> Loading model from /content/test-clm/gpt-2-0.1/checkpoint-500.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-08 22:11:12,466 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-08 22:11:12,467 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-08 22:11:12,467 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-08 22:11:12,467 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-08 22:11:12,467 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-08 22:11:12,467 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-08 22:11:12,467 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-08 22:11:12,467 >>   Number of trainable parameters = 124439808\n",
            "[INFO|trainer.py:1651] 2022-11-08 22:11:12,468 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1652] 2022-11-08 22:11:12,468 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1653] 2022-11-08 22:11:12,468 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1656] 2022-11-08 22:11:12,468 >>   Will skip the first 1 epochs then the first 210 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/210 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 210/210 [00:02<00:00, 91.11it/s]\n",
            "\n",
            " 58% 501/870 [00:03<00:02, 129.68it/s]\u001b[A\n",
            " 59% 514/870 [00:12<00:10, 33.17it/s] \u001b[A\n",
            " 60% 520/870 [00:15<00:15, 22.69it/s]\u001b[A\n",
            " 60% 524/870 [00:18<00:19, 17.62it/s]\u001b[A\n",
            " 60% 526/870 [00:19<00:22, 15.27it/s]\u001b[A\n",
            " 61% 528/870 [00:21<00:26, 12.90it/s]\u001b[A\n",
            " 61% 529/870 [00:21<00:29, 11.65it/s]\u001b[A\n",
            " 61% 530/870 [00:22<00:33, 10.29it/s]\u001b[A\n",
            " 61% 531/870 [00:22<00:38,  8.87it/s]\u001b[A\n",
            " 61% 532/870 [00:23<00:45,  7.50it/s]\u001b[A\n",
            " 61% 533/870 [00:24<00:54,  6.24it/s]\u001b[A\n",
            " 61% 534/870 [00:24<01:05,  5.15it/s]\u001b[A\n",
            " 61% 535/870 [00:25<01:18,  4.25it/s]\u001b[A\n",
            " 62% 536/870 [00:26<01:34,  3.55it/s]\u001b[A\n",
            " 62% 537/870 [00:26<01:50,  3.01it/s]\u001b[A\n",
            " 62% 538/870 [00:27<02:07,  2.61it/s]\u001b[A\n",
            " 62% 539/870 [00:28<02:23,  2.31it/s]\u001b[A\n",
            " 62% 540/870 [00:28<02:37,  2.10it/s]\u001b[A\n",
            " 62% 541/870 [00:29<02:48,  1.95it/s]\u001b[A\n",
            " 62% 542/870 [00:29<02:59,  1.83it/s]\u001b[A\n",
            " 62% 543/870 [00:30<03:06,  1.75it/s]\u001b[A\n",
            " 63% 544/870 [00:31<03:11,  1.70it/s]\u001b[A\n",
            " 63% 545/870 [00:31<03:15,  1.66it/s]\u001b[A\n",
            " 63% 546/870 [00:32<03:17,  1.64it/s]\u001b[A\n",
            " 63% 547/870 [00:33<03:19,  1.62it/s]\u001b[A\n",
            " 63% 548/870 [00:33<03:21,  1.60it/s]\u001b[A\n",
            " 63% 549/870 [00:34<03:21,  1.59it/s]\u001b[A\n",
            " 63% 550/870 [00:35<03:21,  1.59it/s]\u001b[A\n",
            " 63% 551/870 [00:35<03:21,  1.58it/s]\u001b[A\n",
            " 63% 552/870 [00:36<03:21,  1.58it/s]\u001b[A\n",
            " 64% 553/870 [00:36<03:20,  1.58it/s]\u001b[A\n",
            " 64% 554/870 [00:37<03:20,  1.58it/s]\u001b[A\n",
            " 64% 555/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 556/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 557/870 [00:39<03:18,  1.58it/s]\u001b[A\n",
            " 64% 558/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 559/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 560/870 [00:41<03:16,  1.57it/s]\u001b[A\n",
            " 64% 561/870 [00:41<03:16,  1.57it/s]\u001b[A\n",
            " 65% 562/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 563/870 [00:43<03:14,  1.57it/s]\u001b[A\n",
            " 65% 564/870 [00:43<03:14,  1.57it/s]\u001b[A\n",
            " 65% 565/870 [00:44<03:13,  1.57it/s]\u001b[A\n",
            " 65% 566/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 567/870 [00:45<03:12,  1.57it/s]\u001b[A\n",
            " 65% 568/870 [00:46<03:11,  1.58it/s]\u001b[A\n",
            " 65% 569/870 [00:47<03:11,  1.58it/s]\u001b[A\n",
            " 66% 570/870 [00:47<03:10,  1.58it/s]\u001b[A\n",
            " 66% 571/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 572/870 [00:48<03:09,  1.57it/s]\u001b[A\n",
            " 66% 573/870 [00:49<03:08,  1.57it/s]\u001b[A\n",
            " 66% 574/870 [00:50<03:07,  1.57it/s]\u001b[A\n",
            " 66% 575/870 [00:50<03:07,  1.57it/s]\u001b[A\n",
            " 66% 576/870 [00:51<03:06,  1.58it/s]\u001b[A\n",
            " 66% 577/870 [00:52<03:05,  1.58it/s]\u001b[A\n",
            " 66% 578/870 [00:52<03:05,  1.58it/s]\u001b[A\n",
            " 67% 579/870 [00:53<03:04,  1.58it/s]\u001b[A\n",
            " 67% 580/870 [00:53<02:50,  1.70it/s]\u001b[A\n",
            " 67% 581/870 [00:54<02:53,  1.66it/s]\u001b[A\n",
            " 67% 582/870 [00:55<02:56,  1.64it/s]\u001b[A\n",
            " 67% 583/870 [00:55<02:57,  1.62it/s]\u001b[A\n",
            " 67% 584/870 [00:56<02:58,  1.61it/s]\u001b[A\n",
            " 67% 585/870 [00:57<02:58,  1.60it/s]\u001b[A\n",
            " 67% 586/870 [00:57<02:58,  1.59it/s]\u001b[A\n",
            " 67% 587/870 [00:58<02:58,  1.59it/s]\u001b[A\n",
            " 68% 588/870 [00:58<02:57,  1.58it/s]\u001b[A\n",
            " 68% 589/870 [00:59<02:57,  1.58it/s]\u001b[A\n",
            " 68% 590/870 [01:00<02:57,  1.58it/s]\u001b[A\n",
            " 68% 591/870 [01:00<02:56,  1.58it/s]\u001b[A\n",
            " 68% 592/870 [01:01<02:56,  1.58it/s]\u001b[A\n",
            " 68% 593/870 [01:02<02:55,  1.58it/s]\u001b[A\n",
            " 68% 594/870 [01:02<02:54,  1.58it/s]\u001b[A\n",
            " 68% 595/870 [01:03<02:54,  1.58it/s]\u001b[A\n",
            " 69% 596/870 [01:04<02:53,  1.58it/s]\u001b[A\n",
            " 69% 597/870 [01:04<02:52,  1.58it/s]\u001b[A\n",
            " 69% 598/870 [01:05<02:52,  1.58it/s]\u001b[A\n",
            " 69% 599/870 [01:05<02:51,  1.58it/s]\u001b[A\n",
            " 69% 600/870 [01:06<02:51,  1.57it/s]\u001b[A\n",
            " 69% 601/870 [01:07<02:51,  1.57it/s]\u001b[A\n",
            " 69% 602/870 [01:07<02:50,  1.57it/s]\u001b[A\n",
            " 69% 603/870 [01:08<02:49,  1.57it/s]\u001b[A\n",
            " 69% 604/870 [01:09<02:49,  1.57it/s]\u001b[A\n",
            " 70% 605/870 [01:09<02:48,  1.57it/s]\u001b[A\n",
            " 70% 606/870 [01:10<02:47,  1.57it/s]\u001b[A\n",
            " 70% 607/870 [01:11<02:47,  1.57it/s]\u001b[A\n",
            " 70% 608/870 [01:11<02:46,  1.57it/s]\u001b[A\n",
            " 70% 609/870 [01:12<02:46,  1.57it/s]\u001b[A\n",
            " 70% 610/870 [01:12<02:45,  1.57it/s]\u001b[A\n",
            " 70% 611/870 [01:13<02:44,  1.57it/s]\u001b[A\n",
            " 70% 612/870 [01:14<02:44,  1.57it/s]\u001b[A\n",
            " 70% 613/870 [01:14<02:43,  1.57it/s]\u001b[A\n",
            " 71% 614/870 [01:15<02:43,  1.57it/s]\u001b[A\n",
            " 71% 615/870 [01:16<02:42,  1.57it/s]\u001b[A\n",
            " 71% 616/870 [01:16<02:41,  1.57it/s]\u001b[A\n",
            " 71% 617/870 [01:17<02:40,  1.57it/s]\u001b[A\n",
            " 71% 618/870 [01:18<02:40,  1.57it/s]\u001b[A\n",
            " 71% 619/870 [01:18<02:40,  1.57it/s]\u001b[A\n",
            " 71% 620/870 [01:19<02:39,  1.57it/s]\u001b[A\n",
            " 71% 621/870 [01:19<02:38,  1.57it/s]\u001b[A\n",
            " 71% 622/870 [01:20<02:37,  1.57it/s]\u001b[A\n",
            " 72% 623/870 [01:21<02:37,  1.57it/s]\u001b[A\n",
            " 72% 624/870 [01:21<02:36,  1.57it/s]\u001b[A\n",
            " 72% 625/870 [01:22<02:35,  1.57it/s]\u001b[A\n",
            " 72% 626/870 [01:23<02:35,  1.57it/s]\u001b[A\n",
            " 72% 627/870 [01:23<02:34,  1.57it/s]\u001b[A\n",
            " 72% 628/870 [01:24<02:33,  1.57it/s]\u001b[A\n",
            " 72% 629/870 [01:25<02:33,  1.57it/s]\u001b[A\n",
            " 72% 630/870 [01:25<02:32,  1.57it/s]\u001b[A\n",
            " 73% 631/870 [01:26<02:31,  1.57it/s]\u001b[A\n",
            " 73% 632/870 [01:26<02:31,  1.57it/s]\u001b[A\n",
            " 73% 633/870 [01:27<02:30,  1.57it/s]\u001b[A\n",
            " 73% 634/870 [01:28<02:29,  1.57it/s]\u001b[A\n",
            " 73% 635/870 [01:28<02:29,  1.57it/s]\u001b[A\n",
            " 73% 636/870 [01:29<02:28,  1.57it/s]\u001b[A\n",
            " 73% 637/870 [01:30<02:28,  1.57it/s]\u001b[A\n",
            " 73% 638/870 [01:30<02:27,  1.57it/s]\u001b[A\n",
            " 73% 639/870 [01:31<02:26,  1.57it/s]\u001b[A\n",
            " 74% 640/870 [01:32<02:25,  1.58it/s]\u001b[A\n",
            " 74% 641/870 [01:32<02:25,  1.57it/s]\u001b[A\n",
            " 74% 642/870 [01:33<02:24,  1.57it/s]\u001b[A\n",
            " 74% 643/870 [01:33<02:24,  1.57it/s]\u001b[A\n",
            " 74% 644/870 [01:34<02:23,  1.57it/s]\u001b[A\n",
            " 74% 645/870 [01:35<02:23,  1.57it/s]\u001b[A\n",
            " 74% 646/870 [01:35<02:22,  1.57it/s]\u001b[A\n",
            " 74% 647/870 [01:36<02:21,  1.57it/s]\u001b[A\n",
            " 74% 648/870 [01:37<02:21,  1.57it/s]\u001b[A\n",
            " 75% 649/870 [01:37<02:20,  1.57it/s]\u001b[A\n",
            " 75% 650/870 [01:38<02:19,  1.57it/s]\u001b[A\n",
            " 75% 651/870 [01:39<02:19,  1.57it/s]\u001b[A\n",
            " 75% 652/870 [01:39<02:18,  1.57it/s]\u001b[A\n",
            " 75% 653/870 [01:40<02:18,  1.57it/s]\u001b[A\n",
            " 75% 654/870 [01:40<02:17,  1.57it/s]\u001b[A\n",
            " 75% 655/870 [01:41<02:16,  1.57it/s]\u001b[A\n",
            " 75% 656/870 [01:42<02:16,  1.57it/s]\u001b[A\n",
            " 76% 657/870 [01:42<02:15,  1.57it/s]\u001b[A\n",
            " 76% 658/870 [01:43<02:14,  1.57it/s]\u001b[A\n",
            " 76% 659/870 [01:44<02:14,  1.57it/s]\u001b[A\n",
            " 76% 660/870 [01:44<02:13,  1.57it/s]\u001b[A\n",
            " 76% 661/870 [01:45<02:12,  1.57it/s]\u001b[A\n",
            " 76% 662/870 [01:46<02:12,  1.57it/s]\u001b[A\n",
            " 76% 663/870 [01:46<02:11,  1.57it/s]\u001b[A\n",
            " 76% 664/870 [01:47<02:10,  1.57it/s]\u001b[A\n",
            " 76% 665/870 [01:47<02:10,  1.57it/s]\u001b[A\n",
            " 77% 666/870 [01:48<02:09,  1.57it/s]\u001b[A\n",
            " 77% 667/870 [01:49<02:09,  1.57it/s]\u001b[A\n",
            " 77% 668/870 [01:49<02:08,  1.57it/s]\u001b[A\n",
            " 77% 669/870 [01:50<02:07,  1.57it/s]\u001b[A\n",
            " 77% 670/870 [01:51<02:07,  1.57it/s]\u001b[A\n",
            " 77% 671/870 [01:51<02:06,  1.57it/s]\u001b[A\n",
            " 77% 672/870 [01:52<02:05,  1.57it/s]\u001b[A\n",
            " 77% 673/870 [01:53<02:05,  1.57it/s]\u001b[A\n",
            " 77% 674/870 [01:53<02:04,  1.57it/s]\u001b[A\n",
            " 78% 675/870 [01:54<02:03,  1.57it/s]\u001b[A\n",
            " 78% 676/870 [01:54<02:03,  1.57it/s]\u001b[A\n",
            " 78% 677/870 [01:55<02:02,  1.57it/s]\u001b[A\n",
            " 78% 678/870 [01:56<02:01,  1.57it/s]\u001b[A\n",
            " 78% 679/870 [01:56<02:01,  1.57it/s]\u001b[A\n",
            " 78% 680/870 [01:57<02:00,  1.57it/s]\u001b[A\n",
            " 78% 681/870 [01:58<02:00,  1.57it/s]\u001b[A\n",
            " 78% 682/870 [01:58<01:59,  1.57it/s]\u001b[A\n",
            " 79% 683/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 684/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 685/870 [02:00<01:57,  1.57it/s]\u001b[A\n",
            " 79% 686/870 [02:01<01:56,  1.57it/s]\u001b[A\n",
            " 79% 687/870 [02:01<01:56,  1.57it/s]\u001b[A\n",
            " 79% 688/870 [02:02<01:55,  1.57it/s]\u001b[A\n",
            " 79% 689/870 [02:03<01:55,  1.57it/s]\u001b[A\n",
            " 79% 690/870 [02:03<01:54,  1.57it/s]\u001b[A\n",
            " 79% 691/870 [02:04<01:53,  1.57it/s]\u001b[A\n",
            " 80% 692/870 [02:05<01:53,  1.57it/s]\u001b[A\n",
            " 80% 693/870 [02:05<01:52,  1.57it/s]\u001b[A\n",
            " 80% 694/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 695/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 696/870 [02:07<01:50,  1.57it/s]\u001b[A\n",
            " 80% 697/870 [02:08<01:50,  1.57it/s]\u001b[A\n",
            " 80% 698/870 [02:08<01:49,  1.57it/s]\u001b[A\n",
            " 80% 699/870 [02:09<01:48,  1.57it/s]\u001b[A\n",
            " 80% 700/870 [02:10<01:47,  1.57it/s]\u001b[A\n",
            " 81% 701/870 [02:10<01:47,  1.57it/s]\u001b[A\n",
            " 81% 702/870 [02:11<01:46,  1.57it/s]\u001b[A\n",
            " 81% 703/870 [02:12<01:46,  1.57it/s]\u001b[A\n",
            " 81% 704/870 [02:12<01:45,  1.57it/s]\u001b[A\n",
            " 81% 705/870 [02:13<01:44,  1.57it/s]\u001b[A\n",
            " 81% 706/870 [02:13<01:44,  1.57it/s]\u001b[A\n",
            " 81% 707/870 [02:14<01:43,  1.57it/s]\u001b[A\n",
            " 81% 708/870 [02:15<01:42,  1.57it/s]\u001b[A\n",
            " 81% 709/870 [02:15<01:42,  1.57it/s]\u001b[A\n",
            " 82% 710/870 [02:16<01:41,  1.57it/s]\u001b[A\n",
            " 82% 711/870 [02:17<01:40,  1.57it/s]\u001b[A\n",
            " 82% 712/870 [02:17<01:40,  1.57it/s]\u001b[A\n",
            " 82% 713/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 714/870 [02:19<01:39,  1.57it/s]\u001b[A\n",
            " 82% 715/870 [02:19<01:38,  1.58it/s]\u001b[A\n",
            " 82% 716/870 [02:20<01:38,  1.57it/s]\u001b[A\n",
            " 82% 717/870 [02:20<01:37,  1.57it/s]\u001b[A\n",
            " 83% 718/870 [02:21<01:36,  1.57it/s]\u001b[A\n",
            " 83% 719/870 [02:22<01:35,  1.57it/s]\u001b[A\n",
            " 83% 720/870 [02:22<01:35,  1.58it/s]\u001b[A\n",
            " 83% 721/870 [02:23<01:34,  1.58it/s]\u001b[A\n",
            " 83% 722/870 [02:24<01:33,  1.58it/s]\u001b[A\n",
            " 83% 723/870 [02:24<01:33,  1.58it/s]\u001b[A\n",
            " 83% 724/870 [02:25<01:32,  1.58it/s]\u001b[A\n",
            " 83% 725/870 [02:26<01:31,  1.58it/s]\u001b[A\n",
            " 83% 726/870 [02:26<01:31,  1.58it/s]\u001b[A\n",
            " 84% 727/870 [02:27<01:30,  1.58it/s]\u001b[A\n",
            " 84% 728/870 [02:27<01:30,  1.57it/s]\u001b[A\n",
            " 84% 729/870 [02:28<01:29,  1.57it/s]\u001b[A\n",
            " 84% 730/870 [02:29<01:28,  1.57it/s]\u001b[A\n",
            " 84% 731/870 [02:29<01:28,  1.58it/s]\u001b[A\n",
            " 84% 732/870 [02:30<01:27,  1.58it/s]\u001b[A\n",
            " 84% 733/870 [02:31<01:26,  1.58it/s]\u001b[A\n",
            " 84% 734/870 [02:31<01:26,  1.58it/s]\u001b[A\n",
            " 84% 735/870 [02:32<01:25,  1.58it/s]\u001b[A\n",
            " 85% 736/870 [02:33<01:25,  1.58it/s]\u001b[A\n",
            " 85% 737/870 [02:33<01:24,  1.57it/s]\u001b[A\n",
            " 85% 738/870 [02:34<01:23,  1.57it/s]\u001b[A\n",
            " 85% 739/870 [02:34<01:23,  1.57it/s]\u001b[A\n",
            " 85% 740/870 [02:35<01:22,  1.57it/s]\u001b[A\n",
            " 85% 741/870 [02:36<01:21,  1.58it/s]\u001b[A\n",
            " 85% 742/870 [02:36<01:21,  1.58it/s]\u001b[A\n",
            " 85% 743/870 [02:37<01:20,  1.57it/s]\u001b[A\n",
            " 86% 744/870 [02:38<01:20,  1.57it/s]\u001b[A\n",
            " 86% 745/870 [02:38<01:19,  1.57it/s]\u001b[A\n",
            " 86% 746/870 [02:39<01:18,  1.57it/s]\u001b[A\n",
            " 86% 747/870 [02:40<01:18,  1.57it/s]\u001b[A\n",
            " 86% 748/870 [02:40<01:17,  1.57it/s]\u001b[A\n",
            " 86% 749/870 [02:41<01:16,  1.57it/s]\u001b[A\n",
            " 86% 750/870 [02:41<01:16,  1.57it/s]\u001b[A\n",
            " 86% 751/870 [02:42<01:15,  1.57it/s]\u001b[A\n",
            " 86% 752/870 [02:43<01:14,  1.57it/s]\u001b[A\n",
            " 87% 753/870 [02:43<01:14,  1.57it/s]\u001b[A\n",
            " 87% 754/870 [02:44<01:13,  1.57it/s]\u001b[A\n",
            " 87% 755/870 [02:45<01:13,  1.57it/s]\u001b[A\n",
            " 87% 756/870 [02:45<01:12,  1.57it/s]\u001b[A\n",
            " 87% 757/870 [02:46<01:11,  1.57it/s]\u001b[A\n",
            " 87% 758/870 [02:47<01:11,  1.57it/s]\u001b[A\n",
            " 87% 759/870 [02:47<01:10,  1.57it/s]\u001b[A\n",
            " 87% 760/870 [02:48<01:09,  1.57it/s]\u001b[A\n",
            " 87% 761/870 [02:48<01:09,  1.57it/s]\u001b[A\n",
            " 88% 762/870 [02:49<01:08,  1.57it/s]\u001b[A\n",
            " 88% 763/870 [02:50<01:08,  1.57it/s]\u001b[A\n",
            " 88% 764/870 [02:50<01:07,  1.57it/s]\u001b[A\n",
            " 88% 765/870 [02:51<01:06,  1.57it/s]\u001b[A\n",
            " 88% 766/870 [02:52<01:06,  1.57it/s]\u001b[A\n",
            " 88% 767/870 [02:52<01:05,  1.57it/s]\u001b[A\n",
            " 88% 768/870 [02:53<01:04,  1.57it/s]\u001b[A\n",
            " 88% 769/870 [02:54<01:04,  1.57it/s]\u001b[A\n",
            " 89% 770/870 [02:54<01:03,  1.57it/s]\u001b[A\n",
            " 89% 771/870 [02:55<01:03,  1.57it/s]\u001b[A\n",
            " 89% 772/870 [02:55<01:02,  1.57it/s]\u001b[A\n",
            " 89% 773/870 [02:56<01:01,  1.57it/s]\u001b[A\n",
            " 89% 774/870 [02:57<01:01,  1.57it/s]\u001b[A\n",
            " 89% 775/870 [02:57<01:00,  1.57it/s]\u001b[A\n",
            " 89% 776/870 [02:58<00:59,  1.57it/s]\u001b[A\n",
            " 89% 777/870 [02:59<00:59,  1.57it/s]\u001b[A\n",
            " 89% 778/870 [02:59<00:58,  1.57it/s]\u001b[A\n",
            " 90% 779/870 [03:00<00:57,  1.57it/s]\u001b[A\n",
            " 90% 780/870 [03:01<00:57,  1.57it/s]\u001b[A\n",
            " 90% 781/870 [03:01<00:56,  1.57it/s]\u001b[A\n",
            " 90% 782/870 [03:02<00:55,  1.57it/s]\u001b[A\n",
            " 90% 783/870 [03:02<00:55,  1.57it/s]\u001b[A\n",
            " 90% 784/870 [03:03<00:54,  1.57it/s]\u001b[A\n",
            " 90% 785/870 [03:04<00:54,  1.57it/s]\u001b[A\n",
            " 90% 786/870 [03:04<00:53,  1.57it/s]\u001b[A\n",
            " 90% 787/870 [03:05<00:52,  1.57it/s]\u001b[A\n",
            " 91% 788/870 [03:06<00:52,  1.57it/s]\u001b[A\n",
            " 91% 789/870 [03:06<00:51,  1.57it/s]\u001b[A\n",
            " 91% 790/870 [03:07<00:50,  1.57it/s]\u001b[A\n",
            " 91% 791/870 [03:07<00:50,  1.57it/s]\u001b[A\n",
            " 91% 792/870 [03:08<00:49,  1.57it/s]\u001b[A\n",
            " 91% 793/870 [03:09<00:49,  1.57it/s]\u001b[A\n",
            " 91% 794/870 [03:09<00:48,  1.57it/s]\u001b[A\n",
            " 91% 795/870 [03:10<00:47,  1.57it/s]\u001b[A\n",
            " 91% 796/870 [03:11<00:47,  1.57it/s]\u001b[A\n",
            " 92% 797/870 [03:11<00:46,  1.57it/s]\u001b[A\n",
            " 92% 798/870 [03:12<00:45,  1.57it/s]\u001b[A\n",
            " 92% 799/870 [03:13<00:45,  1.57it/s]\u001b[A\n",
            " 92% 800/870 [03:13<00:44,  1.57it/s]\u001b[A\n",
            " 92% 801/870 [03:14<00:43,  1.57it/s]\u001b[A\n",
            " 92% 802/870 [03:15<00:43,  1.57it/s]\u001b[A\n",
            " 92% 803/870 [03:15<00:42,  1.57it/s]\u001b[A\n",
            " 92% 804/870 [03:16<00:42,  1.57it/s]\u001b[A\n",
            " 93% 805/870 [03:16<00:41,  1.56it/s]\u001b[A\n",
            " 93% 806/870 [03:17<00:40,  1.56it/s]\u001b[A\n",
            " 93% 807/870 [03:18<00:40,  1.57it/s]\u001b[A\n",
            " 93% 808/870 [03:18<00:39,  1.57it/s]\u001b[A\n",
            " 93% 809/870 [03:19<00:38,  1.56it/s]\u001b[A\n",
            " 93% 810/870 [03:20<00:38,  1.57it/s]\u001b[A\n",
            " 93% 811/870 [03:20<00:37,  1.57it/s]\u001b[A\n",
            " 93% 812/870 [03:21<00:36,  1.57it/s]\u001b[A\n",
            " 93% 813/870 [03:22<00:36,  1.57it/s]\u001b[A\n",
            " 94% 814/870 [03:22<00:35,  1.57it/s]\u001b[A\n",
            " 94% 815/870 [03:23<00:34,  1.57it/s]\u001b[A\n",
            " 94% 816/870 [03:23<00:34,  1.57it/s]\u001b[A\n",
            " 94% 817/870 [03:24<00:33,  1.57it/s]\u001b[A\n",
            " 94% 818/870 [03:25<00:33,  1.57it/s]\u001b[A\n",
            " 94% 819/870 [03:25<00:32,  1.57it/s]\u001b[A\n",
            " 94% 820/870 [03:26<00:31,  1.57it/s]\u001b[A\n",
            " 94% 821/870 [03:27<00:31,  1.57it/s]\u001b[A\n",
            " 94% 822/870 [03:27<00:30,  1.57it/s]\u001b[A\n",
            " 95% 823/870 [03:28<00:29,  1.57it/s]\u001b[A\n",
            " 95% 824/870 [03:29<00:29,  1.57it/s]\u001b[A\n",
            " 95% 825/870 [03:29<00:28,  1.57it/s]\u001b[A\n",
            " 95% 826/870 [03:30<00:27,  1.57it/s]\u001b[A\n",
            " 95% 827/870 [03:30<00:27,  1.57it/s]\u001b[A\n",
            " 95% 828/870 [03:31<00:26,  1.57it/s]\u001b[A\n",
            " 95% 829/870 [03:32<00:26,  1.57it/s]\u001b[A\n",
            " 95% 830/870 [03:32<00:25,  1.58it/s]\u001b[A\n",
            " 96% 831/870 [03:33<00:24,  1.58it/s]\u001b[A\n",
            " 96% 832/870 [03:34<00:24,  1.58it/s]\u001b[A\n",
            " 96% 833/870 [03:34<00:23,  1.58it/s]\u001b[A\n",
            " 96% 834/870 [03:35<00:22,  1.57it/s]\u001b[A\n",
            " 96% 835/870 [03:36<00:22,  1.58it/s]\u001b[A\n",
            " 96% 836/870 [03:36<00:21,  1.58it/s]\u001b[A\n",
            " 96% 837/870 [03:37<00:20,  1.57it/s]\u001b[A\n",
            " 96% 838/870 [03:37<00:20,  1.57it/s]\u001b[A\n",
            " 96% 839/870 [03:38<00:19,  1.57it/s]\u001b[A\n",
            " 97% 840/870 [03:39<00:19,  1.57it/s]\u001b[A\n",
            " 97% 841/870 [03:39<00:18,  1.57it/s]\u001b[A\n",
            " 97% 842/870 [03:40<00:17,  1.57it/s]\u001b[A\n",
            " 97% 843/870 [03:41<00:17,  1.57it/s]\u001b[A\n",
            " 97% 844/870 [03:41<00:16,  1.57it/s]\u001b[A\n",
            " 97% 845/870 [03:42<00:15,  1.57it/s]\u001b[A\n",
            " 97% 846/870 [03:43<00:15,  1.57it/s]\u001b[A\n",
            " 97% 847/870 [03:43<00:14,  1.57it/s]\u001b[A\n",
            " 97% 848/870 [03:44<00:13,  1.57it/s]\u001b[A\n",
            " 98% 849/870 [03:44<00:13,  1.57it/s]\u001b[A\n",
            " 98% 850/870 [03:45<00:12,  1.57it/s]\u001b[A\n",
            " 98% 851/870 [03:46<00:12,  1.57it/s]\u001b[A\n",
            " 98% 852/870 [03:46<00:11,  1.57it/s]\u001b[A\n",
            " 98% 853/870 [03:47<00:10,  1.57it/s]\u001b[A\n",
            " 98% 854/870 [03:48<00:10,  1.57it/s]\u001b[A\n",
            " 98% 855/870 [03:48<00:09,  1.57it/s]\u001b[A\n",
            " 98% 856/870 [03:49<00:08,  1.58it/s]\u001b[A\n",
            " 99% 857/870 [03:49<00:08,  1.58it/s]\u001b[A\n",
            " 99% 858/870 [03:50<00:07,  1.58it/s]\u001b[A\n",
            " 99% 859/870 [03:51<00:06,  1.58it/s]\u001b[A\n",
            " 99% 860/870 [03:51<00:06,  1.58it/s]\u001b[A\n",
            " 99% 861/870 [03:52<00:05,  1.58it/s]\u001b[A\n",
            " 99% 862/870 [03:53<00:05,  1.58it/s]\u001b[A\n",
            " 99% 863/870 [03:53<00:04,  1.58it/s]\u001b[A\n",
            " 99% 864/870 [03:54<00:03,  1.58it/s]\u001b[A\n",
            " 99% 865/870 [03:55<00:03,  1.58it/s]\u001b[A\n",
            "100% 866/870 [03:55<00:02,  1.58it/s]\u001b[A\n",
            "100% 867/870 [03:56<00:01,  1.58it/s]\u001b[A\n",
            "100% 868/870 [03:56<00:01,  1.57it/s]\u001b[A\n",
            "100% 869/870 [03:57<00:00,  1.57it/s]\u001b[A\n",
            "100% 870/870 [03:58<00:00,  1.70it/s]\u001b[A[INFO|trainer.py:1873] 2022-11-08 22:15:10,564 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A{'train_runtime': 238.0965, 'train_samples_per_second': 29.207, 'train_steps_per_second': 3.654, 'train_loss': 2.416890321928879, 'epoch': 3.0}\n",
            "\n",
            "100% 870/870 [03:58<00:00,  3.65it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-08 22:15:10,566 >> Saving model checkpoint to /content/test-clm/gpt-2-0.1\n",
            "[INFO|configuration_utils.py:447] 2022-11-08 22:15:10,567 >> Configuration saved in /content/test-clm/gpt-2-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-08 22:15:11,973 >> Model weights saved in /content/test-clm/gpt-2-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-08 22:15:11,973 >> tokenizer config file saved in /content/test-clm/gpt-2-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-08 22:15:11,974 >> Special tokens file saved in /content/test-clm/gpt-2-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.4169\n",
            "  train_runtime            = 0:03:58.09\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     29.207\n",
            "  train_steps_per_second   =      3.654\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-08 22:15:12,083 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-08 22:15:12,083 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-08 22:15:12,083 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.55\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.754\n",
            "  eval_steps_per_second   =      3.969\n",
            "  perplexity              =   296.4128\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.5/runs/Nov08_22-15-25_2921ed0a3794,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.5,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.5,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:Checkpoint detected, resuming training at /content/test-clm/gpt-2-0.5/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 642.71it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-08 22:15:26,019 >> loading configuration file /content/models/gpt-2-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:15:26,020 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-08 22:15:26,118 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:15:26,215 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:15:26,216 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,407 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,408 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,408 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,408 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,408 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:15:26,408 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:15:26,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:15:26,409 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-08 22:15:26,484 >> loading weights file /content/models/gpt-2-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-08 22:15:28,999 >> Some weights of the model checkpoint at /content/models/gpt-2-0.5 were not used when initializing GPT2LMHeadModel: ['transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-08 22:15:28,999 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.5 and are newly initialized: ['transformer.h.11.attn.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "[INFO|trainer.py:1944] 2022-11-08 22:15:31,251 >> Loading model from /content/test-clm/gpt-2-0.5/checkpoint-500.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-08 22:15:32,233 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-08 22:15:32,233 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-08 22:15:32,233 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-08 22:15:32,233 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-08 22:15:32,233 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-08 22:15:32,233 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-08 22:15:32,233 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-08 22:15:32,234 >>   Number of trainable parameters = 124439808\n",
            "[INFO|trainer.py:1651] 2022-11-08 22:15:32,234 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1652] 2022-11-08 22:15:32,234 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1653] 2022-11-08 22:15:32,234 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1656] 2022-11-08 22:15:32,234 >>   Will skip the first 1 epochs then the first 210 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/210 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 210/210 [00:02<00:00, 97.22it/s]\n",
            "\n",
            " 58% 501/870 [00:03<00:02, 133.51it/s]\u001b[A\n",
            " 59% 515/870 [00:12<00:11, 31.62it/s] \u001b[A\n",
            " 60% 521/870 [00:16<00:15, 22.00it/s]\u001b[A\n",
            " 60% 525/870 [00:18<00:20, 17.23it/s]\u001b[A\n",
            " 61% 527/870 [00:20<00:22, 14.99it/s]\u001b[A\n",
            " 61% 529/870 [00:21<00:26, 12.70it/s]\u001b[A\n",
            " 61% 530/870 [00:22<00:29, 11.48it/s]\u001b[A\n",
            " 61% 531/870 [00:22<00:33, 10.16it/s]\u001b[A\n",
            " 61% 532/870 [00:23<00:38,  8.79it/s]\u001b[A\n",
            " 61% 533/870 [00:24<00:45,  7.42it/s]\u001b[A\n",
            " 61% 534/870 [00:24<00:54,  6.17it/s]\u001b[A\n",
            " 61% 535/870 [00:25<01:05,  5.09it/s]\u001b[A\n",
            " 62% 536/870 [00:26<01:19,  4.22it/s]\u001b[A\n",
            " 62% 537/870 [00:26<01:34,  3.53it/s]\u001b[A\n",
            " 62% 538/870 [00:27<01:50,  3.00it/s]\u001b[A\n",
            " 62% 539/870 [00:27<02:06,  2.61it/s]\u001b[A\n",
            " 62% 540/870 [00:28<02:22,  2.31it/s]\u001b[A\n",
            " 62% 541/870 [00:29<02:37,  2.09it/s]\u001b[A\n",
            " 62% 542/870 [00:29<02:48,  1.94it/s]\u001b[A\n",
            " 62% 543/870 [00:30<02:58,  1.84it/s]\u001b[A\n",
            " 63% 544/870 [00:31<03:05,  1.76it/s]\u001b[A\n",
            " 63% 545/870 [00:31<03:10,  1.70it/s]\u001b[A\n",
            " 63% 546/870 [00:32<03:14,  1.67it/s]\u001b[A\n",
            " 63% 547/870 [00:33<03:16,  1.64it/s]\u001b[A\n",
            " 63% 548/870 [00:33<03:18,  1.62it/s]\u001b[A\n",
            " 63% 549/870 [00:34<03:20,  1.60it/s]\u001b[A\n",
            " 63% 550/870 [00:34<03:20,  1.60it/s]\u001b[A\n",
            " 63% 551/870 [00:35<03:20,  1.59it/s]\u001b[A\n",
            " 63% 552/870 [00:36<03:20,  1.59it/s]\u001b[A\n",
            " 64% 553/870 [00:36<03:20,  1.58it/s]\u001b[A\n",
            " 64% 554/870 [00:37<03:19,  1.58it/s]\u001b[A\n",
            " 64% 555/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 556/870 [00:38<03:18,  1.58it/s]\u001b[A\n",
            " 64% 557/870 [00:39<03:18,  1.58it/s]\u001b[A\n",
            " 64% 558/870 [00:39<03:17,  1.58it/s]\u001b[A\n",
            " 64% 559/870 [00:40<03:16,  1.58it/s]\u001b[A\n",
            " 64% 560/870 [00:41<03:16,  1.58it/s]\u001b[A\n",
            " 64% 561/870 [00:41<03:15,  1.58it/s]\u001b[A\n",
            " 65% 562/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 563/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 564/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 565/870 [00:44<03:13,  1.58it/s]\u001b[A\n",
            " 65% 566/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 567/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 568/870 [00:46<03:12,  1.57it/s]\u001b[A\n",
            " 65% 569/870 [00:46<03:11,  1.57it/s]\u001b[A\n",
            " 66% 570/870 [00:47<03:10,  1.57it/s]\u001b[A\n",
            " 66% 571/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 572/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 573/870 [00:49<03:08,  1.58it/s]\u001b[A\n",
            " 66% 574/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 575/870 [00:50<03:06,  1.58it/s]\u001b[A\n",
            " 66% 576/870 [00:51<03:06,  1.58it/s]\u001b[A\n",
            " 66% 577/870 [00:52<03:05,  1.58it/s]\u001b[A\n",
            " 66% 578/870 [00:52<03:04,  1.58it/s]\u001b[A\n",
            " 67% 579/870 [00:53<03:05,  1.57it/s]\u001b[A\n",
            " 67% 580/870 [00:53<02:51,  1.69it/s]\u001b[A\n",
            " 67% 581/870 [00:54<02:54,  1.65it/s]\u001b[A\n",
            " 67% 582/870 [00:55<02:56,  1.63it/s]\u001b[A\n",
            " 67% 583/870 [00:55<02:57,  1.61it/s]\u001b[A\n",
            " 67% 584/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 585/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 586/870 [00:57<02:58,  1.59it/s]\u001b[A\n",
            " 67% 587/870 [00:58<02:58,  1.59it/s]\u001b[A\n",
            " 68% 588/870 [00:58<02:57,  1.58it/s]\u001b[A\n",
            " 68% 589/870 [00:59<02:57,  1.58it/s]\u001b[A\n",
            " 68% 590/870 [01:00<02:56,  1.58it/s]\u001b[A\n",
            " 68% 591/870 [01:00<02:56,  1.58it/s]\u001b[A\n",
            " 68% 592/870 [01:01<02:55,  1.58it/s]\u001b[A\n",
            " 68% 593/870 [01:02<02:55,  1.58it/s]\u001b[A\n",
            " 68% 594/870 [01:02<02:54,  1.58it/s]\u001b[A\n",
            " 68% 595/870 [01:03<02:54,  1.58it/s]\u001b[A\n",
            " 69% 596/870 [01:03<02:53,  1.58it/s]\u001b[A\n",
            " 69% 597/870 [01:04<02:52,  1.58it/s]\u001b[A\n",
            " 69% 598/870 [01:05<02:52,  1.58it/s]\u001b[A\n",
            " 69% 599/870 [01:05<02:51,  1.58it/s]\u001b[A\n",
            " 69% 600/870 [01:06<02:50,  1.58it/s]\u001b[A\n",
            " 69% 601/870 [01:07<02:50,  1.58it/s]\u001b[A\n",
            " 69% 602/870 [01:07<02:49,  1.58it/s]\u001b[A\n",
            " 69% 603/870 [01:08<02:49,  1.57it/s]\u001b[A\n",
            " 69% 604/870 [01:09<02:48,  1.58it/s]\u001b[A\n",
            " 70% 605/870 [01:09<02:48,  1.58it/s]\u001b[A\n",
            " 70% 606/870 [01:10<02:47,  1.58it/s]\u001b[A\n",
            " 70% 607/870 [01:10<02:46,  1.58it/s]\u001b[A\n",
            " 70% 608/870 [01:11<02:45,  1.58it/s]\u001b[A\n",
            " 70% 609/870 [01:12<02:45,  1.58it/s]\u001b[A\n",
            " 70% 610/870 [01:12<02:44,  1.58it/s]\u001b[A\n",
            " 70% 611/870 [01:13<02:43,  1.58it/s]\u001b[A\n",
            " 70% 612/870 [01:14<02:43,  1.58it/s]\u001b[A\n",
            " 70% 613/870 [01:14<02:42,  1.58it/s]\u001b[A\n",
            " 71% 614/870 [01:15<02:42,  1.58it/s]\u001b[A\n",
            " 71% 615/870 [01:15<02:41,  1.58it/s]\u001b[A\n",
            " 71% 616/870 [01:16<02:40,  1.58it/s]\u001b[A\n",
            " 71% 617/870 [01:17<02:40,  1.58it/s]\u001b[A\n",
            " 71% 618/870 [01:17<02:39,  1.58it/s]\u001b[A\n",
            " 71% 619/870 [01:18<02:38,  1.58it/s]\u001b[A\n",
            " 71% 620/870 [01:19<02:38,  1.58it/s]\u001b[A\n",
            " 71% 621/870 [01:19<02:37,  1.58it/s]\u001b[A\n",
            " 71% 622/870 [01:20<02:37,  1.58it/s]\u001b[A\n",
            " 72% 623/870 [01:21<02:37,  1.57it/s]\u001b[A\n",
            " 72% 624/870 [01:21<02:36,  1.57it/s]\u001b[A\n",
            " 72% 625/870 [01:22<02:35,  1.57it/s]\u001b[A\n",
            " 72% 626/870 [01:22<02:35,  1.57it/s]\u001b[A\n",
            " 72% 627/870 [01:23<02:34,  1.57it/s]\u001b[A\n",
            " 72% 628/870 [01:24<02:33,  1.57it/s]\u001b[A\n",
            " 72% 629/870 [01:24<02:33,  1.57it/s]\u001b[A\n",
            " 72% 630/870 [01:25<02:32,  1.58it/s]\u001b[A\n",
            " 73% 631/870 [01:26<02:31,  1.58it/s]\u001b[A\n",
            " 73% 632/870 [01:26<02:31,  1.58it/s]\u001b[A\n",
            " 73% 633/870 [01:27<02:30,  1.58it/s]\u001b[A\n",
            " 73% 634/870 [01:28<02:29,  1.58it/s]\u001b[A\n",
            " 73% 635/870 [01:28<02:29,  1.58it/s]\u001b[A\n",
            " 73% 636/870 [01:29<02:28,  1.58it/s]\u001b[A\n",
            " 73% 637/870 [01:29<02:27,  1.58it/s]\u001b[A\n",
            " 73% 638/870 [01:30<02:27,  1.58it/s]\u001b[A\n",
            " 73% 639/870 [01:31<02:26,  1.58it/s]\u001b[A\n",
            " 74% 640/870 [01:31<02:25,  1.58it/s]\u001b[A\n",
            " 74% 641/870 [01:32<02:25,  1.58it/s]\u001b[A\n",
            " 74% 642/870 [01:33<02:25,  1.57it/s]\u001b[A\n",
            " 74% 643/870 [01:33<02:24,  1.57it/s]\u001b[A\n",
            " 74% 644/870 [01:34<02:23,  1.57it/s]\u001b[A\n",
            " 74% 645/870 [01:35<02:22,  1.58it/s]\u001b[A\n",
            " 74% 646/870 [01:35<02:22,  1.58it/s]\u001b[A\n",
            " 74% 647/870 [01:36<02:21,  1.58it/s]\u001b[A\n",
            " 74% 648/870 [01:36<02:20,  1.58it/s]\u001b[A\n",
            " 75% 649/870 [01:37<02:20,  1.58it/s]\u001b[A\n",
            " 75% 650/870 [01:38<02:19,  1.58it/s]\u001b[A\n",
            " 75% 651/870 [01:38<02:18,  1.58it/s]\u001b[A\n",
            " 75% 652/870 [01:39<02:18,  1.58it/s]\u001b[A\n",
            " 75% 653/870 [01:40<02:17,  1.57it/s]\u001b[A\n",
            " 75% 654/870 [01:40<02:17,  1.57it/s]\u001b[A\n",
            " 75% 655/870 [01:41<02:16,  1.58it/s]\u001b[A\n",
            " 75% 656/870 [01:41<02:15,  1.58it/s]\u001b[A\n",
            " 76% 657/870 [01:42<02:15,  1.58it/s]\u001b[A\n",
            " 76% 658/870 [01:43<02:14,  1.58it/s]\u001b[A\n",
            " 76% 659/870 [01:43<02:13,  1.58it/s]\u001b[A\n",
            " 76% 660/870 [01:44<02:13,  1.58it/s]\u001b[A\n",
            " 76% 661/870 [01:45<02:13,  1.57it/s]\u001b[A\n",
            " 76% 662/870 [01:45<02:12,  1.57it/s]\u001b[A\n",
            " 76% 663/870 [01:46<02:11,  1.57it/s]\u001b[A\n",
            " 76% 664/870 [01:47<02:10,  1.58it/s]\u001b[A\n",
            " 76% 665/870 [01:47<02:10,  1.58it/s]\u001b[A\n",
            " 77% 666/870 [01:48<02:09,  1.58it/s]\u001b[A\n",
            " 77% 667/870 [01:48<02:08,  1.58it/s]\u001b[A\n",
            " 77% 668/870 [01:49<02:07,  1.58it/s]\u001b[A\n",
            " 77% 669/870 [01:50<02:07,  1.58it/s]\u001b[A\n",
            " 77% 670/870 [01:50<02:06,  1.58it/s]\u001b[A\n",
            " 77% 671/870 [01:51<02:06,  1.58it/s]\u001b[A\n",
            " 77% 672/870 [01:52<02:05,  1.58it/s]\u001b[A\n",
            " 77% 673/870 [01:52<02:04,  1.58it/s]\u001b[A\n",
            " 77% 674/870 [01:53<02:04,  1.58it/s]\u001b[A\n",
            " 78% 675/870 [01:54<02:03,  1.58it/s]\u001b[A\n",
            " 78% 676/870 [01:54<02:02,  1.58it/s]\u001b[A\n",
            " 78% 677/870 [01:55<02:02,  1.58it/s]\u001b[A\n",
            " 78% 678/870 [01:55<02:01,  1.58it/s]\u001b[A\n",
            " 78% 679/870 [01:56<02:01,  1.58it/s]\u001b[A\n",
            " 78% 680/870 [01:57<02:00,  1.58it/s]\u001b[A\n",
            " 78% 681/870 [01:57<01:59,  1.58it/s]\u001b[A\n",
            " 78% 682/870 [01:58<01:59,  1.58it/s]\u001b[A\n",
            " 79% 683/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 684/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 685/870 [02:00<01:57,  1.57it/s]\u001b[A\n",
            " 79% 686/870 [02:01<01:56,  1.58it/s]\u001b[A\n",
            " 79% 687/870 [02:01<01:56,  1.58it/s]\u001b[A\n",
            " 79% 688/870 [02:02<01:55,  1.57it/s]\u001b[A\n",
            " 79% 689/870 [02:02<01:55,  1.57it/s]\u001b[A\n",
            " 79% 690/870 [02:03<01:54,  1.57it/s]\u001b[A\n",
            " 79% 691/870 [02:04<01:53,  1.58it/s]\u001b[A\n",
            " 80% 692/870 [02:04<01:52,  1.58it/s]\u001b[A\n",
            " 80% 693/870 [02:05<01:52,  1.58it/s]\u001b[A\n",
            " 80% 694/870 [02:06<01:51,  1.58it/s]\u001b[A\n",
            " 80% 695/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 696/870 [02:07<01:50,  1.57it/s]\u001b[A\n",
            " 80% 697/870 [02:08<01:49,  1.57it/s]\u001b[A\n",
            " 80% 698/870 [02:08<01:49,  1.57it/s]\u001b[A\n",
            " 80% 699/870 [02:09<01:48,  1.58it/s]\u001b[A\n",
            " 80% 700/870 [02:09<01:47,  1.58it/s]\u001b[A\n",
            " 81% 701/870 [02:10<01:47,  1.57it/s]\u001b[A\n",
            " 81% 702/870 [02:11<01:46,  1.57it/s]\u001b[A\n",
            " 81% 703/870 [02:11<01:46,  1.57it/s]\u001b[A\n",
            " 81% 704/870 [02:12<01:45,  1.57it/s]\u001b[A\n",
            " 81% 705/870 [02:13<01:44,  1.58it/s]\u001b[A\n",
            " 81% 706/870 [02:13<01:44,  1.58it/s]\u001b[A\n",
            " 81% 707/870 [02:14<01:43,  1.58it/s]\u001b[A\n",
            " 81% 708/870 [02:14<01:42,  1.58it/s]\u001b[A\n",
            " 81% 709/870 [02:15<01:42,  1.58it/s]\u001b[A\n",
            " 82% 710/870 [02:16<01:41,  1.58it/s]\u001b[A\n",
            " 82% 711/870 [02:16<01:40,  1.58it/s]\u001b[A\n",
            " 82% 712/870 [02:17<01:40,  1.58it/s]\u001b[A\n",
            " 82% 713/870 [02:18<01:39,  1.58it/s]\u001b[A\n",
            " 82% 714/870 [02:18<01:38,  1.58it/s]\u001b[A\n",
            " 82% 715/870 [02:19<01:38,  1.58it/s]\u001b[A\n",
            " 82% 716/870 [02:20<01:37,  1.58it/s]\u001b[A\n",
            " 82% 717/870 [02:20<01:37,  1.58it/s]\u001b[A\n",
            " 83% 718/870 [02:21<01:36,  1.58it/s]\u001b[A\n",
            " 83% 719/870 [02:21<01:35,  1.58it/s]\u001b[A\n",
            " 83% 720/870 [02:22<01:35,  1.58it/s]\u001b[A\n",
            " 83% 721/870 [02:23<01:34,  1.58it/s]\u001b[A\n",
            " 83% 722/870 [02:23<01:33,  1.58it/s]\u001b[A\n",
            " 83% 723/870 [02:24<01:33,  1.58it/s]\u001b[A\n",
            " 83% 724/870 [02:25<01:32,  1.58it/s]\u001b[A\n",
            " 83% 725/870 [02:25<01:31,  1.58it/s]\u001b[A\n",
            " 83% 726/870 [02:26<01:31,  1.58it/s]\u001b[A\n",
            " 84% 727/870 [02:27<01:30,  1.58it/s]\u001b[A\n",
            " 84% 728/870 [02:27<01:29,  1.58it/s]\u001b[A\n",
            " 84% 729/870 [02:28<01:29,  1.58it/s]\u001b[A\n",
            " 84% 730/870 [02:28<01:28,  1.58it/s]\u001b[A\n",
            " 84% 731/870 [02:29<01:28,  1.58it/s]\u001b[A\n",
            " 84% 732/870 [02:30<01:27,  1.58it/s]\u001b[A\n",
            " 84% 733/870 [02:30<01:26,  1.58it/s]\u001b[A\n",
            " 84% 734/870 [02:31<01:26,  1.58it/s]\u001b[A\n",
            " 84% 735/870 [02:32<01:25,  1.58it/s]\u001b[A\n",
            " 85% 736/870 [02:32<01:24,  1.58it/s]\u001b[A\n",
            " 85% 737/870 [02:33<01:24,  1.58it/s]\u001b[A\n",
            " 85% 738/870 [02:34<01:23,  1.58it/s]\u001b[A\n",
            " 85% 739/870 [02:34<01:22,  1.58it/s]\u001b[A\n",
            " 85% 740/870 [02:35<01:22,  1.58it/s]\u001b[A\n",
            " 85% 741/870 [02:35<01:21,  1.58it/s]\u001b[A\n",
            " 85% 742/870 [02:36<01:20,  1.58it/s]\u001b[A\n",
            " 85% 743/870 [02:37<01:20,  1.58it/s]\u001b[A\n",
            " 86% 744/870 [02:37<01:19,  1.58it/s]\u001b[A\n",
            " 86% 745/870 [02:38<01:19,  1.58it/s]\u001b[A\n",
            " 86% 746/870 [02:39<01:18,  1.58it/s]\u001b[A\n",
            " 86% 747/870 [02:39<01:17,  1.58it/s]\u001b[A\n",
            " 86% 748/870 [02:40<01:17,  1.58it/s]\u001b[A\n",
            " 86% 749/870 [02:40<01:16,  1.58it/s]\u001b[A\n",
            " 86% 750/870 [02:41<01:15,  1.58it/s]\u001b[A\n",
            " 86% 751/870 [02:42<01:15,  1.58it/s]\u001b[A\n",
            " 86% 752/870 [02:42<01:14,  1.58it/s]\u001b[A\n",
            " 87% 753/870 [02:43<01:14,  1.58it/s]\u001b[A\n",
            " 87% 754/870 [02:44<01:13,  1.58it/s]\u001b[A\n",
            " 87% 755/870 [02:44<01:12,  1.58it/s]\u001b[A\n",
            " 87% 756/870 [02:45<01:12,  1.58it/s]\u001b[A\n",
            " 87% 757/870 [02:46<01:11,  1.58it/s]\u001b[A\n",
            " 87% 758/870 [02:46<01:10,  1.58it/s]\u001b[A\n",
            " 87% 759/870 [02:47<01:10,  1.58it/s]\u001b[A\n",
            " 87% 760/870 [02:47<01:09,  1.58it/s]\u001b[A\n",
            " 87% 761/870 [02:48<01:09,  1.58it/s]\u001b[A\n",
            " 88% 762/870 [02:49<01:08,  1.58it/s]\u001b[A\n",
            " 88% 763/870 [02:49<01:07,  1.58it/s]\u001b[A\n",
            " 88% 764/870 [02:50<01:07,  1.58it/s]\u001b[A\n",
            " 88% 765/870 [02:51<01:06,  1.58it/s]\u001b[A\n",
            " 88% 766/870 [02:51<01:05,  1.58it/s]\u001b[A\n",
            " 88% 767/870 [02:52<01:05,  1.58it/s]\u001b[A\n",
            " 88% 768/870 [02:53<01:04,  1.58it/s]\u001b[A\n",
            " 88% 769/870 [02:53<01:04,  1.58it/s]\u001b[A\n",
            " 89% 770/870 [02:54<01:03,  1.58it/s]\u001b[A\n",
            " 89% 771/870 [02:54<01:02,  1.58it/s]\u001b[A\n",
            " 89% 772/870 [02:55<01:02,  1.58it/s]\u001b[A\n",
            " 89% 773/870 [02:56<01:01,  1.58it/s]\u001b[A\n",
            " 89% 774/870 [02:56<01:00,  1.58it/s]\u001b[A\n",
            " 89% 775/870 [02:57<01:00,  1.58it/s]\u001b[A\n",
            " 89% 776/870 [02:58<00:59,  1.58it/s]\u001b[A\n",
            " 89% 777/870 [02:58<00:58,  1.58it/s]\u001b[A\n",
            " 89% 778/870 [02:59<00:58,  1.58it/s]\u001b[A\n",
            " 90% 779/870 [02:59<00:57,  1.58it/s]\u001b[A\n",
            " 90% 780/870 [03:00<00:57,  1.58it/s]\u001b[A\n",
            " 90% 781/870 [03:01<00:56,  1.58it/s]\u001b[A\n",
            " 90% 782/870 [03:01<00:55,  1.58it/s]\u001b[A\n",
            " 90% 783/870 [03:02<00:55,  1.58it/s]\u001b[A\n",
            " 90% 784/870 [03:03<00:54,  1.58it/s]\u001b[A\n",
            " 90% 785/870 [03:03<00:53,  1.58it/s]\u001b[A\n",
            " 90% 786/870 [03:04<00:53,  1.58it/s]\u001b[A\n",
            " 90% 787/870 [03:05<00:52,  1.58it/s]\u001b[A\n",
            " 91% 788/870 [03:05<00:51,  1.58it/s]\u001b[A\n",
            " 91% 789/870 [03:06<00:51,  1.58it/s]\u001b[A\n",
            " 91% 790/870 [03:06<00:50,  1.58it/s]\u001b[A\n",
            " 91% 791/870 [03:07<00:50,  1.58it/s]\u001b[A\n",
            " 91% 792/870 [03:08<00:49,  1.58it/s]\u001b[A\n",
            " 91% 793/870 [03:08<00:48,  1.58it/s]\u001b[A\n",
            " 91% 794/870 [03:09<00:48,  1.58it/s]\u001b[A\n",
            " 91% 795/870 [03:10<00:47,  1.58it/s]\u001b[A\n",
            " 91% 796/870 [03:10<00:46,  1.58it/s]\u001b[A\n",
            " 92% 797/870 [03:11<00:46,  1.58it/s]\u001b[A\n",
            " 92% 798/870 [03:12<00:45,  1.58it/s]\u001b[A\n",
            " 92% 799/870 [03:12<00:45,  1.58it/s]\u001b[A\n",
            " 92% 800/870 [03:13<00:44,  1.58it/s]\u001b[A\n",
            " 92% 801/870 [03:13<00:43,  1.58it/s]\u001b[A\n",
            " 92% 802/870 [03:14<00:43,  1.58it/s]\u001b[A\n",
            " 92% 803/870 [03:15<00:42,  1.58it/s]\u001b[A\n",
            " 92% 804/870 [03:15<00:41,  1.58it/s]\u001b[A\n",
            " 93% 805/870 [03:16<00:41,  1.58it/s]\u001b[A\n",
            " 93% 806/870 [03:17<00:40,  1.58it/s]\u001b[A\n",
            " 93% 807/870 [03:17<00:39,  1.58it/s]\u001b[A\n",
            " 93% 808/870 [03:18<00:39,  1.58it/s]\u001b[A\n",
            " 93% 809/870 [03:19<00:38,  1.58it/s]\u001b[A\n",
            " 93% 810/870 [03:19<00:38,  1.58it/s]\u001b[A\n",
            " 93% 811/870 [03:20<00:37,  1.58it/s]\u001b[A\n",
            " 93% 812/870 [03:20<00:36,  1.58it/s]\u001b[A\n",
            " 93% 813/870 [03:21<00:36,  1.58it/s]\u001b[A\n",
            " 94% 814/870 [03:22<00:35,  1.58it/s]\u001b[A\n",
            " 94% 815/870 [03:22<00:34,  1.58it/s]\u001b[A\n",
            " 94% 816/870 [03:23<00:34,  1.58it/s]\u001b[A\n",
            " 94% 817/870 [03:24<00:33,  1.58it/s]\u001b[A\n",
            " 94% 818/870 [03:24<00:32,  1.58it/s]\u001b[A\n",
            " 94% 819/870 [03:25<00:32,  1.58it/s]\u001b[A\n",
            " 94% 820/870 [03:26<00:31,  1.56it/s]\u001b[A\n",
            " 94% 821/870 [03:26<00:31,  1.56it/s]\u001b[A\n",
            " 94% 822/870 [03:27<00:30,  1.57it/s]\u001b[A\n",
            " 95% 823/870 [03:27<00:29,  1.57it/s]\u001b[A\n",
            " 95% 824/870 [03:28<00:29,  1.57it/s]\u001b[A\n",
            " 95% 825/870 [03:29<00:28,  1.57it/s]\u001b[A\n",
            " 95% 826/870 [03:29<00:27,  1.57it/s]\u001b[A\n",
            " 95% 827/870 [03:30<00:27,  1.57it/s]\u001b[A\n",
            " 95% 828/870 [03:31<00:26,  1.58it/s]\u001b[A\n",
            " 95% 829/870 [03:31<00:26,  1.58it/s]\u001b[A\n",
            " 95% 830/870 [03:32<00:25,  1.58it/s]\u001b[A\n",
            " 96% 831/870 [03:32<00:24,  1.58it/s]\u001b[A\n",
            " 96% 832/870 [03:33<00:24,  1.58it/s]\u001b[A\n",
            " 96% 833/870 [03:34<00:23,  1.57it/s]\u001b[A\n",
            " 96% 834/870 [03:34<00:22,  1.57it/s]\u001b[A\n",
            " 96% 835/870 [03:35<00:22,  1.58it/s]\u001b[A\n",
            " 96% 836/870 [03:36<00:21,  1.58it/s]\u001b[A\n",
            " 96% 837/870 [03:36<00:20,  1.58it/s]\u001b[A\n",
            " 96% 838/870 [03:37<00:20,  1.58it/s]\u001b[A\n",
            " 96% 839/870 [03:38<00:19,  1.58it/s]\u001b[A\n",
            " 97% 840/870 [03:38<00:19,  1.58it/s]\u001b[A\n",
            " 97% 841/870 [03:39<00:18,  1.58it/s]\u001b[A\n",
            " 97% 842/870 [03:39<00:17,  1.58it/s]\u001b[A\n",
            " 97% 843/870 [03:40<00:17,  1.57it/s]\u001b[A\n",
            " 97% 844/870 [03:41<00:16,  1.57it/s]\u001b[A\n",
            " 97% 845/870 [03:41<00:15,  1.57it/s]\u001b[A\n",
            " 97% 846/870 [03:42<00:15,  1.58it/s]\u001b[A\n",
            " 97% 847/870 [03:43<00:14,  1.58it/s]\u001b[A\n",
            " 97% 848/870 [03:43<00:13,  1.58it/s]\u001b[A\n",
            " 98% 849/870 [03:44<00:13,  1.58it/s]\u001b[A\n",
            " 98% 850/870 [03:45<00:12,  1.58it/s]\u001b[A\n",
            " 98% 851/870 [03:45<00:12,  1.58it/s]\u001b[A\n",
            " 98% 852/870 [03:46<00:11,  1.58it/s]\u001b[A\n",
            " 98% 853/870 [03:46<00:10,  1.58it/s]\u001b[A\n",
            " 98% 854/870 [03:47<00:10,  1.58it/s]\u001b[A\n",
            " 98% 855/870 [03:48<00:09,  1.57it/s]\u001b[A\n",
            " 98% 856/870 [03:48<00:08,  1.57it/s]\u001b[A\n",
            " 99% 857/870 [03:49<00:08,  1.57it/s]\u001b[A\n",
            " 99% 858/870 [03:50<00:07,  1.57it/s]\u001b[A\n",
            " 99% 859/870 [03:50<00:06,  1.58it/s]\u001b[A\n",
            " 99% 860/870 [03:51<00:06,  1.58it/s]\u001b[A\n",
            " 99% 861/870 [03:52<00:05,  1.58it/s]\u001b[A\n",
            " 99% 862/870 [03:52<00:05,  1.58it/s]\u001b[A\n",
            " 99% 863/870 [03:53<00:04,  1.57it/s]\u001b[A\n",
            " 99% 864/870 [03:53<00:03,  1.57it/s]\u001b[A\n",
            " 99% 865/870 [03:54<00:03,  1.57it/s]\u001b[A\n",
            "100% 866/870 [03:55<00:02,  1.57it/s]\u001b[A\n",
            "100% 867/870 [03:55<00:01,  1.57it/s]\u001b[A\n",
            "100% 868/870 [03:56<00:01,  1.57it/s]\u001b[A\n",
            "100% 869/870 [03:57<00:00,  1.57it/s]\u001b[A\n",
            "100% 870/870 [03:57<00:00,  1.70it/s]\u001b[A[INFO|trainer.py:1873] 2022-11-08 22:19:29,835 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A{'train_runtime': 237.6009, 'train_samples_per_second': 29.268, 'train_steps_per_second': 3.662, 'train_loss': 2.416890321928879, 'epoch': 3.0}\n",
            "\n",
            "100% 870/870 [03:57<00:00,  3.66it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-08 22:19:29,837 >> Saving model checkpoint to /content/test-clm/gpt-2-0.5\n",
            "[INFO|configuration_utils.py:447] 2022-11-08 22:19:29,838 >> Configuration saved in /content/test-clm/gpt-2-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-08 22:19:31,323 >> Model weights saved in /content/test-clm/gpt-2-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-08 22:19:31,324 >> tokenizer config file saved in /content/test-clm/gpt-2-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-08 22:19:31,324 >> Special tokens file saved in /content/test-clm/gpt-2-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.4169\n",
            "  train_runtime            = 0:03:57.60\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     29.268\n",
            "  train_steps_per_second   =      3.662\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-08 22:19:31,430 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-08 22:19:31,430 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-08 22:19:31,430 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.10it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.52\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.887\n",
            "  eval_steps_per_second   =      3.986\n",
            "  perplexity              =   296.4128\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.9/runs/Nov08_22-19-44_2921ed0a3794,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.9,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.9,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:Checkpoint detected, resuming training at /content/test-clm/gpt-2-0.9/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 747.07it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-08 22:19:44,917 >> loading configuration file /content/models/gpt-2-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:19:44,918 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.9\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-08 22:19:45,014 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:19:45,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:19:45,109 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,300 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,300 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,300 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,300 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,301 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:19:45,301 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:19:45,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:19:45,302 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-08 22:19:45,367 >> loading weights file /content/models/gpt-2-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-08 22:19:47,857 >> Some weights of the model checkpoint at /content/models/gpt-2-0.9 were not used when initializing GPT2LMHeadModel: ['transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-08 22:19:47,857 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.9 and are newly initialized: ['transformer.h.3.mlp.c_fc.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.5.attn.c_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "[INFO|trainer.py:1944] 2022-11-08 22:19:50,025 >> Loading model from /content/test-clm/gpt-2-0.9/checkpoint-500.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-08 22:19:51,026 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-08 22:19:51,026 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-08 22:19:51,026 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-08 22:19:51,026 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-08 22:19:51,026 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-08 22:19:51,026 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-08 22:19:51,026 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-08 22:19:51,027 >>   Number of trainable parameters = 124439808\n",
            "[INFO|trainer.py:1651] 2022-11-08 22:19:51,027 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1652] 2022-11-08 22:19:51,027 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1653] 2022-11-08 22:19:51,027 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1656] 2022-11-08 22:19:51,027 >>   Will skip the first 1 epochs then the first 210 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/210 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 210/210 [00:02<00:00, 98.57it/s] \n",
            "\n",
            " 58% 501/870 [00:03<00:02, 133.68it/s]\u001b[A\n",
            " 59% 515/870 [00:12<00:11, 31.69it/s] \u001b[A\n",
            " 60% 521/870 [00:16<00:15, 22.02it/s]\u001b[A\n",
            " 60% 525/870 [00:18<00:20, 17.24it/s]\u001b[A\n",
            " 61% 527/870 [00:20<00:22, 14.99it/s]\u001b[A\n",
            " 61% 529/870 [00:21<00:26, 12.67it/s]\u001b[A\n",
            " 61% 530/870 [00:22<00:29, 11.47it/s]\u001b[A\n",
            " 61% 531/870 [00:22<00:33, 10.15it/s]\u001b[A\n",
            " 61% 532/870 [00:23<00:38,  8.78it/s]\u001b[A\n",
            " 61% 533/870 [00:24<00:45,  7.44it/s]\u001b[A\n",
            " 61% 534/870 [00:24<00:54,  6.21it/s]\u001b[A\n",
            " 61% 535/870 [00:25<01:05,  5.14it/s]\u001b[A\n",
            " 62% 536/870 [00:25<01:18,  4.25it/s]\u001b[A\n",
            " 62% 537/870 [00:26<01:33,  3.55it/s]\u001b[A\n",
            " 62% 538/870 [00:27<01:50,  3.02it/s]\u001b[A\n",
            " 62% 539/870 [00:27<02:06,  2.61it/s]\u001b[A\n",
            " 62% 540/870 [00:28<02:22,  2.31it/s]\u001b[A\n",
            " 62% 541/870 [00:29<02:36,  2.10it/s]\u001b[A\n",
            " 62% 542/870 [00:29<02:48,  1.95it/s]\u001b[A\n",
            " 62% 543/870 [00:30<02:57,  1.84it/s]\u001b[A\n",
            " 63% 544/870 [00:31<03:05,  1.76it/s]\u001b[A\n",
            " 63% 545/870 [00:31<03:10,  1.71it/s]\u001b[A\n",
            " 63% 546/870 [00:32<03:14,  1.67it/s]\u001b[A\n",
            " 63% 547/870 [00:32<03:16,  1.64it/s]\u001b[A\n",
            " 63% 548/870 [00:33<03:19,  1.62it/s]\u001b[A\n",
            " 63% 549/870 [00:34<03:19,  1.61it/s]\u001b[A\n",
            " 63% 550/870 [00:34<03:20,  1.60it/s]\u001b[A\n",
            " 63% 551/870 [00:35<03:20,  1.59it/s]\u001b[A\n",
            " 63% 552/870 [00:36<03:20,  1.59it/s]\u001b[A\n",
            " 64% 553/870 [00:36<03:20,  1.58it/s]\u001b[A\n",
            " 64% 554/870 [00:37<03:19,  1.58it/s]\u001b[A\n",
            " 64% 555/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 556/870 [00:38<03:18,  1.58it/s]\u001b[A\n",
            " 64% 557/870 [00:39<03:18,  1.58it/s]\u001b[A\n",
            " 64% 558/870 [00:39<03:17,  1.58it/s]\u001b[A\n",
            " 64% 559/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 560/870 [00:41<03:16,  1.58it/s]\u001b[A\n",
            " 64% 561/870 [00:41<03:15,  1.58it/s]\u001b[A\n",
            " 65% 562/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 563/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 564/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 565/870 [00:44<03:13,  1.58it/s]\u001b[A\n",
            " 65% 566/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 567/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 568/870 [00:46<03:11,  1.58it/s]\u001b[A\n",
            " 65% 569/870 [00:46<03:10,  1.58it/s]\u001b[A\n",
            " 66% 570/870 [00:47<03:10,  1.58it/s]\u001b[A\n",
            " 66% 571/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 572/870 [00:48<03:08,  1.58it/s]\u001b[A\n",
            " 66% 573/870 [00:49<03:08,  1.58it/s]\u001b[A\n",
            " 66% 574/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 575/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 576/870 [00:51<03:06,  1.58it/s]\u001b[A\n",
            " 66% 577/870 [00:51<03:05,  1.58it/s]\u001b[A\n",
            " 66% 578/870 [00:52<03:05,  1.58it/s]\u001b[A\n",
            " 67% 579/870 [00:53<03:04,  1.58it/s]\u001b[A\n",
            " 67% 580/870 [00:53<02:50,  1.70it/s]\u001b[A\n",
            " 67% 581/870 [00:54<02:54,  1.66it/s]\u001b[A\n",
            " 67% 582/870 [00:55<02:56,  1.63it/s]\u001b[A\n",
            " 67% 583/870 [00:55<02:57,  1.62it/s]\u001b[A\n",
            " 67% 584/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 585/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 586/870 [00:57<02:58,  1.59it/s]\u001b[A\n",
            " 67% 587/870 [00:58<02:58,  1.59it/s]\u001b[A\n",
            " 68% 588/870 [00:58<02:58,  1.58it/s]\u001b[A\n",
            " 68% 589/870 [00:59<02:57,  1.58it/s]\u001b[A\n",
            " 68% 590/870 [01:00<02:57,  1.58it/s]\u001b[A\n",
            " 68% 591/870 [01:00<02:56,  1.58it/s]\u001b[A\n",
            " 68% 592/870 [01:01<02:56,  1.58it/s]\u001b[A\n",
            " 68% 593/870 [01:01<02:55,  1.58it/s]\u001b[A\n",
            " 68% 594/870 [01:02<02:54,  1.58it/s]\u001b[A\n",
            " 68% 595/870 [01:03<02:54,  1.58it/s]\u001b[A\n",
            " 69% 596/870 [01:03<02:53,  1.58it/s]\u001b[A\n",
            " 69% 597/870 [01:04<02:53,  1.58it/s]\u001b[A\n",
            " 69% 598/870 [01:05<02:52,  1.58it/s]\u001b[A\n",
            " 69% 599/870 [01:05<02:51,  1.58it/s]\u001b[A\n",
            " 69% 600/870 [01:06<02:51,  1.58it/s]\u001b[A\n",
            " 69% 601/870 [01:07<02:50,  1.58it/s]\u001b[A\n",
            " 69% 602/870 [01:07<02:49,  1.58it/s]\u001b[A\n",
            " 69% 603/870 [01:08<02:49,  1.58it/s]\u001b[A\n",
            " 69% 604/870 [01:08<02:48,  1.58it/s]\u001b[A\n",
            " 70% 605/870 [01:09<02:47,  1.58it/s]\u001b[A\n",
            " 70% 606/870 [01:10<02:47,  1.58it/s]\u001b[A\n",
            " 70% 607/870 [01:10<02:46,  1.58it/s]\u001b[A\n",
            " 70% 608/870 [01:11<02:46,  1.58it/s]\u001b[A\n",
            " 70% 609/870 [01:12<02:45,  1.58it/s]\u001b[A\n",
            " 70% 610/870 [01:12<02:44,  1.58it/s]\u001b[A\n",
            " 70% 611/870 [01:13<02:44,  1.58it/s]\u001b[A\n",
            " 70% 612/870 [01:14<02:43,  1.58it/s]\u001b[A\n",
            " 70% 613/870 [01:14<02:42,  1.58it/s]\u001b[A\n",
            " 71% 614/870 [01:15<02:42,  1.58it/s]\u001b[A\n",
            " 71% 615/870 [01:15<02:41,  1.58it/s]\u001b[A\n",
            " 71% 616/870 [01:16<02:40,  1.58it/s]\u001b[A\n",
            " 71% 617/870 [01:17<02:40,  1.58it/s]\u001b[A\n",
            " 71% 618/870 [01:17<02:39,  1.58it/s]\u001b[A\n",
            " 71% 619/870 [01:18<02:38,  1.58it/s]\u001b[A\n",
            " 71% 620/870 [01:19<02:38,  1.58it/s]\u001b[A\n",
            " 71% 621/870 [01:19<02:37,  1.58it/s]\u001b[A\n",
            " 71% 622/870 [01:20<02:36,  1.58it/s]\u001b[A\n",
            " 72% 623/870 [01:20<02:36,  1.58it/s]\u001b[A\n",
            " 72% 624/870 [01:21<02:35,  1.58it/s]\u001b[A\n",
            " 72% 625/870 [01:22<02:35,  1.58it/s]\u001b[A\n",
            " 72% 626/870 [01:22<02:34,  1.58it/s]\u001b[A\n",
            " 72% 627/870 [01:23<02:34,  1.57it/s]\u001b[A\n",
            " 72% 628/870 [01:24<02:33,  1.58it/s]\u001b[A\n",
            " 72% 629/870 [01:24<02:32,  1.58it/s]\u001b[A\n",
            " 72% 630/870 [01:25<02:32,  1.58it/s]\u001b[A\n",
            " 73% 631/870 [01:26<02:31,  1.58it/s]\u001b[A\n",
            " 73% 632/870 [01:26<02:30,  1.58it/s]\u001b[A\n",
            " 73% 633/870 [01:27<02:30,  1.58it/s]\u001b[A\n",
            " 73% 634/870 [01:27<02:29,  1.58it/s]\u001b[A\n",
            " 73% 635/870 [01:28<02:29,  1.58it/s]\u001b[A\n",
            " 73% 636/870 [01:29<02:28,  1.58it/s]\u001b[A\n",
            " 73% 637/870 [01:29<02:27,  1.58it/s]\u001b[A\n",
            " 73% 638/870 [01:30<02:27,  1.58it/s]\u001b[A\n",
            " 73% 639/870 [01:31<02:26,  1.58it/s]\u001b[A\n",
            " 74% 640/870 [01:31<02:26,  1.58it/s]\u001b[A\n",
            " 74% 641/870 [01:32<02:25,  1.57it/s]\u001b[A\n",
            " 74% 642/870 [01:33<02:24,  1.57it/s]\u001b[A\n",
            " 74% 643/870 [01:33<02:24,  1.57it/s]\u001b[A\n",
            " 74% 644/870 [01:34<02:23,  1.57it/s]\u001b[A\n",
            " 74% 645/870 [01:34<02:22,  1.57it/s]\u001b[A\n",
            " 74% 646/870 [01:35<02:22,  1.58it/s]\u001b[A\n",
            " 74% 647/870 [01:36<02:21,  1.58it/s]\u001b[A\n",
            " 74% 648/870 [01:36<02:20,  1.58it/s]\u001b[A\n",
            " 75% 649/870 [01:37<02:20,  1.58it/s]\u001b[A\n",
            " 75% 650/870 [01:38<02:19,  1.58it/s]\u001b[A\n",
            " 75% 651/870 [01:38<02:18,  1.58it/s]\u001b[A\n",
            " 75% 652/870 [01:39<02:18,  1.57it/s]\u001b[A\n",
            " 75% 653/870 [01:40<02:18,  1.57it/s]\u001b[A\n",
            " 75% 654/870 [01:40<02:17,  1.57it/s]\u001b[A\n",
            " 75% 655/870 [01:41<02:16,  1.57it/s]\u001b[A\n",
            " 75% 656/870 [01:41<02:15,  1.57it/s]\u001b[A\n",
            " 76% 657/870 [01:42<02:15,  1.57it/s]\u001b[A\n",
            " 76% 658/870 [01:43<02:15,  1.57it/s]\u001b[A\n",
            " 76% 659/870 [01:43<02:14,  1.57it/s]\u001b[A\n",
            " 76% 660/870 [01:44<02:13,  1.57it/s]\u001b[A\n",
            " 76% 661/870 [01:45<02:12,  1.57it/s]\u001b[A\n",
            " 76% 662/870 [01:45<02:12,  1.57it/s]\u001b[A\n",
            " 76% 663/870 [01:46<02:11,  1.58it/s]\u001b[A\n",
            " 76% 664/870 [01:47<02:10,  1.57it/s]\u001b[A\n",
            " 76% 665/870 [01:47<02:10,  1.58it/s]\u001b[A\n",
            " 77% 666/870 [01:48<02:09,  1.58it/s]\u001b[A\n",
            " 77% 667/870 [01:48<02:08,  1.58it/s]\u001b[A\n",
            " 77% 668/870 [01:49<02:08,  1.58it/s]\u001b[A\n",
            " 77% 669/870 [01:50<02:07,  1.58it/s]\u001b[A\n",
            " 77% 670/870 [01:50<02:06,  1.58it/s]\u001b[A\n",
            " 77% 671/870 [01:51<02:06,  1.58it/s]\u001b[A\n",
            " 77% 672/870 [01:52<02:05,  1.58it/s]\u001b[A\n",
            " 77% 673/870 [01:52<02:04,  1.58it/s]\u001b[A\n",
            " 77% 674/870 [01:53<02:04,  1.58it/s]\u001b[A\n",
            " 78% 675/870 [01:54<02:03,  1.58it/s]\u001b[A\n",
            " 78% 676/870 [01:54<02:03,  1.58it/s]\u001b[A\n",
            " 78% 677/870 [01:55<02:02,  1.58it/s]\u001b[A\n",
            " 78% 678/870 [01:55<02:01,  1.58it/s]\u001b[A\n",
            " 78% 679/870 [01:56<02:01,  1.58it/s]\u001b[A\n",
            " 78% 680/870 [01:57<02:00,  1.58it/s]\u001b[A\n",
            " 78% 681/870 [01:57<01:59,  1.58it/s]\u001b[A\n",
            " 78% 682/870 [01:58<01:59,  1.58it/s]\u001b[A\n",
            " 79% 683/870 [01:59<01:58,  1.58it/s]\u001b[A\n",
            " 79% 684/870 [01:59<01:57,  1.58it/s]\u001b[A\n",
            " 79% 685/870 [02:00<01:57,  1.58it/s]\u001b[A\n",
            " 79% 686/870 [02:00<01:56,  1.58it/s]\u001b[A\n",
            " 79% 687/870 [02:01<01:56,  1.58it/s]\u001b[A\n",
            " 79% 688/870 [02:02<01:55,  1.58it/s]\u001b[A\n",
            " 79% 689/870 [02:02<01:54,  1.58it/s]\u001b[A\n",
            " 79% 690/870 [02:03<01:54,  1.57it/s]\u001b[A\n",
            " 79% 691/870 [02:04<01:53,  1.57it/s]\u001b[A\n",
            " 80% 692/870 [02:04<01:53,  1.57it/s]\u001b[A\n",
            " 80% 693/870 [02:05<01:52,  1.57it/s]\u001b[A\n",
            " 80% 694/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 695/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 696/870 [02:07<01:50,  1.57it/s]\u001b[A\n",
            " 80% 697/870 [02:07<01:49,  1.57it/s]\u001b[A\n",
            " 80% 698/870 [02:08<01:49,  1.58it/s]\u001b[A\n",
            " 80% 699/870 [02:09<01:48,  1.58it/s]\u001b[A\n",
            " 80% 700/870 [02:09<01:47,  1.58it/s]\u001b[A\n",
            " 81% 701/870 [02:10<01:47,  1.58it/s]\u001b[A\n",
            " 81% 702/870 [02:11<01:46,  1.58it/s]\u001b[A\n",
            " 81% 703/870 [02:11<01:45,  1.58it/s]\u001b[A\n",
            " 81% 704/870 [02:12<01:45,  1.57it/s]\u001b[A\n",
            " 81% 705/870 [02:13<01:44,  1.58it/s]\u001b[A\n",
            " 81% 706/870 [02:13<01:44,  1.58it/s]\u001b[A\n",
            " 81% 707/870 [02:14<01:43,  1.58it/s]\u001b[A\n",
            " 81% 708/870 [02:14<01:43,  1.57it/s]\u001b[A\n",
            " 81% 709/870 [02:15<01:42,  1.57it/s]\u001b[A\n",
            " 82% 710/870 [02:16<01:41,  1.57it/s]\u001b[A\n",
            " 82% 711/870 [02:16<01:40,  1.57it/s]\u001b[A\n",
            " 82% 712/870 [02:17<01:40,  1.57it/s]\u001b[A\n",
            " 82% 713/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 714/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 715/870 [02:19<01:38,  1.57it/s]\u001b[A\n",
            " 82% 716/870 [02:20<01:37,  1.58it/s]\u001b[A\n",
            " 82% 717/870 [02:20<01:37,  1.58it/s]\u001b[A\n",
            " 83% 718/870 [02:21<01:36,  1.58it/s]\u001b[A\n",
            " 83% 719/870 [02:21<01:35,  1.58it/s]\u001b[A\n",
            " 83% 720/870 [02:22<01:35,  1.58it/s]\u001b[A\n",
            " 83% 721/870 [02:23<01:34,  1.57it/s]\u001b[A\n",
            " 83% 722/870 [02:23<01:34,  1.57it/s]\u001b[A\n",
            " 83% 723/870 [02:24<01:33,  1.57it/s]\u001b[A\n",
            " 83% 724/870 [02:25<01:32,  1.57it/s]\u001b[A\n",
            " 83% 725/870 [02:25<01:32,  1.57it/s]\u001b[A\n",
            " 83% 726/870 [02:26<01:31,  1.57it/s]\u001b[A\n",
            " 84% 727/870 [02:27<01:31,  1.57it/s]\u001b[A\n",
            " 84% 728/870 [02:27<01:30,  1.57it/s]\u001b[A\n",
            " 84% 729/870 [02:28<01:29,  1.57it/s]\u001b[A\n",
            " 84% 730/870 [02:28<01:28,  1.57it/s]\u001b[A\n",
            " 84% 731/870 [02:29<01:28,  1.57it/s]\u001b[A\n",
            " 84% 732/870 [02:30<01:27,  1.58it/s]\u001b[A\n",
            " 84% 733/870 [02:30<01:26,  1.58it/s]\u001b[A\n",
            " 84% 734/870 [02:31<01:26,  1.57it/s]\u001b[A\n",
            " 84% 735/870 [02:32<01:25,  1.58it/s]\u001b[A\n",
            " 85% 736/870 [02:32<01:25,  1.58it/s]\u001b[A\n",
            " 85% 737/870 [02:33<01:24,  1.57it/s]\u001b[A\n",
            " 85% 738/870 [02:34<01:24,  1.57it/s]\u001b[A\n",
            " 85% 739/870 [02:34<01:23,  1.57it/s]\u001b[A\n",
            " 85% 740/870 [02:35<01:22,  1.57it/s]\u001b[A\n",
            " 85% 741/870 [02:35<01:22,  1.57it/s]\u001b[A\n",
            " 85% 742/870 [02:36<01:21,  1.57it/s]\u001b[A\n",
            " 85% 743/870 [02:37<01:20,  1.57it/s]\u001b[A\n",
            " 86% 744/870 [02:37<01:19,  1.58it/s]\u001b[A\n",
            " 86% 745/870 [02:38<01:19,  1.57it/s]\u001b[A\n",
            " 86% 746/870 [02:39<01:18,  1.57it/s]\u001b[A\n",
            " 86% 747/870 [02:39<01:18,  1.57it/s]\u001b[A\n",
            " 86% 748/870 [02:40<01:17,  1.57it/s]\u001b[A\n",
            " 86% 749/870 [02:41<01:16,  1.57it/s]\u001b[A\n",
            " 86% 750/870 [02:41<01:16,  1.58it/s]\u001b[A\n",
            " 86% 751/870 [02:42<01:15,  1.58it/s]\u001b[A\n",
            " 86% 752/870 [02:42<01:14,  1.58it/s]\u001b[A\n",
            " 87% 753/870 [02:43<01:14,  1.58it/s]\u001b[A\n",
            " 87% 754/870 [02:44<01:13,  1.58it/s]\u001b[A\n",
            " 87% 755/870 [02:44<01:12,  1.58it/s]\u001b[A\n",
            " 87% 756/870 [02:45<01:12,  1.58it/s]\u001b[A\n",
            " 87% 757/870 [02:46<01:11,  1.58it/s]\u001b[A\n",
            " 87% 758/870 [02:46<01:10,  1.58it/s]\u001b[A\n",
            " 87% 759/870 [02:47<01:10,  1.58it/s]\u001b[A\n",
            " 87% 760/870 [02:47<01:09,  1.58it/s]\u001b[A\n",
            " 87% 761/870 [02:48<01:09,  1.57it/s]\u001b[A\n",
            " 88% 762/870 [02:49<01:08,  1.57it/s]\u001b[A\n",
            " 88% 763/870 [02:49<01:07,  1.58it/s]\u001b[A\n",
            " 88% 764/870 [02:50<01:07,  1.58it/s]\u001b[A\n",
            " 88% 765/870 [02:51<01:06,  1.58it/s]\u001b[A\n",
            " 88% 766/870 [02:51<01:05,  1.58it/s]\u001b[A\n",
            " 88% 767/870 [02:52<01:05,  1.58it/s]\u001b[A\n",
            " 88% 768/870 [02:53<01:04,  1.58it/s]\u001b[A\n",
            " 88% 769/870 [02:53<01:03,  1.58it/s]\u001b[A\n",
            " 89% 770/870 [02:54<01:03,  1.58it/s]\u001b[A\n",
            " 89% 771/870 [02:54<01:02,  1.58it/s]\u001b[A\n",
            " 89% 772/870 [02:55<01:02,  1.58it/s]\u001b[A\n",
            " 89% 773/870 [02:56<01:01,  1.58it/s]\u001b[A\n",
            " 89% 774/870 [02:56<01:00,  1.58it/s]\u001b[A\n",
            " 89% 775/870 [02:57<01:00,  1.58it/s]\u001b[A\n",
            " 89% 776/870 [02:58<00:59,  1.58it/s]\u001b[A\n",
            " 89% 777/870 [02:58<00:58,  1.58it/s]\u001b[A\n",
            " 89% 778/870 [02:59<00:58,  1.57it/s]\u001b[A\n",
            " 90% 779/870 [03:00<00:57,  1.57it/s]\u001b[A\n",
            " 90% 780/870 [03:00<00:57,  1.57it/s]\u001b[A\n",
            " 90% 781/870 [03:01<00:56,  1.58it/s]\u001b[A\n",
            " 90% 782/870 [03:01<00:55,  1.57it/s]\u001b[A\n",
            " 90% 783/870 [03:02<00:55,  1.57it/s]\u001b[A\n",
            " 90% 784/870 [03:03<00:54,  1.57it/s]\u001b[A\n",
            " 90% 785/870 [03:03<00:53,  1.57it/s]\u001b[A\n",
            " 90% 786/870 [03:04<00:53,  1.57it/s]\u001b[A\n",
            " 90% 787/870 [03:05<00:52,  1.57it/s]\u001b[A\n",
            " 91% 788/870 [03:05<00:52,  1.57it/s]\u001b[A\n",
            " 91% 789/870 [03:06<00:51,  1.57it/s]\u001b[A\n",
            " 91% 790/870 [03:07<00:51,  1.57it/s]\u001b[A\n",
            " 91% 791/870 [03:07<00:50,  1.57it/s]\u001b[A\n",
            " 91% 792/870 [03:08<00:49,  1.57it/s]\u001b[A\n",
            " 91% 793/870 [03:08<00:48,  1.57it/s]\u001b[A\n",
            " 91% 794/870 [03:09<00:48,  1.57it/s]\u001b[A\n",
            " 91% 795/870 [03:10<00:47,  1.57it/s]\u001b[A\n",
            " 91% 796/870 [03:10<00:47,  1.57it/s]\u001b[A\n",
            " 92% 797/870 [03:11<00:46,  1.57it/s]\u001b[A\n",
            " 92% 798/870 [03:12<00:45,  1.57it/s]\u001b[A\n",
            " 92% 799/870 [03:12<00:45,  1.57it/s]\u001b[A\n",
            " 92% 800/870 [03:13<00:44,  1.57it/s]\u001b[A\n",
            " 92% 801/870 [03:14<00:43,  1.57it/s]\u001b[A\n",
            " 92% 802/870 [03:14<00:43,  1.57it/s]\u001b[A\n",
            " 92% 803/870 [03:15<00:42,  1.57it/s]\u001b[A\n",
            " 92% 804/870 [03:15<00:41,  1.57it/s]\u001b[A\n",
            " 93% 805/870 [03:16<00:41,  1.57it/s]\u001b[A\n",
            " 93% 806/870 [03:17<00:40,  1.57it/s]\u001b[A\n",
            " 93% 807/870 [03:17<00:40,  1.57it/s]\u001b[A\n",
            " 93% 808/870 [03:18<00:39,  1.57it/s]\u001b[A\n",
            " 93% 809/870 [03:19<00:38,  1.57it/s]\u001b[A\n",
            " 93% 810/870 [03:19<00:38,  1.57it/s]\u001b[A\n",
            " 93% 811/870 [03:20<00:37,  1.57it/s]\u001b[A\n",
            " 93% 812/870 [03:21<00:36,  1.57it/s]\u001b[A\n",
            " 93% 813/870 [03:21<00:36,  1.57it/s]\u001b[A\n",
            " 94% 814/870 [03:22<00:35,  1.57it/s]\u001b[A\n",
            " 94% 815/870 [03:22<00:34,  1.57it/s]\u001b[A\n",
            " 94% 816/870 [03:23<00:34,  1.57it/s]\u001b[A\n",
            " 94% 817/870 [03:24<00:33,  1.57it/s]\u001b[A\n",
            " 94% 818/870 [03:24<00:33,  1.57it/s]\u001b[A\n",
            " 94% 819/870 [03:25<00:32,  1.57it/s]\u001b[A\n",
            " 94% 820/870 [03:26<00:31,  1.57it/s]\u001b[A\n",
            " 94% 821/870 [03:26<00:31,  1.57it/s]\u001b[A\n",
            " 94% 822/870 [03:27<00:30,  1.57it/s]\u001b[A\n",
            " 95% 823/870 [03:28<00:29,  1.57it/s]\u001b[A\n",
            " 95% 824/870 [03:28<00:29,  1.57it/s]\u001b[A\n",
            " 95% 825/870 [03:29<00:28,  1.57it/s]\u001b[A\n",
            " 95% 826/870 [03:29<00:28,  1.57it/s]\u001b[A\n",
            " 95% 827/870 [03:30<00:27,  1.57it/s]\u001b[A\n",
            " 95% 828/870 [03:31<00:26,  1.57it/s]\u001b[A\n",
            " 95% 829/870 [03:31<00:26,  1.57it/s]\u001b[A\n",
            " 95% 830/870 [03:32<00:25,  1.57it/s]\u001b[A\n",
            " 96% 831/870 [03:33<00:24,  1.58it/s]\u001b[A\n",
            " 96% 832/870 [03:33<00:24,  1.58it/s]\u001b[A\n",
            " 96% 833/870 [03:34<00:23,  1.58it/s]\u001b[A\n",
            " 96% 834/870 [03:34<00:22,  1.58it/s]\u001b[A\n",
            " 96% 835/870 [03:35<00:22,  1.58it/s]\u001b[A\n",
            " 96% 836/870 [03:36<00:21,  1.58it/s]\u001b[A\n",
            " 96% 837/870 [03:36<00:20,  1.58it/s]\u001b[A\n",
            " 96% 838/870 [03:37<00:20,  1.58it/s]\u001b[A\n",
            " 96% 839/870 [03:38<00:19,  1.58it/s]\u001b[A\n",
            " 97% 840/870 [03:38<00:19,  1.58it/s]\u001b[A\n",
            " 97% 841/870 [03:39<00:18,  1.58it/s]\u001b[A\n",
            " 97% 842/870 [03:40<00:17,  1.58it/s]\u001b[A\n",
            " 97% 843/870 [03:40<00:17,  1.58it/s]\u001b[A\n",
            " 97% 844/870 [03:41<00:16,  1.58it/s]\u001b[A\n",
            " 97% 845/870 [03:41<00:15,  1.58it/s]\u001b[A\n",
            " 97% 846/870 [03:42<00:15,  1.58it/s]\u001b[A\n",
            " 97% 847/870 [03:43<00:14,  1.58it/s]\u001b[A\n",
            " 97% 848/870 [03:43<00:13,  1.58it/s]\u001b[A\n",
            " 98% 849/870 [03:44<00:13,  1.56it/s]\u001b[A\n",
            " 98% 850/870 [03:45<00:12,  1.57it/s]\u001b[A\n",
            " 98% 851/870 [03:45<00:12,  1.57it/s]\u001b[A\n",
            " 98% 852/870 [03:46<00:11,  1.57it/s]\u001b[A\n",
            " 98% 853/870 [03:47<00:10,  1.57it/s]\u001b[A\n",
            " 98% 854/870 [03:47<00:10,  1.57it/s]\u001b[A\n",
            " 98% 855/870 [03:48<00:09,  1.57it/s]\u001b[A\n",
            " 98% 856/870 [03:48<00:08,  1.57it/s]\u001b[A\n",
            " 99% 857/870 [03:49<00:08,  1.57it/s]\u001b[A\n",
            " 99% 858/870 [03:50<00:07,  1.57it/s]\u001b[A\n",
            " 99% 859/870 [03:50<00:06,  1.57it/s]\u001b[A\n",
            " 99% 860/870 [03:51<00:06,  1.57it/s]\u001b[A\n",
            " 99% 861/870 [03:52<00:05,  1.57it/s]\u001b[A\n",
            " 99% 862/870 [03:52<00:05,  1.57it/s]\u001b[A\n",
            " 99% 863/870 [03:53<00:04,  1.57it/s]\u001b[A\n",
            " 99% 864/870 [03:54<00:03,  1.57it/s]\u001b[A\n",
            " 99% 865/870 [03:54<00:03,  1.57it/s]\u001b[A\n",
            "100% 866/870 [03:55<00:02,  1.57it/s]\u001b[A\n",
            "100% 867/870 [03:55<00:01,  1.57it/s]\u001b[A\n",
            "100% 868/870 [03:56<00:01,  1.58it/s]\u001b[A\n",
            "100% 869/870 [03:57<00:00,  1.57it/s]\u001b[A\n",
            "100% 870/870 [03:57<00:00,  1.70it/s]\u001b[A[INFO|trainer.py:1873] 2022-11-08 22:23:48,764 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A{'train_runtime': 237.7369, 'train_samples_per_second': 29.251, 'train_steps_per_second': 3.66, 'train_loss': 2.416890321928879, 'epoch': 3.0}\n",
            "\n",
            "100% 870/870 [03:57<00:00,  3.66it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-08 22:23:48,766 >> Saving model checkpoint to /content/test-clm/gpt-2-0.9\n",
            "[INFO|configuration_utils.py:447] 2022-11-08 22:23:48,767 >> Configuration saved in /content/test-clm/gpt-2-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-08 22:23:50,287 >> Model weights saved in /content/test-clm/gpt-2-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-08 22:23:50,287 >> tokenizer config file saved in /content/test-clm/gpt-2-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-08 22:23:50,288 >> Special tokens file saved in /content/test-clm/gpt-2-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.4169\n",
            "  train_runtime            = 0:03:57.73\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     29.251\n",
            "  train_steps_per_second   =       3.66\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-08 22:23:50,400 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-08 22:23:50,400 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-08 22:23:50,400 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.57\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.692\n",
            "  eval_steps_per_second   =      3.962\n",
            "  perplexity              =   296.4128\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.95/runs/Nov08_22-24-03_2921ed0a3794,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.95,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.95,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:Checkpoint detected, resuming training at /content/test-clm/gpt-2-0.95/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 772.10it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-08 22:24:04,193 >> loading configuration file /content/models/gpt-2-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:24:04,193 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.95\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-08 22:24:04,292 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:24:04,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:24:04,385 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:24:04,576 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:24:04,576 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:24:04,577 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-08 22:24:04,644 >> loading weights file /content/models/gpt-2-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-08 22:24:07,116 >> Some weights of the model checkpoint at /content/models/gpt-2-0.95 were not used when initializing GPT2LMHeadModel: ['transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-08 22:24:07,116 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.95 and are newly initialized: ['transformer.h.10.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.9.attn.c_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "[INFO|trainer.py:1944] 2022-11-08 22:24:09,254 >> Loading model from /content/test-clm/gpt-2-0.95/checkpoint-500.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-08 22:24:10,182 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-08 22:24:10,182 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-08 22:24:10,182 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-08 22:24:10,182 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-08 22:24:10,182 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-08 22:24:10,182 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-08 22:24:10,182 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-08 22:24:10,182 >>   Number of trainable parameters = 124439808\n",
            "[INFO|trainer.py:1651] 2022-11-08 22:24:10,183 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1652] 2022-11-08 22:24:10,183 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1653] 2022-11-08 22:24:10,183 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1656] 2022-11-08 22:24:10,183 >>   Will skip the first 1 epochs then the first 210 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/210 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 210/210 [00:02<00:00, 100.10it/s]\n",
            "\n",
            " 58% 501/870 [00:03<00:02, 137.70it/s]\u001b[A\n",
            " 59% 515/870 [00:12<00:11, 31.91it/s] \u001b[A\n",
            " 60% 521/870 [00:16<00:15, 22.13it/s]\u001b[A\n",
            " 60% 525/870 [00:18<00:19, 17.30it/s]\u001b[A\n",
            " 61% 528/870 [00:20<00:24, 14.14it/s]\u001b[A\n",
            " 61% 530/870 [00:22<00:28, 12.12it/s]\u001b[A\n",
            " 61% 531/870 [00:22<00:30, 11.03it/s]\u001b[A\n",
            " 61% 532/870 [00:23<00:34,  9.82it/s]\u001b[A\n",
            " 61% 533/870 [00:23<00:39,  8.54it/s]\u001b[A\n",
            " 61% 534/870 [00:24<00:46,  7.28it/s]\u001b[A\n",
            " 61% 535/870 [00:25<00:54,  6.11it/s]\u001b[A\n",
            " 62% 536/870 [00:25<01:05,  5.08it/s]\u001b[A\n",
            " 62% 537/870 [00:26<01:18,  4.22it/s]\u001b[A\n",
            " 62% 538/870 [00:27<01:33,  3.53it/s]\u001b[A\n",
            " 62% 539/870 [00:27<01:50,  3.00it/s]\u001b[A\n",
            " 62% 540/870 [00:28<02:06,  2.61it/s]\u001b[A\n",
            " 62% 541/870 [00:29<02:22,  2.31it/s]\u001b[A\n",
            " 62% 542/870 [00:29<02:36,  2.10it/s]\u001b[A\n",
            " 62% 543/870 [00:30<02:47,  1.95it/s]\u001b[A\n",
            " 63% 544/870 [00:30<02:57,  1.84it/s]\u001b[A\n",
            " 63% 545/870 [00:31<03:04,  1.76it/s]\u001b[A\n",
            " 63% 546/870 [00:32<03:09,  1.71it/s]\u001b[A\n",
            " 63% 547/870 [00:32<03:13,  1.67it/s]\u001b[A\n",
            " 63% 548/870 [00:33<03:16,  1.64it/s]\u001b[A\n",
            " 63% 549/870 [00:34<03:17,  1.62it/s]\u001b[A\n",
            " 63% 550/870 [00:34<03:18,  1.61it/s]\u001b[A\n",
            " 63% 551/870 [00:35<03:19,  1.60it/s]\u001b[A\n",
            " 63% 552/870 [00:35<03:19,  1.59it/s]\u001b[A\n",
            " 64% 553/870 [00:36<03:21,  1.58it/s]\u001b[A\n",
            " 64% 554/870 [00:37<03:20,  1.57it/s]\u001b[A\n",
            " 64% 555/870 [00:37<03:20,  1.57it/s]\u001b[A\n",
            " 64% 556/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 557/870 [00:39<03:18,  1.58it/s]\u001b[A\n",
            " 64% 558/870 [00:39<03:17,  1.58it/s]\u001b[A\n",
            " 64% 559/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 560/870 [00:41<03:16,  1.58it/s]\u001b[A\n",
            " 64% 561/870 [00:41<03:15,  1.58it/s]\u001b[A\n",
            " 65% 562/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 563/870 [00:42<03:14,  1.58it/s]\u001b[A\n",
            " 65% 564/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 565/870 [00:44<03:13,  1.58it/s]\u001b[A\n",
            " 65% 566/870 [00:44<03:12,  1.58it/s]\u001b[A\n",
            " 65% 567/870 [00:45<03:12,  1.58it/s]\u001b[A\n",
            " 65% 568/870 [00:46<03:11,  1.58it/s]\u001b[A\n",
            " 65% 569/870 [00:46<03:11,  1.58it/s]\u001b[A\n",
            " 66% 570/870 [00:47<03:10,  1.58it/s]\u001b[A\n",
            " 66% 571/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 572/870 [00:48<03:08,  1.58it/s]\u001b[A\n",
            " 66% 573/870 [00:49<03:08,  1.58it/s]\u001b[A\n",
            " 66% 574/870 [00:49<03:07,  1.58it/s]\u001b[A\n",
            " 66% 575/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 576/870 [00:51<03:06,  1.57it/s]\u001b[A\n",
            " 66% 577/870 [00:51<03:05,  1.58it/s]\u001b[A\n",
            " 66% 578/870 [00:52<03:05,  1.58it/s]\u001b[A\n",
            " 67% 579/870 [00:53<03:04,  1.58it/s]\u001b[A\n",
            " 67% 580/870 [00:53<02:50,  1.70it/s]\u001b[A\n",
            " 67% 581/870 [00:54<02:54,  1.66it/s]\u001b[A\n",
            " 67% 582/870 [00:54<02:56,  1.63it/s]\u001b[A\n",
            " 67% 583/870 [00:55<02:57,  1.62it/s]\u001b[A\n",
            " 67% 584/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 585/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 586/870 [00:57<02:58,  1.59it/s]\u001b[A\n",
            " 67% 587/870 [00:58<02:58,  1.59it/s]\u001b[A\n",
            " 68% 588/870 [00:58<02:58,  1.58it/s]\u001b[A\n",
            " 68% 589/870 [00:59<02:57,  1.58it/s]\u001b[A\n",
            " 68% 590/870 [00:59<02:57,  1.58it/s]\u001b[A\n",
            " 68% 591/870 [01:00<02:56,  1.58it/s]\u001b[A\n",
            " 68% 592/870 [01:01<02:56,  1.58it/s]\u001b[A\n",
            " 68% 593/870 [01:01<02:55,  1.58it/s]\u001b[A\n",
            " 68% 594/870 [01:02<02:54,  1.58it/s]\u001b[A\n",
            " 68% 595/870 [01:03<02:54,  1.58it/s]\u001b[A\n",
            " 69% 596/870 [01:03<02:53,  1.58it/s]\u001b[A\n",
            " 69% 597/870 [01:04<02:53,  1.58it/s]\u001b[A\n",
            " 69% 598/870 [01:05<02:52,  1.58it/s]\u001b[A\n",
            " 69% 599/870 [01:05<02:51,  1.58it/s]\u001b[A\n",
            " 69% 600/870 [01:06<02:51,  1.58it/s]\u001b[A\n",
            " 69% 601/870 [01:06<02:50,  1.58it/s]\u001b[A\n",
            " 69% 602/870 [01:07<02:49,  1.58it/s]\u001b[A\n",
            " 69% 603/870 [01:08<02:49,  1.58it/s]\u001b[A\n",
            " 69% 604/870 [01:08<02:48,  1.58it/s]\u001b[A\n",
            " 70% 605/870 [01:09<02:48,  1.58it/s]\u001b[A\n",
            " 70% 606/870 [01:10<02:47,  1.58it/s]\u001b[A\n",
            " 70% 607/870 [01:10<02:46,  1.58it/s]\u001b[A\n",
            " 70% 608/870 [01:11<02:46,  1.58it/s]\u001b[A\n",
            " 70% 609/870 [01:12<02:45,  1.58it/s]\u001b[A\n",
            " 70% 610/870 [01:12<02:45,  1.57it/s]\u001b[A\n",
            " 70% 611/870 [01:13<02:44,  1.57it/s]\u001b[A\n",
            " 70% 612/870 [01:13<02:43,  1.57it/s]\u001b[A\n",
            " 70% 613/870 [01:14<02:43,  1.57it/s]\u001b[A\n",
            " 71% 614/870 [01:15<02:42,  1.58it/s]\u001b[A\n",
            " 71% 615/870 [01:15<02:41,  1.58it/s]\u001b[A\n",
            " 71% 616/870 [01:16<02:41,  1.58it/s]\u001b[A\n",
            " 71% 617/870 [01:17<02:41,  1.57it/s]\u001b[A\n",
            " 71% 618/870 [01:17<02:40,  1.57it/s]\u001b[A\n",
            " 71% 619/870 [01:18<02:39,  1.57it/s]\u001b[A\n",
            " 71% 620/870 [01:19<02:38,  1.57it/s]\u001b[A\n",
            " 71% 621/870 [01:19<02:37,  1.58it/s]\u001b[A\n",
            " 71% 622/870 [01:20<02:37,  1.58it/s]\u001b[A\n",
            " 72% 623/870 [01:20<02:36,  1.58it/s]\u001b[A\n",
            " 72% 624/870 [01:21<02:35,  1.58it/s]\u001b[A\n",
            " 72% 625/870 [01:22<02:35,  1.57it/s]\u001b[A\n",
            " 72% 626/870 [01:22<02:34,  1.58it/s]\u001b[A\n",
            " 72% 627/870 [01:23<02:34,  1.58it/s]\u001b[A\n",
            " 72% 628/870 [01:24<02:33,  1.58it/s]\u001b[A\n",
            " 72% 629/870 [01:24<02:32,  1.58it/s]\u001b[A\n",
            " 72% 630/870 [01:25<02:32,  1.58it/s]\u001b[A\n",
            " 73% 631/870 [01:25<02:31,  1.58it/s]\u001b[A\n",
            " 73% 632/870 [01:26<02:30,  1.58it/s]\u001b[A\n",
            " 73% 633/870 [01:27<02:30,  1.58it/s]\u001b[A\n",
            " 73% 634/870 [01:27<02:29,  1.58it/s]\u001b[A\n",
            " 73% 635/870 [01:28<02:28,  1.58it/s]\u001b[A\n",
            " 73% 636/870 [01:29<02:28,  1.58it/s]\u001b[A\n",
            " 73% 637/870 [01:29<02:27,  1.58it/s]\u001b[A\n",
            " 73% 638/870 [01:30<02:27,  1.57it/s]\u001b[A\n",
            " 73% 639/870 [01:31<02:27,  1.57it/s]\u001b[A\n",
            " 74% 640/870 [01:31<02:26,  1.57it/s]\u001b[A\n",
            " 74% 641/870 [01:32<02:25,  1.57it/s]\u001b[A\n",
            " 74% 642/870 [01:32<02:24,  1.57it/s]\u001b[A\n",
            " 74% 643/870 [01:33<02:24,  1.58it/s]\u001b[A\n",
            " 74% 644/870 [01:34<02:23,  1.58it/s]\u001b[A\n",
            " 74% 645/870 [01:34<02:22,  1.58it/s]\u001b[A\n",
            " 74% 646/870 [01:35<02:21,  1.58it/s]\u001b[A\n",
            " 74% 647/870 [01:36<02:21,  1.58it/s]\u001b[A\n",
            " 74% 648/870 [01:36<02:21,  1.57it/s]\u001b[A\n",
            " 75% 649/870 [01:37<02:20,  1.58it/s]\u001b[A\n",
            " 75% 650/870 [01:38<02:19,  1.58it/s]\u001b[A\n",
            " 75% 651/870 [01:38<02:18,  1.58it/s]\u001b[A\n",
            " 75% 652/870 [01:39<02:18,  1.57it/s]\u001b[A\n",
            " 75% 653/870 [01:39<02:17,  1.57it/s]\u001b[A\n",
            " 75% 654/870 [01:40<02:17,  1.57it/s]\u001b[A\n",
            " 75% 655/870 [01:41<02:16,  1.58it/s]\u001b[A\n",
            " 75% 656/870 [01:41<02:15,  1.58it/s]\u001b[A\n",
            " 76% 657/870 [01:42<02:15,  1.58it/s]\u001b[A\n",
            " 76% 658/870 [01:43<02:14,  1.57it/s]\u001b[A\n",
            " 76% 659/870 [01:43<02:14,  1.57it/s]\u001b[A\n",
            " 76% 660/870 [01:44<02:13,  1.57it/s]\u001b[A\n",
            " 76% 661/870 [01:45<02:12,  1.57it/s]\u001b[A\n",
            " 76% 662/870 [01:45<02:12,  1.58it/s]\u001b[A\n",
            " 76% 663/870 [01:46<02:11,  1.58it/s]\u001b[A\n",
            " 76% 664/870 [01:46<02:10,  1.58it/s]\u001b[A\n",
            " 76% 665/870 [01:47<02:10,  1.58it/s]\u001b[A\n",
            " 77% 666/870 [01:48<02:09,  1.58it/s]\u001b[A\n",
            " 77% 667/870 [01:48<02:08,  1.58it/s]\u001b[A\n",
            " 77% 668/870 [01:49<02:08,  1.58it/s]\u001b[A\n",
            " 77% 669/870 [01:50<02:07,  1.58it/s]\u001b[A\n",
            " 77% 670/870 [01:50<02:06,  1.58it/s]\u001b[A\n",
            " 77% 671/870 [01:51<02:06,  1.57it/s]\u001b[A\n",
            " 77% 672/870 [01:52<02:06,  1.57it/s]\u001b[A\n",
            " 77% 673/870 [01:52<02:05,  1.57it/s]\u001b[A\n",
            " 77% 674/870 [01:53<02:04,  1.57it/s]\u001b[A\n",
            " 78% 675/870 [01:53<02:03,  1.57it/s]\u001b[A\n",
            " 78% 676/870 [01:54<02:03,  1.58it/s]\u001b[A\n",
            " 78% 677/870 [01:55<02:02,  1.58it/s]\u001b[A\n",
            " 78% 678/870 [01:55<02:01,  1.58it/s]\u001b[A\n",
            " 78% 679/870 [01:56<02:01,  1.58it/s]\u001b[A\n",
            " 78% 680/870 [01:57<02:00,  1.58it/s]\u001b[A\n",
            " 78% 681/870 [01:57<01:59,  1.58it/s]\u001b[A\n",
            " 78% 682/870 [01:58<01:59,  1.58it/s]\u001b[A\n",
            " 79% 683/870 [01:58<01:58,  1.58it/s]\u001b[A\n",
            " 79% 684/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 685/870 [02:00<01:57,  1.57it/s]\u001b[A\n",
            " 79% 686/870 [02:00<01:56,  1.57it/s]\u001b[A\n",
            " 79% 687/870 [02:01<01:56,  1.58it/s]\u001b[A\n",
            " 79% 688/870 [02:02<01:55,  1.58it/s]\u001b[A\n",
            " 79% 689/870 [02:02<01:54,  1.58it/s]\u001b[A\n",
            " 79% 690/870 [02:03<01:54,  1.58it/s]\u001b[A\n",
            " 79% 691/870 [02:04<01:53,  1.58it/s]\u001b[A\n",
            " 80% 692/870 [02:04<01:52,  1.58it/s]\u001b[A\n",
            " 80% 693/870 [02:05<01:52,  1.58it/s]\u001b[A\n",
            " 80% 694/870 [02:05<01:51,  1.58it/s]\u001b[A\n",
            " 80% 695/870 [02:06<01:50,  1.58it/s]\u001b[A\n",
            " 80% 696/870 [02:07<01:50,  1.58it/s]\u001b[A\n",
            " 80% 697/870 [02:07<01:49,  1.58it/s]\u001b[A\n",
            " 80% 698/870 [02:08<01:49,  1.58it/s]\u001b[A\n",
            " 80% 699/870 [02:09<01:48,  1.58it/s]\u001b[A\n",
            " 80% 700/870 [02:09<01:47,  1.58it/s]\u001b[A\n",
            " 81% 701/870 [02:10<01:47,  1.58it/s]\u001b[A\n",
            " 81% 702/870 [02:11<01:46,  1.58it/s]\u001b[A\n",
            " 81% 703/870 [02:11<01:45,  1.58it/s]\u001b[A\n",
            " 81% 704/870 [02:12<01:45,  1.58it/s]\u001b[A\n",
            " 81% 705/870 [02:12<01:44,  1.58it/s]\u001b[A\n",
            " 81% 706/870 [02:13<01:43,  1.58it/s]\u001b[A\n",
            " 81% 707/870 [02:14<01:43,  1.58it/s]\u001b[A\n",
            " 81% 708/870 [02:14<01:42,  1.57it/s]\u001b[A\n",
            " 81% 709/870 [02:15<01:42,  1.57it/s]\u001b[A\n",
            " 82% 710/870 [02:16<01:42,  1.56it/s]\u001b[A\n",
            " 82% 711/870 [02:16<01:41,  1.57it/s]\u001b[A\n",
            " 82% 712/870 [02:17<01:40,  1.57it/s]\u001b[A\n",
            " 82% 713/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 714/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 715/870 [02:19<01:38,  1.57it/s]\u001b[A\n",
            " 82% 716/870 [02:19<01:37,  1.57it/s]\u001b[A\n",
            " 82% 717/870 [02:20<01:37,  1.57it/s]\u001b[A\n",
            " 83% 718/870 [02:21<01:36,  1.58it/s]\u001b[A\n",
            " 83% 719/870 [02:21<01:35,  1.57it/s]\u001b[A\n",
            " 83% 720/870 [02:22<01:35,  1.58it/s]\u001b[A\n",
            " 83% 721/870 [02:23<01:34,  1.58it/s]\u001b[A\n",
            " 83% 722/870 [02:23<01:33,  1.58it/s]\u001b[A\n",
            " 83% 723/870 [02:24<01:33,  1.57it/s]\u001b[A\n",
            " 83% 724/870 [02:25<01:32,  1.58it/s]\u001b[A\n",
            " 83% 725/870 [02:25<01:32,  1.58it/s]\u001b[A\n",
            " 83% 726/870 [02:26<01:31,  1.58it/s]\u001b[A\n",
            " 84% 727/870 [02:26<01:30,  1.58it/s]\u001b[A\n",
            " 84% 728/870 [02:27<01:30,  1.58it/s]\u001b[A\n",
            " 84% 729/870 [02:28<01:29,  1.57it/s]\u001b[A\n",
            " 84% 730/870 [02:28<01:29,  1.57it/s]\u001b[A\n",
            " 84% 731/870 [02:29<01:28,  1.57it/s]\u001b[A\n",
            " 84% 732/870 [02:30<01:27,  1.57it/s]\u001b[A\n",
            " 84% 733/870 [02:30<01:26,  1.58it/s]\u001b[A\n",
            " 84% 734/870 [02:31<01:26,  1.58it/s]\u001b[A\n",
            " 84% 735/870 [02:31<01:25,  1.58it/s]\u001b[A\n",
            " 85% 736/870 [02:32<01:24,  1.58it/s]\u001b[A\n",
            " 85% 737/870 [02:33<01:24,  1.58it/s]\u001b[A\n",
            " 85% 738/870 [02:33<01:23,  1.58it/s]\u001b[A\n",
            " 85% 739/870 [02:34<01:23,  1.58it/s]\u001b[A\n",
            " 85% 740/870 [02:35<01:22,  1.58it/s]\u001b[A\n",
            " 85% 741/870 [02:35<01:21,  1.58it/s]\u001b[A\n",
            " 85% 742/870 [02:36<01:21,  1.58it/s]\u001b[A\n",
            " 85% 743/870 [02:37<01:20,  1.58it/s]\u001b[A\n",
            " 86% 744/870 [02:37<01:19,  1.58it/s]\u001b[A\n",
            " 86% 745/870 [02:38<01:19,  1.57it/s]\u001b[A\n",
            " 86% 746/870 [02:38<01:18,  1.57it/s]\u001b[A\n",
            " 86% 747/870 [02:39<01:18,  1.57it/s]\u001b[A\n",
            " 86% 748/870 [02:40<01:17,  1.57it/s]\u001b[A\n",
            " 86% 749/870 [02:40<01:16,  1.58it/s]\u001b[A\n",
            " 86% 750/870 [02:41<01:16,  1.58it/s]\u001b[A\n",
            " 86% 751/870 [02:42<01:15,  1.58it/s]\u001b[A\n",
            " 86% 752/870 [02:42<01:14,  1.58it/s]\u001b[A\n",
            " 87% 753/870 [02:43<01:14,  1.58it/s]\u001b[A\n",
            " 87% 754/870 [02:44<01:13,  1.58it/s]\u001b[A\n",
            " 87% 755/870 [02:44<01:12,  1.58it/s]\u001b[A\n",
            " 87% 756/870 [02:45<01:12,  1.58it/s]\u001b[A\n",
            " 87% 757/870 [02:45<01:11,  1.58it/s]\u001b[A\n",
            " 87% 758/870 [02:46<01:11,  1.58it/s]\u001b[A\n",
            " 87% 759/870 [02:47<01:10,  1.58it/s]\u001b[A\n",
            " 87% 760/870 [02:47<01:09,  1.58it/s]\u001b[A\n",
            " 87% 761/870 [02:48<01:09,  1.58it/s]\u001b[A\n",
            " 88% 762/870 [02:49<01:08,  1.58it/s]\u001b[A\n",
            " 88% 763/870 [02:49<01:07,  1.58it/s]\u001b[A\n",
            " 88% 764/870 [02:50<01:07,  1.58it/s]\u001b[A\n",
            " 88% 765/870 [02:51<01:06,  1.58it/s]\u001b[A\n",
            " 88% 766/870 [02:51<01:05,  1.58it/s]\u001b[A\n",
            " 88% 767/870 [02:52<01:05,  1.57it/s]\u001b[A\n",
            " 88% 768/870 [02:52<01:04,  1.57it/s]\u001b[A\n",
            " 88% 769/870 [02:53<01:04,  1.58it/s]\u001b[A\n",
            " 89% 770/870 [02:54<01:03,  1.58it/s]\u001b[A\n",
            " 89% 771/870 [02:54<01:02,  1.58it/s]\u001b[A\n",
            " 89% 772/870 [02:55<01:02,  1.58it/s]\u001b[A\n",
            " 89% 773/870 [02:56<01:01,  1.58it/s]\u001b[A\n",
            " 89% 774/870 [02:56<01:00,  1.58it/s]\u001b[A\n",
            " 89% 775/870 [02:57<01:00,  1.58it/s]\u001b[A\n",
            " 89% 776/870 [02:58<00:59,  1.58it/s]\u001b[A\n",
            " 89% 777/870 [02:58<00:58,  1.58it/s]\u001b[A\n",
            " 89% 778/870 [02:59<00:58,  1.58it/s]\u001b[A\n",
            " 90% 779/870 [02:59<00:57,  1.58it/s]\u001b[A\n",
            " 90% 780/870 [03:00<00:56,  1.58it/s]\u001b[A\n",
            " 90% 781/870 [03:01<00:56,  1.58it/s]\u001b[A\n",
            " 90% 782/870 [03:01<00:55,  1.58it/s]\u001b[A\n",
            " 90% 783/870 [03:02<00:55,  1.58it/s]\u001b[A\n",
            " 90% 784/870 [03:03<00:54,  1.58it/s]\u001b[A\n",
            " 90% 785/870 [03:03<00:53,  1.58it/s]\u001b[A\n",
            " 90% 786/870 [03:04<00:53,  1.58it/s]\u001b[A\n",
            " 90% 787/870 [03:04<00:52,  1.58it/s]\u001b[A\n",
            " 91% 788/870 [03:05<00:51,  1.58it/s]\u001b[A\n",
            " 91% 789/870 [03:06<00:51,  1.58it/s]\u001b[A\n",
            " 91% 790/870 [03:06<00:50,  1.58it/s]\u001b[A\n",
            " 91% 791/870 [03:07<00:49,  1.58it/s]\u001b[A\n",
            " 91% 792/870 [03:08<00:49,  1.58it/s]\u001b[A\n",
            " 91% 793/870 [03:08<00:48,  1.58it/s]\u001b[A\n",
            " 91% 794/870 [03:09<00:48,  1.58it/s]\u001b[A\n",
            " 91% 795/870 [03:10<00:47,  1.58it/s]\u001b[A\n",
            " 91% 796/870 [03:10<00:46,  1.58it/s]\u001b[A\n",
            " 92% 797/870 [03:11<00:46,  1.58it/s]\u001b[A\n",
            " 92% 798/870 [03:11<00:45,  1.58it/s]\u001b[A\n",
            " 92% 799/870 [03:12<00:45,  1.58it/s]\u001b[A\n",
            " 92% 800/870 [03:13<00:44,  1.58it/s]\u001b[A\n",
            " 92% 801/870 [03:13<00:43,  1.58it/s]\u001b[A\n",
            " 92% 802/870 [03:14<00:43,  1.58it/s]\u001b[A\n",
            " 92% 803/870 [03:15<00:42,  1.58it/s]\u001b[A\n",
            " 92% 804/870 [03:15<00:41,  1.58it/s]\u001b[A\n",
            " 93% 805/870 [03:16<00:41,  1.57it/s]\u001b[A\n",
            " 93% 806/870 [03:17<00:40,  1.57it/s]\u001b[A\n",
            " 93% 807/870 [03:17<00:40,  1.57it/s]\u001b[A\n",
            " 93% 808/870 [03:18<00:39,  1.57it/s]\u001b[A\n",
            " 93% 809/870 [03:18<00:38,  1.58it/s]\u001b[A\n",
            " 93% 810/870 [03:19<00:38,  1.58it/s]\u001b[A\n",
            " 93% 811/870 [03:20<00:37,  1.58it/s]\u001b[A\n",
            " 93% 812/870 [03:20<00:36,  1.58it/s]\u001b[A\n",
            " 93% 813/870 [03:21<00:36,  1.58it/s]\u001b[A\n",
            " 94% 814/870 [03:22<00:35,  1.58it/s]\u001b[A\n",
            " 94% 815/870 [03:22<00:34,  1.58it/s]\u001b[A\n",
            " 94% 816/870 [03:23<00:34,  1.58it/s]\u001b[A\n",
            " 94% 817/870 [03:23<00:33,  1.58it/s]\u001b[A\n",
            " 94% 818/870 [03:24<00:32,  1.58it/s]\u001b[A\n",
            " 94% 819/870 [03:25<00:32,  1.58it/s]\u001b[A\n",
            " 94% 820/870 [03:25<00:31,  1.58it/s]\u001b[A\n",
            " 94% 821/870 [03:26<00:31,  1.58it/s]\u001b[A\n",
            " 94% 822/870 [03:27<00:30,  1.58it/s]\u001b[A\n",
            " 95% 823/870 [03:27<00:29,  1.58it/s]\u001b[A\n",
            " 95% 824/870 [03:28<00:29,  1.58it/s]\u001b[A\n",
            " 95% 825/870 [03:29<00:28,  1.58it/s]\u001b[A\n",
            " 95% 826/870 [03:29<00:27,  1.58it/s]\u001b[A\n",
            " 95% 827/870 [03:30<00:27,  1.58it/s]\u001b[A\n",
            " 95% 828/870 [03:30<00:26,  1.58it/s]\u001b[A\n",
            " 95% 829/870 [03:31<00:25,  1.58it/s]\u001b[A\n",
            " 95% 830/870 [03:32<00:25,  1.57it/s]\u001b[A\n",
            " 96% 831/870 [03:32<00:24,  1.57it/s]\u001b[A\n",
            " 96% 832/870 [03:33<00:24,  1.57it/s]\u001b[A\n",
            " 96% 833/870 [03:34<00:23,  1.58it/s]\u001b[A\n",
            " 96% 834/870 [03:34<00:22,  1.58it/s]\u001b[A\n",
            " 96% 835/870 [03:35<00:22,  1.58it/s]\u001b[A\n",
            " 96% 836/870 [03:36<00:21,  1.58it/s]\u001b[A\n",
            " 96% 837/870 [03:36<00:20,  1.58it/s]\u001b[A\n",
            " 96% 838/870 [03:37<00:20,  1.58it/s]\u001b[A\n",
            " 96% 839/870 [03:37<00:19,  1.58it/s]\u001b[A\n",
            " 97% 840/870 [03:38<00:19,  1.58it/s]\u001b[A\n",
            " 97% 841/870 [03:39<00:18,  1.58it/s]\u001b[A\n",
            " 97% 842/870 [03:39<00:17,  1.58it/s]\u001b[A\n",
            " 97% 843/870 [03:40<00:17,  1.58it/s]\u001b[A\n",
            " 97% 844/870 [03:41<00:16,  1.58it/s]\u001b[A\n",
            " 97% 845/870 [03:41<00:15,  1.58it/s]\u001b[A\n",
            " 97% 846/870 [03:42<00:15,  1.58it/s]\u001b[A\n",
            " 97% 847/870 [03:43<00:14,  1.57it/s]\u001b[A\n",
            " 97% 848/870 [03:43<00:13,  1.57it/s]\u001b[A\n",
            " 98% 849/870 [03:44<00:13,  1.58it/s]\u001b[A\n",
            " 98% 850/870 [03:44<00:12,  1.58it/s]\u001b[A\n",
            " 98% 851/870 [03:45<00:12,  1.57it/s]\u001b[A\n",
            " 98% 852/870 [03:46<00:11,  1.56it/s]\u001b[A\n",
            " 98% 853/870 [03:46<00:10,  1.56it/s]\u001b[A\n",
            " 98% 854/870 [03:47<00:10,  1.57it/s]\u001b[A\n",
            " 98% 855/870 [03:48<00:09,  1.57it/s]\u001b[A\n",
            " 98% 856/870 [03:48<00:08,  1.57it/s]\u001b[A\n",
            " 99% 857/870 [03:49<00:08,  1.57it/s]\u001b[A\n",
            " 99% 858/870 [03:50<00:07,  1.57it/s]\u001b[A\n",
            " 99% 859/870 [03:50<00:06,  1.57it/s]\u001b[A\n",
            " 99% 860/870 [03:51<00:06,  1.57it/s]\u001b[A\n",
            " 99% 861/870 [03:51<00:05,  1.57it/s]\u001b[A\n",
            " 99% 862/870 [03:52<00:05,  1.57it/s]\u001b[A\n",
            " 99% 863/870 [03:53<00:04,  1.57it/s]\u001b[A\n",
            " 99% 864/870 [03:53<00:03,  1.57it/s]\u001b[A\n",
            " 99% 865/870 [03:54<00:03,  1.57it/s]\u001b[A\n",
            "100% 866/870 [03:55<00:02,  1.57it/s]\u001b[A\n",
            "100% 867/870 [03:55<00:01,  1.57it/s]\u001b[A\n",
            "100% 868/870 [03:56<00:01,  1.57it/s]\u001b[A\n",
            "100% 869/870 [03:57<00:00,  1.57it/s]\u001b[A\n",
            "100% 870/870 [03:57<00:00,  1.69it/s]\u001b[A[INFO|trainer.py:1873] 2022-11-08 22:28:07,708 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A{'train_runtime': 237.5254, 'train_samples_per_second': 29.277, 'train_steps_per_second': 3.663, 'train_loss': 2.416890321928879, 'epoch': 3.0}\n",
            "\n",
            "100% 870/870 [03:57<00:00,  3.66it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-08 22:28:07,710 >> Saving model checkpoint to /content/test-clm/gpt-2-0.95\n",
            "[INFO|configuration_utils.py:447] 2022-11-08 22:28:07,711 >> Configuration saved in /content/test-clm/gpt-2-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-08 22:28:09,173 >> Model weights saved in /content/test-clm/gpt-2-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-08 22:28:09,174 >> tokenizer config file saved in /content/test-clm/gpt-2-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-08 22:28:09,174 >> Special tokens file saved in /content/test-clm/gpt-2-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.4169\n",
            "  train_runtime            = 0:03:57.52\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     29.277\n",
            "  train_steps_per_second   =      3.663\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-08 22:28:09,284 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-08 22:28:09,284 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-08 22:28:09,284 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.07it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.58\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.653\n",
            "  eval_steps_per_second   =      3.957\n",
            "  perplexity              =   296.4128\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-clm/gpt-2-0.99/runs/Nov08_22-28-22_2921ed0a3794,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/content/test-clm/gpt-2-0.99,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-clm/gpt-2-0.99,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:Checkpoint detected, resuming training at /content/test-clm/gpt-2-0.99/checkpoint-500. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wikitext/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n",
            "100% 3/3 [00:00<00:00, 718.41it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-08 22:28:23,225 >> loading configuration file /content/models/gpt-2-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:28:23,226 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.99\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-08 22:28:23,319 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:28:23,410 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:28:23,411 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-08 22:28:23,615 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-08 22:28:23,616 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-08 22:28:23,616 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-08 22:28:23,687 >> loading weights file /content/models/gpt-2-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-08 22:28:26,191 >> Some weights of the model checkpoint at /content/models/gpt-2-0.99 were not used when initializing GPT2LMHeadModel: ['transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-08 22:28:26,191 >> Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /content/models/gpt-2-0.99 and are newly initialized: ['transformer.h.11.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b8f1ce012785446a.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2aeb986029940d75.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1b8b75a801954f60.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f7fd86bcc0eef7ce.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a36a2ed817b2ee0c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9c9fc90ff3fa5e84.arrow\n",
            "[INFO|trainer.py:1944] 2022-11-08 22:28:28,432 >> Loading model from /content/test-clm/gpt-2-0.99/checkpoint-500.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-08 22:28:29,397 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-08 22:28:29,397 >>   Num examples = 2318\n",
            "[INFO|trainer.py:1624] 2022-11-08 22:28:29,397 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-08 22:28:29,397 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1626] 2022-11-08 22:28:29,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1627] 2022-11-08 22:28:29,398 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-08 22:28:29,398 >>   Total optimization steps = 870\n",
            "[INFO|trainer.py:1630] 2022-11-08 22:28:29,398 >>   Number of trainable parameters = 124439808\n",
            "[INFO|trainer.py:1651] 2022-11-08 22:28:29,398 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1652] 2022-11-08 22:28:29,398 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1653] 2022-11-08 22:28:29,399 >>   Continuing training from global step 500\n",
            "[INFO|trainer.py:1656] 2022-11-08 22:28:29,399 >>   Will skip the first 1 epochs then the first 210 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches:   0% 0/210 [00:00<?, ?it/s]\n",
            "Skipping the first batches: 100% 210/210 [00:02<00:00, 88.33it/s]\n",
            "\n",
            " 58% 501/870 [00:03<00:02, 127.24it/s]\u001b[A\n",
            " 59% 514/870 [00:12<00:10, 32.96it/s] \u001b[A\n",
            " 60% 520/870 [00:16<00:15, 22.56it/s]\u001b[A\n",
            " 60% 523/870 [00:17<00:18, 18.56it/s]\u001b[A\n",
            " 60% 525/870 [00:19<00:21, 15.93it/s]\u001b[A\n",
            " 61% 527/870 [00:20<00:25, 13.34it/s]\u001b[A\n",
            " 61% 528/870 [00:21<00:28, 12.00it/s]\u001b[A\n",
            " 61% 529/870 [00:21<00:32, 10.55it/s]\u001b[A\n",
            " 61% 530/870 [00:22<00:37,  9.05it/s]\u001b[A\n",
            " 61% 531/870 [00:23<00:44,  7.62it/s]\u001b[A\n",
            " 61% 532/870 [00:23<00:53,  6.31it/s]\u001b[A\n",
            " 61% 533/870 [00:24<01:04,  5.20it/s]\u001b[A\n",
            " 61% 534/870 [00:24<01:18,  4.27it/s]\u001b[A\n",
            " 61% 535/870 [00:25<01:34,  3.55it/s]\u001b[A\n",
            " 62% 536/870 [00:26<01:51,  3.01it/s]\u001b[A\n",
            " 62% 537/870 [00:26<02:07,  2.60it/s]\u001b[A\n",
            " 62% 538/870 [00:27<02:23,  2.31it/s]\u001b[A\n",
            " 62% 539/870 [00:28<02:37,  2.10it/s]\u001b[A\n",
            " 62% 540/870 [00:28<02:49,  1.94it/s]\u001b[A\n",
            " 62% 541/870 [00:29<02:59,  1.83it/s]\u001b[A\n",
            " 62% 542/870 [00:30<03:06,  1.76it/s]\u001b[A\n",
            " 62% 543/870 [00:30<03:11,  1.70it/s]\u001b[A\n",
            " 63% 544/870 [00:31<03:15,  1.67it/s]\u001b[A\n",
            " 63% 545/870 [00:31<03:18,  1.64it/s]\u001b[A\n",
            " 63% 546/870 [00:32<03:19,  1.62it/s]\u001b[A\n",
            " 63% 547/870 [00:33<03:21,  1.61it/s]\u001b[A\n",
            " 63% 548/870 [00:33<03:21,  1.60it/s]\u001b[A\n",
            " 63% 549/870 [00:34<03:21,  1.59it/s]\u001b[A\n",
            " 63% 550/870 [00:35<03:21,  1.59it/s]\u001b[A\n",
            " 63% 551/870 [00:35<03:21,  1.58it/s]\u001b[A\n",
            " 63% 552/870 [00:36<03:21,  1.58it/s]\u001b[A\n",
            " 64% 553/870 [00:37<03:20,  1.58it/s]\u001b[A\n",
            " 64% 554/870 [00:37<03:20,  1.58it/s]\u001b[A\n",
            " 64% 555/870 [00:38<03:19,  1.58it/s]\u001b[A\n",
            " 64% 556/870 [00:38<03:18,  1.58it/s]\u001b[A\n",
            " 64% 557/870 [00:39<03:18,  1.58it/s]\u001b[A\n",
            " 64% 558/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 559/870 [00:40<03:17,  1.58it/s]\u001b[A\n",
            " 64% 560/870 [00:41<03:16,  1.58it/s]\u001b[A\n",
            " 64% 561/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 562/870 [00:42<03:15,  1.58it/s]\u001b[A\n",
            " 65% 563/870 [00:43<03:14,  1.58it/s]\u001b[A\n",
            " 65% 564/870 [00:44<03:14,  1.57it/s]\u001b[A\n",
            " 65% 565/870 [00:44<03:14,  1.57it/s]\u001b[A\n",
            " 65% 566/870 [00:45<03:13,  1.57it/s]\u001b[A\n",
            " 65% 567/870 [00:45<03:13,  1.57it/s]\u001b[A\n",
            " 65% 568/870 [00:46<03:12,  1.57it/s]\u001b[A\n",
            " 65% 569/870 [00:47<03:11,  1.57it/s]\u001b[A\n",
            " 66% 570/870 [00:47<03:10,  1.57it/s]\u001b[A\n",
            " 66% 571/870 [00:48<03:09,  1.58it/s]\u001b[A\n",
            " 66% 572/870 [00:49<03:09,  1.58it/s]\u001b[A\n",
            " 66% 573/870 [00:49<03:08,  1.58it/s]\u001b[A\n",
            " 66% 574/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 575/870 [00:50<03:07,  1.58it/s]\u001b[A\n",
            " 66% 576/870 [00:51<03:06,  1.57it/s]\u001b[A\n",
            " 66% 577/870 [00:52<03:07,  1.57it/s]\u001b[A\n",
            " 66% 578/870 [00:52<03:06,  1.57it/s]\u001b[A\n",
            " 67% 579/870 [00:53<03:05,  1.57it/s]\u001b[A\n",
            " 67% 580/870 [00:54<02:51,  1.69it/s]\u001b[A\n",
            " 67% 581/870 [00:54<02:54,  1.65it/s]\u001b[A\n",
            " 67% 582/870 [00:55<02:56,  1.63it/s]\u001b[A\n",
            " 67% 583/870 [00:55<02:58,  1.61it/s]\u001b[A\n",
            " 67% 584/870 [00:56<02:58,  1.60it/s]\u001b[A\n",
            " 67% 585/870 [00:57<02:59,  1.59it/s]\u001b[A\n",
            " 67% 586/870 [00:57<03:00,  1.58it/s]\u001b[A\n",
            " 67% 587/870 [00:58<02:59,  1.57it/s]\u001b[A\n",
            " 68% 588/870 [00:59<02:59,  1.57it/s]\u001b[A\n",
            " 68% 589/870 [00:59<02:58,  1.57it/s]\u001b[A\n",
            " 68% 590/870 [01:00<02:58,  1.57it/s]\u001b[A\n",
            " 68% 591/870 [01:01<02:57,  1.57it/s]\u001b[A\n",
            " 68% 592/870 [01:01<02:56,  1.57it/s]\u001b[A\n",
            " 68% 593/870 [01:02<02:56,  1.57it/s]\u001b[A\n",
            " 68% 594/870 [01:02<02:55,  1.57it/s]\u001b[A\n",
            " 68% 595/870 [01:03<02:54,  1.57it/s]\u001b[A\n",
            " 69% 596/870 [01:04<02:54,  1.57it/s]\u001b[A\n",
            " 69% 597/870 [01:04<02:53,  1.57it/s]\u001b[A\n",
            " 69% 598/870 [01:05<02:52,  1.57it/s]\u001b[A\n",
            " 69% 599/870 [01:06<02:52,  1.57it/s]\u001b[A\n",
            " 69% 600/870 [01:06<02:51,  1.57it/s]\u001b[A\n",
            " 69% 601/870 [01:07<02:51,  1.57it/s]\u001b[A\n",
            " 69% 602/870 [01:08<02:50,  1.57it/s]\u001b[A\n",
            " 69% 603/870 [01:08<02:49,  1.57it/s]\u001b[A\n",
            " 69% 604/870 [01:09<02:49,  1.57it/s]\u001b[A\n",
            " 70% 605/870 [01:09<02:49,  1.57it/s]\u001b[A\n",
            " 70% 606/870 [01:10<02:48,  1.57it/s]\u001b[A\n",
            " 70% 607/870 [01:11<02:47,  1.57it/s]\u001b[A\n",
            " 70% 608/870 [01:11<02:46,  1.57it/s]\u001b[A\n",
            " 70% 609/870 [01:12<02:46,  1.57it/s]\u001b[A\n",
            " 70% 610/870 [01:13<02:45,  1.57it/s]\u001b[A\n",
            " 70% 611/870 [01:13<02:44,  1.57it/s]\u001b[A\n",
            " 70% 612/870 [01:14<02:44,  1.57it/s]\u001b[A\n",
            " 70% 613/870 [01:15<02:43,  1.57it/s]\u001b[A\n",
            " 71% 614/870 [01:15<02:43,  1.57it/s]\u001b[A\n",
            " 71% 615/870 [01:16<02:42,  1.57it/s]\u001b[A\n",
            " 71% 616/870 [01:16<02:41,  1.57it/s]\u001b[A\n",
            " 71% 617/870 [01:17<02:41,  1.57it/s]\u001b[A\n",
            " 71% 618/870 [01:18<02:40,  1.57it/s]\u001b[A\n",
            " 71% 619/870 [01:18<02:40,  1.57it/s]\u001b[A\n",
            " 71% 620/870 [01:19<02:39,  1.57it/s]\u001b[A\n",
            " 71% 621/870 [01:20<02:38,  1.57it/s]\u001b[A\n",
            " 71% 622/870 [01:20<02:38,  1.57it/s]\u001b[A\n",
            " 72% 623/870 [01:21<02:37,  1.57it/s]\u001b[A\n",
            " 72% 624/870 [01:22<02:36,  1.57it/s]\u001b[A\n",
            " 72% 625/870 [01:22<02:36,  1.57it/s]\u001b[A\n",
            " 72% 626/870 [01:23<02:35,  1.57it/s]\u001b[A\n",
            " 72% 627/870 [01:23<02:34,  1.57it/s]\u001b[A\n",
            " 72% 628/870 [01:24<02:34,  1.57it/s]\u001b[A\n",
            " 72% 629/870 [01:25<02:33,  1.57it/s]\u001b[A\n",
            " 72% 630/870 [01:25<02:32,  1.57it/s]\u001b[A\n",
            " 73% 631/870 [01:26<02:32,  1.57it/s]\u001b[A\n",
            " 73% 632/870 [01:27<02:31,  1.57it/s]\u001b[A\n",
            " 73% 633/870 [01:27<02:31,  1.57it/s]\u001b[A\n",
            " 73% 634/870 [01:28<02:30,  1.57it/s]\u001b[A\n",
            " 73% 635/870 [01:29<02:29,  1.57it/s]\u001b[A\n",
            " 73% 636/870 [01:29<02:28,  1.57it/s]\u001b[A\n",
            " 73% 637/870 [01:30<02:28,  1.57it/s]\u001b[A\n",
            " 73% 638/870 [01:30<02:27,  1.57it/s]\u001b[A\n",
            " 73% 639/870 [01:31<02:26,  1.57it/s]\u001b[A\n",
            " 74% 640/870 [01:32<02:26,  1.57it/s]\u001b[A\n",
            " 74% 641/870 [01:32<02:25,  1.57it/s]\u001b[A\n",
            " 74% 642/870 [01:33<02:25,  1.57it/s]\u001b[A\n",
            " 74% 643/870 [01:34<02:24,  1.57it/s]\u001b[A\n",
            " 74% 644/870 [01:34<02:23,  1.57it/s]\u001b[A\n",
            " 74% 645/870 [01:35<02:23,  1.57it/s]\u001b[A\n",
            " 74% 646/870 [01:36<02:22,  1.57it/s]\u001b[A\n",
            " 74% 647/870 [01:36<02:21,  1.57it/s]\u001b[A\n",
            " 74% 648/870 [01:37<02:21,  1.57it/s]\u001b[A\n",
            " 75% 649/870 [01:37<02:20,  1.57it/s]\u001b[A\n",
            " 75% 650/870 [01:38<02:19,  1.57it/s]\u001b[A\n",
            " 75% 651/870 [01:39<02:19,  1.57it/s]\u001b[A\n",
            " 75% 652/870 [01:39<02:18,  1.57it/s]\u001b[A\n",
            " 75% 653/870 [01:40<02:18,  1.57it/s]\u001b[A\n",
            " 75% 654/870 [01:41<02:17,  1.57it/s]\u001b[A\n",
            " 75% 655/870 [01:41<02:16,  1.57it/s]\u001b[A\n",
            " 75% 656/870 [01:42<02:16,  1.57it/s]\u001b[A\n",
            " 76% 657/870 [01:43<02:15,  1.57it/s]\u001b[A\n",
            " 76% 658/870 [01:43<02:14,  1.57it/s]\u001b[A\n",
            " 76% 659/870 [01:44<02:14,  1.57it/s]\u001b[A\n",
            " 76% 660/870 [01:44<02:14,  1.56it/s]\u001b[A\n",
            " 76% 661/870 [01:45<02:13,  1.57it/s]\u001b[A\n",
            " 76% 662/870 [01:46<02:12,  1.57it/s]\u001b[A\n",
            " 76% 663/870 [01:46<02:11,  1.57it/s]\u001b[A\n",
            " 76% 664/870 [01:47<02:11,  1.57it/s]\u001b[A\n",
            " 76% 665/870 [01:48<02:10,  1.57it/s]\u001b[A\n",
            " 77% 666/870 [01:48<02:09,  1.57it/s]\u001b[A\n",
            " 77% 667/870 [01:49<02:09,  1.57it/s]\u001b[A\n",
            " 77% 668/870 [01:50<02:08,  1.57it/s]\u001b[A\n",
            " 77% 669/870 [01:50<02:07,  1.57it/s]\u001b[A\n",
            " 77% 670/870 [01:51<02:07,  1.57it/s]\u001b[A\n",
            " 77% 671/870 [01:51<02:07,  1.57it/s]\u001b[A\n",
            " 77% 672/870 [01:52<02:06,  1.57it/s]\u001b[A\n",
            " 77% 673/870 [01:53<02:05,  1.57it/s]\u001b[A\n",
            " 77% 674/870 [01:53<02:04,  1.57it/s]\u001b[A\n",
            " 78% 675/870 [01:54<02:04,  1.57it/s]\u001b[A\n",
            " 78% 676/870 [01:55<02:03,  1.57it/s]\u001b[A\n",
            " 78% 677/870 [01:55<02:02,  1.57it/s]\u001b[A\n",
            " 78% 678/870 [01:56<02:02,  1.57it/s]\u001b[A\n",
            " 78% 679/870 [01:57<02:01,  1.57it/s]\u001b[A\n",
            " 78% 680/870 [01:57<02:00,  1.57it/s]\u001b[A\n",
            " 78% 681/870 [01:58<02:00,  1.57it/s]\u001b[A\n",
            " 78% 682/870 [01:58<01:59,  1.57it/s]\u001b[A\n",
            " 79% 683/870 [01:59<01:58,  1.57it/s]\u001b[A\n",
            " 79% 684/870 [02:00<01:58,  1.57it/s]\u001b[A\n",
            " 79% 685/870 [02:00<01:57,  1.57it/s]\u001b[A\n",
            " 79% 686/870 [02:01<01:56,  1.57it/s]\u001b[A\n",
            " 79% 687/870 [02:02<01:56,  1.57it/s]\u001b[A\n",
            " 79% 688/870 [02:02<01:55,  1.57it/s]\u001b[A\n",
            " 79% 689/870 [02:03<01:54,  1.57it/s]\u001b[A\n",
            " 79% 690/870 [02:04<01:54,  1.57it/s]\u001b[A\n",
            " 79% 691/870 [02:04<01:53,  1.58it/s]\u001b[A\n",
            " 80% 692/870 [02:05<01:52,  1.58it/s]\u001b[A\n",
            " 80% 693/870 [02:05<01:52,  1.57it/s]\u001b[A\n",
            " 80% 694/870 [02:06<01:51,  1.57it/s]\u001b[A\n",
            " 80% 695/870 [02:07<01:51,  1.57it/s]\u001b[A\n",
            " 80% 696/870 [02:07<01:50,  1.58it/s]\u001b[A\n",
            " 80% 697/870 [02:08<01:49,  1.58it/s]\u001b[A\n",
            " 80% 698/870 [02:09<01:49,  1.58it/s]\u001b[A\n",
            " 80% 699/870 [02:09<01:48,  1.58it/s]\u001b[A\n",
            " 80% 700/870 [02:10<01:47,  1.58it/s]\u001b[A\n",
            " 81% 701/870 [02:11<01:47,  1.58it/s]\u001b[A\n",
            " 81% 702/870 [02:11<01:46,  1.58it/s]\u001b[A\n",
            " 81% 703/870 [02:12<01:45,  1.58it/s]\u001b[A\n",
            " 81% 704/870 [02:12<01:45,  1.58it/s]\u001b[A\n",
            " 81% 705/870 [02:13<01:45,  1.57it/s]\u001b[A\n",
            " 81% 706/870 [02:14<01:44,  1.57it/s]\u001b[A\n",
            " 81% 707/870 [02:14<01:43,  1.57it/s]\u001b[A\n",
            " 81% 708/870 [02:15<01:42,  1.57it/s]\u001b[A\n",
            " 81% 709/870 [02:16<01:42,  1.57it/s]\u001b[A\n",
            " 82% 710/870 [02:16<01:41,  1.57it/s]\u001b[A\n",
            " 82% 711/870 [02:17<01:40,  1.57it/s]\u001b[A\n",
            " 82% 712/870 [02:18<01:40,  1.57it/s]\u001b[A\n",
            " 82% 713/870 [02:18<01:39,  1.57it/s]\u001b[A\n",
            " 82% 714/870 [02:19<01:39,  1.57it/s]\u001b[A\n",
            " 82% 715/870 [02:19<01:38,  1.57it/s]\u001b[A\n",
            " 82% 716/870 [02:20<01:38,  1.57it/s]\u001b[A\n",
            " 82% 717/870 [02:21<01:37,  1.57it/s]\u001b[A\n",
            " 83% 718/870 [02:21<01:36,  1.57it/s]\u001b[A\n",
            " 83% 719/870 [02:22<01:36,  1.57it/s]\u001b[A\n",
            " 83% 720/870 [02:23<01:35,  1.57it/s]\u001b[A\n",
            " 83% 721/870 [02:23<01:34,  1.57it/s]\u001b[A\n",
            " 83% 722/870 [02:24<01:34,  1.57it/s]\u001b[A\n",
            " 83% 723/870 [02:25<01:33,  1.57it/s]\u001b[A\n",
            " 83% 724/870 [02:25<01:33,  1.56it/s]\u001b[A\n",
            " 83% 725/870 [02:26<01:32,  1.56it/s]\u001b[A\n",
            " 83% 726/870 [02:26<01:32,  1.56it/s]\u001b[A\n",
            " 84% 727/870 [02:27<01:31,  1.56it/s]\u001b[A\n",
            " 84% 728/870 [02:28<01:30,  1.57it/s]\u001b[A\n",
            " 84% 729/870 [02:28<01:29,  1.57it/s]\u001b[A\n",
            " 84% 730/870 [02:29<01:29,  1.57it/s]\u001b[A\n",
            " 84% 731/870 [02:30<01:28,  1.57it/s]\u001b[A\n",
            " 84% 732/870 [02:30<01:27,  1.57it/s]\u001b[A\n",
            " 84% 733/870 [02:31<01:27,  1.57it/s]\u001b[A\n",
            " 84% 734/870 [02:32<01:26,  1.57it/s]\u001b[A\n",
            " 84% 735/870 [02:32<01:25,  1.57it/s]\u001b[A\n",
            " 85% 736/870 [02:33<01:25,  1.57it/s]\u001b[A\n",
            " 85% 737/870 [02:33<01:24,  1.57it/s]\u001b[A\n",
            " 85% 738/870 [02:34<01:24,  1.57it/s]\u001b[A\n",
            " 85% 739/870 [02:35<01:23,  1.57it/s]\u001b[A\n",
            " 85% 740/870 [02:35<01:22,  1.57it/s]\u001b[A\n",
            " 85% 741/870 [02:36<01:22,  1.57it/s]\u001b[A\n",
            " 85% 742/870 [02:37<01:21,  1.57it/s]\u001b[A\n",
            " 85% 743/870 [02:37<01:20,  1.57it/s]\u001b[A\n",
            " 86% 744/870 [02:38<01:20,  1.57it/s]\u001b[A\n",
            " 86% 745/870 [02:39<01:19,  1.57it/s]\u001b[A\n",
            " 86% 746/870 [02:39<01:18,  1.57it/s]\u001b[A\n",
            " 86% 747/870 [02:40<01:18,  1.57it/s]\u001b[A\n",
            " 86% 748/870 [02:40<01:17,  1.57it/s]\u001b[A\n",
            " 86% 749/870 [02:41<01:16,  1.57it/s]\u001b[A\n",
            " 86% 750/870 [02:42<01:16,  1.57it/s]\u001b[A\n",
            " 86% 751/870 [02:42<01:15,  1.57it/s]\u001b[A\n",
            " 86% 752/870 [02:43<01:15,  1.57it/s]\u001b[A\n",
            " 87% 753/870 [02:44<01:14,  1.57it/s]\u001b[A\n",
            " 87% 754/870 [02:44<01:13,  1.57it/s]\u001b[A\n",
            " 87% 755/870 [02:45<01:13,  1.57it/s]\u001b[A\n",
            " 87% 756/870 [02:46<01:12,  1.57it/s]\u001b[A\n",
            " 87% 757/870 [02:46<01:11,  1.57it/s]\u001b[A\n",
            " 87% 758/870 [02:47<01:11,  1.57it/s]\u001b[A\n",
            " 87% 759/870 [02:47<01:10,  1.57it/s]\u001b[A\n",
            " 87% 760/870 [02:48<01:09,  1.57it/s]\u001b[A\n",
            " 87% 761/870 [02:49<01:09,  1.57it/s]\u001b[A\n",
            " 88% 762/870 [02:49<01:08,  1.57it/s]\u001b[A\n",
            " 88% 763/870 [02:50<01:08,  1.57it/s]\u001b[A\n",
            " 88% 764/870 [02:51<01:07,  1.57it/s]\u001b[A\n",
            " 88% 765/870 [02:51<01:06,  1.57it/s]\u001b[A\n",
            " 88% 766/870 [02:52<01:06,  1.57it/s]\u001b[A\n",
            " 88% 767/870 [02:53<01:05,  1.57it/s]\u001b[A\n",
            " 88% 768/870 [02:53<01:04,  1.57it/s]\u001b[A\n",
            " 88% 769/870 [02:54<01:04,  1.57it/s]\u001b[A\n",
            " 89% 770/870 [02:54<01:03,  1.57it/s]\u001b[A\n",
            " 89% 771/870 [02:55<01:03,  1.57it/s]\u001b[A\n",
            " 89% 772/870 [02:56<01:02,  1.57it/s]\u001b[A\n",
            " 89% 773/870 [02:56<01:01,  1.57it/s]\u001b[A\n",
            " 89% 774/870 [02:57<01:01,  1.57it/s]\u001b[A\n",
            " 89% 775/870 [02:58<01:00,  1.57it/s]\u001b[A\n",
            " 89% 776/870 [02:58<00:59,  1.57it/s]\u001b[A\n",
            " 89% 777/870 [02:59<00:59,  1.57it/s]\u001b[A\n",
            " 89% 778/870 [03:00<00:58,  1.57it/s]\u001b[A\n",
            " 90% 779/870 [03:00<00:57,  1.57it/s]\u001b[A\n",
            " 90% 780/870 [03:01<00:57,  1.57it/s]\u001b[A\n",
            " 90% 781/870 [03:01<00:56,  1.57it/s]\u001b[A\n",
            " 90% 782/870 [03:02<00:56,  1.57it/s]\u001b[A\n",
            " 90% 783/870 [03:03<00:55,  1.57it/s]\u001b[A\n",
            " 90% 784/870 [03:03<00:54,  1.57it/s]\u001b[A\n",
            " 90% 785/870 [03:04<00:54,  1.57it/s]\u001b[A\n",
            " 90% 786/870 [03:05<00:53,  1.57it/s]\u001b[A\n",
            " 90% 787/870 [03:05<00:52,  1.57it/s]\u001b[A\n",
            " 91% 788/870 [03:06<00:52,  1.57it/s]\u001b[A\n",
            " 91% 789/870 [03:07<00:51,  1.57it/s]\u001b[A\n",
            " 91% 790/870 [03:07<00:50,  1.57it/s]\u001b[A\n",
            " 91% 791/870 [03:08<00:50,  1.57it/s]\u001b[A\n",
            " 91% 792/870 [03:08<00:49,  1.57it/s]\u001b[A\n",
            " 91% 793/870 [03:09<00:48,  1.57it/s]\u001b[A\n",
            " 91% 794/870 [03:10<00:48,  1.57it/s]\u001b[A\n",
            " 91% 795/870 [03:10<00:47,  1.57it/s]\u001b[A\n",
            " 91% 796/870 [03:11<00:47,  1.57it/s]\u001b[A\n",
            " 92% 797/870 [03:12<00:46,  1.57it/s]\u001b[A\n",
            " 92% 798/870 [03:12<00:45,  1.57it/s]\u001b[A\n",
            " 92% 799/870 [03:13<00:45,  1.57it/s]\u001b[A\n",
            " 92% 800/870 [03:14<00:44,  1.57it/s]\u001b[A\n",
            " 92% 801/870 [03:14<00:43,  1.57it/s]\u001b[A\n",
            " 92% 802/870 [03:15<00:43,  1.57it/s]\u001b[A\n",
            " 92% 803/870 [03:15<00:42,  1.57it/s]\u001b[A\n",
            " 92% 804/870 [03:16<00:42,  1.57it/s]\u001b[A\n",
            " 93% 805/870 [03:17<00:41,  1.57it/s]\u001b[A\n",
            " 93% 806/870 [03:17<00:40,  1.57it/s]\u001b[A\n",
            " 93% 807/870 [03:18<00:40,  1.57it/s]\u001b[A\n",
            " 93% 808/870 [03:19<00:39,  1.57it/s]\u001b[A\n",
            " 93% 809/870 [03:19<00:38,  1.57it/s]\u001b[A\n",
            " 93% 810/870 [03:20<00:38,  1.57it/s]\u001b[A\n",
            " 93% 811/870 [03:21<00:37,  1.57it/s]\u001b[A\n",
            " 93% 812/870 [03:21<00:36,  1.57it/s]\u001b[A\n",
            " 93% 813/870 [03:22<00:36,  1.57it/s]\u001b[A\n",
            " 94% 814/870 [03:22<00:35,  1.57it/s]\u001b[A\n",
            " 94% 815/870 [03:23<00:34,  1.57it/s]\u001b[A\n",
            " 94% 816/870 [03:24<00:34,  1.57it/s]\u001b[A\n",
            " 94% 817/870 [03:24<00:33,  1.57it/s]\u001b[A\n",
            " 94% 818/870 [03:25<00:33,  1.57it/s]\u001b[A\n",
            " 94% 819/870 [03:26<00:32,  1.57it/s]\u001b[A\n",
            " 94% 820/870 [03:26<00:31,  1.57it/s]\u001b[A\n",
            " 94% 821/870 [03:27<00:31,  1.57it/s]\u001b[A\n",
            " 94% 822/870 [03:28<00:30,  1.57it/s]\u001b[A\n",
            " 95% 823/870 [03:28<00:29,  1.57it/s]\u001b[A\n",
            " 95% 824/870 [03:29<00:29,  1.57it/s]\u001b[A\n",
            " 95% 825/870 [03:29<00:28,  1.57it/s]\u001b[A\n",
            " 95% 826/870 [03:30<00:27,  1.57it/s]\u001b[A\n",
            " 95% 827/870 [03:31<00:27,  1.57it/s]\u001b[A\n",
            " 95% 828/870 [03:31<00:26,  1.57it/s]\u001b[A\n",
            " 95% 829/870 [03:32<00:26,  1.57it/s]\u001b[A\n",
            " 95% 830/870 [03:33<00:25,  1.57it/s]\u001b[A\n",
            " 96% 831/870 [03:33<00:24,  1.57it/s]\u001b[A\n",
            " 96% 832/870 [03:34<00:24,  1.57it/s]\u001b[A\n",
            " 96% 833/870 [03:35<00:23,  1.57it/s]\u001b[A\n",
            " 96% 834/870 [03:35<00:22,  1.57it/s]\u001b[A\n",
            " 96% 835/870 [03:36<00:22,  1.57it/s]\u001b[A\n",
            " 96% 836/870 [03:36<00:21,  1.57it/s]\u001b[A\n",
            " 96% 837/870 [03:37<00:20,  1.57it/s]\u001b[A\n",
            " 96% 838/870 [03:38<00:20,  1.57it/s]\u001b[A\n",
            " 96% 839/870 [03:38<00:19,  1.57it/s]\u001b[A\n",
            " 97% 840/870 [03:39<00:19,  1.57it/s]\u001b[A\n",
            " 97% 841/870 [03:40<00:18,  1.57it/s]\u001b[A\n",
            " 97% 842/870 [03:40<00:17,  1.57it/s]\u001b[A\n",
            " 97% 843/870 [03:41<00:17,  1.57it/s]\u001b[A\n",
            " 97% 844/870 [03:42<00:16,  1.57it/s]\u001b[A\n",
            " 97% 845/870 [03:42<00:15,  1.57it/s]\u001b[A\n",
            " 97% 846/870 [03:43<00:15,  1.57it/s]\u001b[A\n",
            " 97% 847/870 [03:43<00:14,  1.57it/s]\u001b[A\n",
            " 97% 848/870 [03:44<00:13,  1.57it/s]\u001b[A\n",
            " 98% 849/870 [03:45<00:13,  1.57it/s]\u001b[A\n",
            " 98% 850/870 [03:45<00:12,  1.58it/s]\u001b[A\n",
            " 98% 851/870 [03:46<00:12,  1.57it/s]\u001b[A\n",
            " 98% 852/870 [03:47<00:11,  1.57it/s]\u001b[A\n",
            " 98% 853/870 [03:47<00:10,  1.57it/s]\u001b[A\n",
            " 98% 854/870 [03:48<00:10,  1.57it/s]\u001b[A\n",
            " 98% 855/870 [03:49<00:09,  1.57it/s]\u001b[A\n",
            " 98% 856/870 [03:49<00:08,  1.57it/s]\u001b[A\n",
            " 99% 857/870 [03:50<00:08,  1.57it/s]\u001b[A\n",
            " 99% 858/870 [03:50<00:07,  1.57it/s]\u001b[A\n",
            " 99% 859/870 [03:51<00:07,  1.57it/s]\u001b[A\n",
            " 99% 860/870 [03:52<00:06,  1.57it/s]\u001b[A\n",
            " 99% 861/870 [03:52<00:05,  1.57it/s]\u001b[A\n",
            " 99% 862/870 [03:53<00:05,  1.57it/s]\u001b[A\n",
            " 99% 863/870 [03:54<00:04,  1.57it/s]\u001b[A\n",
            " 99% 864/870 [03:54<00:03,  1.57it/s]\u001b[A\n",
            " 99% 865/870 [03:55<00:03,  1.57it/s]\u001b[A\n",
            "100% 866/870 [03:56<00:02,  1.57it/s]\u001b[A\n",
            "100% 867/870 [03:56<00:01,  1.57it/s]\u001b[A\n",
            "100% 868/870 [03:57<00:01,  1.57it/s]\u001b[A\n",
            "100% 869/870 [03:57<00:00,  1.57it/s]\u001b[A\n",
            "100% 870/870 [03:58<00:00,  1.69it/s]\u001b[A[INFO|trainer.py:1873] 2022-11-08 22:32:27,884 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A{'train_runtime': 238.4859, 'train_samples_per_second': 29.159, 'train_steps_per_second': 3.648, 'train_loss': 2.416890321928879, 'epoch': 3.0}\n",
            "\n",
            "100% 870/870 [03:58<00:00,  3.65it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-08 22:32:27,886 >> Saving model checkpoint to /content/test-clm/gpt-2-0.99\n",
            "[INFO|configuration_utils.py:447] 2022-11-08 22:32:27,887 >> Configuration saved in /content/test-clm/gpt-2-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-08 22:32:29,407 >> Model weights saved in /content/test-clm/gpt-2-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-08 22:32:29,408 >> tokenizer config file saved in /content/test-clm/gpt-2-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-08 22:32:29,408 >> Special tokens file saved in /content/test-clm/gpt-2-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.4169\n",
            "  train_runtime            = 0:03:58.48\n",
            "  train_samples            =       2318\n",
            "  train_samples_per_second =     29.159\n",
            "  train_steps_per_second   =      3.648\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2943] 2022-11-08 22:32:29,523 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-08 22:32:29,523 >>   Num examples = 240\n",
            "[INFO|trainer.py:2948] 2022-11-08 22:32:29,523 >>   Batch size = 8\n",
            "100% 30/30 [00:07<00:00,  4.06it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.1833\n",
            "  eval_loss               =     5.6918\n",
            "  eval_runtime            = 0:00:07.59\n",
            "  eval_samples            =        240\n",
            "  eval_samples_per_second =     31.582\n",
            "  eval_steps_per_second   =      3.948\n",
            "  perplexity              =   296.4128\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path /content/models/gpt-2-0.99\\\n",
        "    --tokenizer_name \"gpt2\" \\\n",
        "    --dataset_name wikitext \\\n",
        "    --dataset_config_name wikitext-2-raw-v1 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8\\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/test-clm/gpt-2-0.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk_jOPoGceOI"
      },
      "source": [
        "Text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpjeF9OSYVDM",
        "outputId": "b079c9e5-4dab-47b7-afa7-b07dae741b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-09 05:57:19--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27264 (27K) [text/plain]\n",
            "Saving to: ‘run_glue.py’\n",
            "\n",
            "run_glue.py         100%[===================>]  26.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-09 05:57:19 (130 MB/s) - ‘run_glue.py’ saved [27264/27264]\n",
            "\n",
            "--2022-11-09 05:57:19--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [text/plain]\n",
            "Saving to: ‘requirements.txt.1’\n",
            "\n",
            "requirements.txt.1  100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-09 05:57:19 (6.96 MB/s) - ‘requirements.txt.1’ saved [102/102]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get codes from huggingface\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qd88j3ZkeZXd",
        "outputId": "0b050774-c86a-4a9d-b5ae-246227bc9751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.8.0\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 45.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.12.1+cu113)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2022.10.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 84.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (6.0)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.13.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 2)) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.6)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets, sentencepiece, evaluate, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed accelerate-0.14.0 datasets-2.6.1 dill-0.3.5.1 evaluate-0.3.0 multiprocess-0.70.13 responses-0.18.0 sentencepiece-0.1.97 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dill",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4wSela6dVW0",
        "outputId": "01b60636-812b-411d-aa61-fb65fb773c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart/runs/Nov09_05-18-13_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 756.28it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:18:19,318 >> loading configuration file /content/models/bart/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:18:19,321 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:18:20,244 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:18:21,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:18:21,204 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 634kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 387kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 1.14MB/s]\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,288 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,289 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,289 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,289 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,289 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:18:34,289 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:18:34,289 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:18:34,290 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:18:34,384 >> loading weights file /content/models/bart/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2609] 2022-11-09 05:18:36,536 >> All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:18:36,536 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "Running tokenizer on dataset:  75% 3/4 [00:00<00:00, 10.05ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cbe5274bc47a86ab.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2a63981699a1377c.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  8.92ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:18:40,929 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:18:40,942 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:18:40,942 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:18:40,942 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:18:40,942 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:18:40,942 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:18:40,942 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:18:40,942 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:18:40,943 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.03it/s][INFO|trainer.py:1873] 2022-11-09 05:19:51,653 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.7103, 'train_samples_per_second': 155.621, 'train_steps_per_second': 2.461, 'train_loss': 0.4735383220102595, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.47it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:19:51,655 >> Saving model checkpoint to tmp/bart/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:19:51,656 >> Configuration saved in tmp/bart/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:19:52,915 >> Model weights saved in tmp/bart/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:19:52,916 >> tokenizer config file saved in tmp/bart/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:19:52,916 >> Special tokens file saved in tmp/bart/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.4735\n",
            "  train_runtime            = 0:01:10.71\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    155.621\n",
            "  train_steps_per_second   =      2.461\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:19:53,027 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:19:53,029 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:19:53,029 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:19:53,029 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 41.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8407\n",
            "  eval_combined_score     =      0.865\n",
            "  eval_f1                 =     0.8893\n",
            "  eval_loss               =     0.3574\n",
            "  eval_runtime            = 0:00:01.23\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    329.139\n",
            "  eval_steps_per_second   =     41.142\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart-0.1 \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart-0.1/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmjDLRC-wKNA",
        "outputId": "66a6fb9c-c67d-4832-bb52-89fc71db9843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart-0.1/runs/Nov09_05-20-01_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart-0.1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart-0.1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 797.50it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:20:07,054 >> loading configuration file /content/models/bart-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:20:07,057 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.1\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:20:07,974 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:20:08,904 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:20:08,905 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:20:10,770 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:20:10,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:20:10,771 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:20:10,860 >> loading weights file /content/models/bart-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:20:13,851 >> Some weights of the model checkpoint at /content/models/bart-0.1 were not used when initializing BartForSequenceClassification: ['decoder.layers.3.encoder_attn.k_proj.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.2.fc2.weight_orig', 'encoder.layers.5.fc1.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.0.fc2.weight_orig', 'encoder.layers.3.fc1.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.fc2.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'encoder.layers.1.fc1.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.fc2.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.2.fc1.weight_mask', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.4.fc1.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'encoder.layers.5.fc1.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'encoder.layers.0.fc1.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.1.fc2.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_mask']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:20:13,928 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart-0.1 and are newly initialized: ['encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'decoder.layers.2.fc2.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.4.fc2.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'classification_head.out_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'classification_head.dense.weight', 'encoder.layers.3.fc1.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'classification_head.dense.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.0.fc1.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'encoder.layers.0.fc2.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.1.fc2.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.4.fc1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'encoder.layers.2.fc2.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'encoder.layers.3.fc2.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'classification_head.out_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c3b3c56fffffa3f.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2a63981699a1377c.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:20:18,094 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:20:18,107 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:20:18,107 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:20:18,107 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:20:18,107 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:20:18,107 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:20:18,107 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:20:18,107 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:20:18,108 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.03it/s][INFO|trainer.py:1873] 2022-11-09 05:21:28,594 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.4859, 'train_samples_per_second': 156.116, 'train_steps_per_second': 2.469, 'train_loss': 0.599645987324331, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.47it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:21:28,596 >> Saving model checkpoint to tmp/bart-0.1/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:21:28,597 >> Configuration saved in tmp/bart-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:21:29,800 >> Model weights saved in tmp/bart-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:21:29,801 >> tokenizer config file saved in tmp/bart-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:21:29,801 >> Special tokens file saved in tmp/bart-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5996\n",
            "  train_runtime            = 0:01:10.48\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    156.116\n",
            "  train_steps_per_second   =      2.469\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:21:29,908 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:21:29,910 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:21:29,910 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:21:29,910 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 44.16it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6789\n",
            "  eval_combined_score     =     0.7333\n",
            "  eval_f1                 =     0.7877\n",
            "  eval_loss               =     0.5858\n",
            "  eval_runtime            = 0:00:01.17\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    346.639\n",
            "  eval_steps_per_second   =      43.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart-0.5 \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart-0.5/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Sq8QhxwL9V",
        "outputId": "75a2c663-9834-435e-ee04-e2cfce52fead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart-0.5/runs/Nov09_05-21-36_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart-0.5/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart-0.5/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 792.67it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:21:42,385 >> loading configuration file /content/models/bart-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:21:42,388 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.5\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:21:43,366 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:21:44,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:21:44,327 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,254 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,254 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,255 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,255 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,255 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:21:46,255 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:21:46,255 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:21:46,256 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:21:46,349 >> loading weights file /content/models/bart-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:21:49,329 >> Some weights of the model checkpoint at /content/models/bart-0.5 were not used when initializing BartForSequenceClassification: ['decoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.0.fc1.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.3.fc2.weight_orig', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.5.fc2.weight_orig', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.2.fc2.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.0.fc2.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'encoder.layers.5.fc1.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.0.fc1.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.1.fc1.weight_mask', 'decoder.layers.2.fc1.weight_mask', 'encoder.layers.5.fc2.weight_orig', 'decoder.layers.1.fc2.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.5.fc2.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'encoder.layers.5.fc1.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'encoder.layers.2.fc2.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'decoder.layers.5.fc1.weight_orig', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.3.fc1.weight_mask', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.3.fc1.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.0.fc2.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.2.fc2.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_orig']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:21:49,402 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart-0.5 and are newly initialized: ['decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'encoder.layers.4.fc1.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'decoder.layers.4.fc1.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.fc1.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.1.fc1.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.3.fc1.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.3.fc2.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.5.fc2.weight', 'encoder.layers.4.fc2.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.0.fc1.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'classification_head.out_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.0.fc2.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'classification_head.dense.weight', 'encoder.layers.0.fc2.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c3b3c56fffffa3f.arrow\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-329abb9a77ad4f4c.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  7.33ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:21:53,649 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:21:53,661 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:21:53,661 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:21:53,661 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:21:53,662 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:21:53,662 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:21:53,662 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:21:53,662 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:21:53,662 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.03it/s][INFO|trainer.py:1873] 2022-11-09 05:23:04,216 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.554, 'train_samples_per_second': 155.966, 'train_steps_per_second': 2.466, 'train_loss': 0.599645987324331, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.47it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:23:04,218 >> Saving model checkpoint to tmp/bart-0.5/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:23:04,219 >> Configuration saved in tmp/bart-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:23:05,472 >> Model weights saved in tmp/bart-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:23:05,473 >> tokenizer config file saved in tmp/bart-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:23:05,473 >> Special tokens file saved in tmp/bart-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5996\n",
            "  train_runtime            = 0:01:10.55\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    155.966\n",
            "  train_steps_per_second   =      2.466\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:23:05,576 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:23:05,578 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:23:05,578 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:23:05,578 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 43.87it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6789\n",
            "  eval_combined_score     =     0.7333\n",
            "  eval_f1                 =     0.7877\n",
            "  eval_loss               =     0.5858\n",
            "  eval_runtime            = 0:00:01.18\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    344.497\n",
            "  eval_steps_per_second   =     43.062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart-0.9 \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart-0.9/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iGO8qXiwNPy",
        "outputId": "669ee240-c41b-419f-cafc-354482b45af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart-0.9/runs/Nov09_05-23-12_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart-0.9/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart-0.9/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 828.53it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:23:18,045 >> loading configuration file /content/models/bart-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:23:18,049 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.9\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:23:19,024 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:23:19,987 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:23:19,988 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:23:21,894 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:23:21,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:23:21,896 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:23:21,987 >> loading weights file /content/models/bart-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:23:24,978 >> Some weights of the model checkpoint at /content/models/bart-0.9 were not used when initializing BartForSequenceClassification: ['encoder.layers.1.fc1.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.0.fc1.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.2.fc2.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'encoder.layers.5.fc1.weight_mask', 'encoder.layers.1.fc2.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'encoder.layers.2.fc2.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.3.fc1.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.2.fc1.weight_mask', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.1.fc2.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.0.fc2.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.5.fc1.weight_mask', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.3.fc1.weight_orig', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.4.fc1.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.fc2.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.0.fc2.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.3.fc2.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'encoder.layers.1.fc1.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.0.fc2.weight_orig', 'encoder.layers.5.fc2.weight_mask']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:23:24,979 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart-0.9 and are newly initialized: ['decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'classification_head.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.2.fc2.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'classification_head.dense.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.2.fc1.weight', 'classification_head.dense.weight', 'encoder.layers.4.fc2.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.fc1.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'classification_head.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.0.fc2.weight', 'encoder.layers.3.fc2.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.fc2.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.2.fc1.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.fc1.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.5.fc1.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.5.fc2.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c3b3c56fffffa3f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-329abb9a77ad4f4c.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:23:29,099 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:23:29,112 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:23:29,112 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:23:29,112 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:23:29,112 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:23:29,112 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:23:29,112 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:23:29,112 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:23:29,113 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.03it/s][INFO|trainer.py:1873] 2022-11-09 05:24:39,674 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.5616, 'train_samples_per_second': 155.949, 'train_steps_per_second': 2.466, 'train_loss': 0.599645987324331, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.47it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:24:39,676 >> Saving model checkpoint to tmp/bart-0.9/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:24:39,677 >> Configuration saved in tmp/bart-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:24:40,962 >> Model weights saved in tmp/bart-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:24:40,963 >> tokenizer config file saved in tmp/bart-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:24:40,963 >> Special tokens file saved in tmp/bart-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5996\n",
            "  train_runtime            = 0:01:10.56\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    155.949\n",
            "  train_steps_per_second   =      2.466\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:24:41,068 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:24:41,070 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:24:41,070 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:24:41,070 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 43.94it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6789\n",
            "  eval_combined_score     =     0.7333\n",
            "  eval_f1                 =     0.7877\n",
            "  eval_loss               =     0.5858\n",
            "  eval_runtime            = 0:00:01.18\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    345.081\n",
            "  eval_steps_per_second   =     43.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart-0.95 \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart-0.95/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02k8FIgSwPAy",
        "outputId": "22a26c34-a012-4530-a7c6-920b0f78811a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart-0.95/runs/Nov09_05-24-47_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart-0.95/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart-0.95/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 780.87it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:24:53,485 >> loading configuration file /content/models/bart-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:24:53,488 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.95\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:24:54,403 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:24:55,352 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:24:55,353 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:24:57,255 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:24:57,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:24:57,256 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:24:57,346 >> loading weights file /content/models/bart-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:25:00,366 >> Some weights of the model checkpoint at /content/models/bart-0.95 were not used when initializing BartForSequenceClassification: ['decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.2.fc2.weight_orig', 'encoder.layers.2.fc2.weight_orig', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'encoder.layers.0.fc1.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'encoder.layers.0.fc2.weight_mask', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.5.fc1.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.1.fc2.weight_mask', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.2.fc2.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'encoder.layers.0.fc1.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.2.fc1.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.2.fc1.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.4.fc2.weight_mask', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'encoder.layers.4.fc1.weight_mask', 'decoder.layers.2.fc1.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.2.fc2.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'encoder.layers.5.fc2.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.fc1.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.5.fc1.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'encoder.layers.5.fc1.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.3.fc1.weight_orig', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'encoder.layers.0.fc2.weight_orig', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.4.fc2.weight_orig', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.fc1.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.3.fc2.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.1.fc2.weight_mask', 'encoder.layers.5.fc2.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.0.fc2.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.1.fc2.weight_orig']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:25:00,377 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart-0.95 and are newly initialized: ['decoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.fc1.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.5.fc2.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.fc2.weight', 'classification_head.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.fc2.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.4.fc2.weight', 'decoder.layers.0.fc2.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.fc1.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.fc2.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.fc1.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'classification_head.out_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.fc2.weight', 'classification_head.dense.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'classification_head.dense.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'encoder.layers.0.fc1.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.2.fc2.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.1.fc1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c3b3c56fffffa3f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-329abb9a77ad4f4c.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:25:04,454 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:25:04,466 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:25:04,466 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:25:04,466 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:25:04,466 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:25:04,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:25:04,466 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:25:04,467 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:25:04,467 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.03it/s][INFO|trainer.py:1873] 2022-11-09 05:26:15,068 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.6009, 'train_samples_per_second': 155.862, 'train_steps_per_second': 2.465, 'train_loss': 0.599645987324331, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.46it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:26:15,070 >> Saving model checkpoint to tmp/bart-0.95/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:26:15,071 >> Configuration saved in tmp/bart-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:26:16,169 >> Model weights saved in tmp/bart-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:26:16,170 >> tokenizer config file saved in tmp/bart-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:26:16,170 >> Special tokens file saved in tmp/bart-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5996\n",
            "  train_runtime            = 0:01:10.60\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    155.862\n",
            "  train_steps_per_second   =      2.465\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:26:16,274 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:26:16,276 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:26:16,276 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:26:16,276 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 44.02it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6789\n",
            "  eval_combined_score     =     0.7333\n",
            "  eval_f1                 =     0.7877\n",
            "  eval_loss               =     0.5858\n",
            "  eval_runtime            = 0:00:01.18\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    345.508\n",
            "  eval_steps_per_second   =     43.188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgHsKaSzff_J",
        "outputId": "fac99128-5f22-4fde-df21-1e03d5830cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bart-0.99/runs/Nov09_05-26-22_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bart-0.99/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bart-0.99/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 828.26it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:26:28,782 >> loading configuration file /content/models/bart-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:26:28,786 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"/content/models/bart-0.99\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:439] 2022-11-09 05:26:29,698 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:26:30,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:26:30,675 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,553 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,553 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,553 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,554 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,554 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:26:32,554 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:26:32,554 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:26:32,555 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:26:32,646 >> loading weights file /content/models/bart-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:26:35,633 >> Some weights of the model checkpoint at /content/models/bart-0.99 were not used when initializing BartForSequenceClassification: ['decoder.layers.0.self_attn.v_proj.weight_mask', 'decoder.layers.3.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.out_proj.weight_mask', 'decoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.4.fc1.weight_orig', 'encoder.layers.5.fc2.weight_orig', 'decoder.layers.5.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.fc1.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn.q_proj.weight_orig', 'encoder.layers.5.self_attn.v_proj.weight_orig', 'encoder.layers.3.fc2.weight_orig', 'encoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.5.fc2.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_orig', 'encoder.layers.3.self_attn.out_proj.weight_mask', 'encoder.layers.2.self_attn.v_proj.weight_mask', 'encoder.layers.3.self_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.1.encoder_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_mask', 'encoder.layers.0.fc1.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_mask', 'decoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.4.fc1.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_orig', 'decoder.layers.2.fc2.weight_mask', 'decoder.layers.1.fc1.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.4.fc2.weight_mask', 'encoder.layers.3.fc2.weight_mask', 'encoder.layers.5.fc1.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.1.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.fc2.weight_mask', 'decoder.layers.5.fc1.weight_mask', 'decoder.layers.0.encoder_attn.out_proj.weight_orig', 'decoder.layers.5.encoder_attn.k_proj.weight_mask', 'decoder.layers.2.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.2.fc1.weight_orig', 'encoder.layers.1.fc2.weight_orig', 'decoder.layers.4.fc1.weight_mask', 'decoder.layers.5.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.5.encoder_attn.out_proj.weight_orig', 'decoder.layers.1.encoder_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.k_proj.weight_orig', 'decoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.out_proj.weight_orig', 'decoder.layers.0.encoder_attn.q_proj.weight_orig', 'decoder.layers.5.fc1.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_orig', 'decoder.layers.5.encoder_attn.v_proj.weight_mask', 'decoder.layers.0.encoder_attn.v_proj.weight_mask', 'decoder.layers.1.fc1.weight_mask', 'decoder.layers.2.self_attn.out_proj.weight_orig', 'encoder.layers.4.fc2.weight_orig', 'decoder.layers.0.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.fc1.weight_mask', 'encoder.layers.1.self_attn.v_proj.weight_mask', 'encoder.layers.2.fc2.weight_orig', 'decoder.layers.5.self_attn.v_proj.weight_orig', 'decoder.layers.4.self_attn.out_proj.weight_orig', 'decoder.layers.4.encoder_attn.k_proj.weight_mask', 'decoder.layers.0.fc1.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_orig', 'decoder.layers.2.encoder_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.2.self_attn.out_proj.weight_mask', 'decoder.layers.5.encoder_attn.k_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_orig', 'decoder.layers.0.encoder_attn.out_proj.weight_mask', 'decoder.layers.2.self_attn.q_proj.weight_mask', 'decoder.layers.4.encoder_attn.v_proj.weight_orig', 'decoder.layers.2.encoder_attn.v_proj.weight_mask', 'decoder.layers.3.encoder_attn.q_proj.weight_orig', 'encoder.layers.2.fc2.weight_mask', 'encoder.layers.0.fc1.weight_orig', 'decoder.layers.1.self_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.self_attn.out_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.q_proj.weight_mask', 'decoder.layers.3.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.v_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_orig', 'encoder.layers.2.fc1.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.q_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.v_proj.weight_mask', 'encoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.4.self_attn.out_proj.weight_mask', 'encoder.layers.0.fc2.weight_orig', 'encoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_orig', 'decoder.layers.2.fc1.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_orig', 'decoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_orig', 'decoder.layers.5.self_attn.out_proj.weight_mask', 'decoder.layers.4.encoder_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.k_proj.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_orig', 'decoder.layers.3.self_attn.out_proj.weight_mask', 'decoder.layers.4.fc2.weight_orig', 'encoder.layers.1.fc2.weight_mask', 'encoder.layers.2.fc1.weight_orig', 'encoder.layers.5.fc2.weight_mask', 'encoder.layers.4.self_attn.k_proj.weight_mask', 'decoder.layers.4.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_mask', 'decoder.layers.0.encoder_attn.k_proj.weight_mask', 'decoder.layers.1.encoder_attn.out_proj.weight_mask', 'encoder.layers.4.self_attn.v_proj.weight_mask', 'decoder.layers.4.fc2.weight_mask', 'encoder.layers.0.fc2.weight_mask', 'encoder.layers.5.self_attn.q_proj.weight_mask', 'decoder.layers.5.fc2.weight_mask', 'encoder.layers.1.fc1.weight_mask', 'decoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.0.fc2.weight_orig', 'decoder.layers.2.fc2.weight_orig', 'encoder.layers.2.self_attn.q_proj.weight_mask', 'encoder.layers.5.self_attn.k_proj.weight_mask', 'encoder.layers.1.self_attn.out_proj.weight_orig', 'encoder.layers.5.self_attn.k_proj.weight_orig', 'decoder.layers.5.self_attn.k_proj.weight_orig', 'encoder.layers.1.self_attn.q_proj.weight_mask', 'decoder.layers.2.encoder_attn.out_proj.weight_orig', 'encoder.layers.3.self_attn.k_proj.weight_mask', 'decoder.layers.5.self_attn.v_proj.weight_mask', 'encoder.layers.0.self_attn.v_proj.weight_orig', 'decoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.5.self_attn.out_proj.weight_mask', 'encoder.layers.3.self_attn.v_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_orig', 'decoder.layers.3.fc2.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_mask', 'decoder.layers.4.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.k_proj.weight_mask', 'encoder.layers.2.self_attn.out_proj.weight_orig', 'decoder.layers.0.encoder_attn.v_proj.weight_orig', 'encoder.layers.4.fc1.weight_mask', 'encoder.layers.0.self_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.q_proj.weight_orig', 'decoder.layers.1.self_attn.k_proj.weight_mask', 'decoder.layers.0.fc2.weight_mask', 'decoder.layers.2.self_attn.v_proj.weight_orig', 'encoder.layers.5.fc1.weight_mask', 'encoder.layers.2.self_attn.k_proj.weight_mask', 'encoder.layers.1.fc1.weight_orig', 'decoder.layers.3.self_attn.q_proj.weight_orig', 'decoder.layers.1.encoder_attn.k_proj.weight_mask', 'decoder.layers.3.encoder_attn.v_proj.weight_mask', 'decoder.layers.5.encoder_attn.v_proj.weight_orig', 'decoder.layers.3.encoder_attn.v_proj.weight_orig', 'encoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.1.fc2.weight_orig', 'encoder.layers.4.self_attn.out_proj.weight_orig', 'encoder.layers.3.fc1.weight_mask', 'encoder.layers.3.fc1.weight_orig', 'decoder.layers.3.fc2.weight_orig', 'decoder.layers.2.self_attn.q_proj.weight_orig', 'encoder.layers.4.self_attn.k_proj.weight_orig', 'decoder.layers.3.encoder_attn.k_proj.weight_orig', 'encoder.layers.2.self_attn.v_proj.weight_orig', 'decoder.layers.0.self_attn.q_proj.weight_mask', 'decoder.layers.3.fc1.weight_orig', 'decoder.layers.4.encoder_attn.out_proj.weight_mask']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:26:35,633 >> Some weights of BartForSequenceClassification were not initialized from the model checkpoint at /content/models/bart-0.99 and are newly initialized: ['decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.5.fc2.weight', 'encoder.layers.2.fc1.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.fc2.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'classification_head.dense.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.fc2.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.fc1.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'classification_head.dense.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.1.fc2.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.layers.2.fc2.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'classification_head.out_proj.bias', 'encoder.layers.1.fc1.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.4.fc1.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.3.fc1.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.0.fc2.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db6c54669b369b95.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c3b3c56fffffa3f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-329abb9a77ad4f4c.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [0, 133, 7069, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 2357, 6052, 9, 2339, 24387, 1857, 136, 123, 479, 2, 2, 10653, 32326, 1033, 58, 551, 62, 19, 3659, 24387, 49, 403, 136, 1918, 1001, 6182, 2156, 2600, 10, 2357, 12, 8596, 17984, 1601, 7, 5, 461, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [0, 16764, 8224, 503, 447, 13, 5, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 8, 8556, 16, 878, 239, 789, 9, 220, 395, 128, 29, 1939, 729, 11, 997, 12, 25566, 3576, 13212, 2636, 479, 2, 2, 35779, 11, 3576, 13212, 2636, 128, 29, 3467, 12, 6996, 168, 32, 10, 7690, 1002, 13, 7369, 2156, 8, 8556, 16, 878, 239, 789, 9, 395, 128, 29, 1939, 729, 11, 5, 997, 12, 6472, 4628, 976, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [0, 35167, 359, 11253, 128, 29, 1764, 388, 1965, 5060, 1981, 204, 4, 1749, 332, 7, 361, 6361, 4, 1096, 2156, 150, 4417, 5946, 5060, 1064, 231, 4, 245, 332, 7, 112, 6, 27639, 4, 1096, 479, 2, 2, 133, 5787, 359, 11253, 128, 29, 1764, 4648, 21, 62, 112, 4, 2545, 332, 2156, 50, 321, 4, 1366, 135, 2156, 7, 361, 4718, 4, 4671, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:26:39,851 >> The following columns in the training set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:26:39,863 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:26:39,863 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:26:39,863 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:26:39,863 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:26:39,863 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:26:39,863 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:26:39,863 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:26:39,864 >>   Number of trainable parameters = 140012546\n",
            "100% 174/174 [01:10<00:00,  3.02it/s][INFO|trainer.py:1873] 2022-11-09 05:27:50,576 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 70.712, 'train_samples_per_second': 155.617, 'train_steps_per_second': 2.461, 'train_loss': 0.599645987324331, 'epoch': 3.0}\n",
            "100% 174/174 [01:10<00:00,  2.46it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:27:50,578 >> Saving model checkpoint to tmp/bart-0.99/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:27:50,579 >> Configuration saved in tmp/bart-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:27:51,841 >> Model weights saved in tmp/bart-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:27:51,842 >> tokenizer config file saved in tmp/bart-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:27:51,842 >> Special tokens file saved in tmp/bart-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5996\n",
            "  train_runtime            = 0:01:10.71\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    155.617\n",
            "  train_steps_per_second   =      2.461\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:27:51,947 >> The following columns in the evaluation set don't have a corresponding argument in `BartForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BartForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:27:51,949 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:27:51,949 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:27:51,949 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 43.65it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6789\n",
            "  eval_combined_score     =     0.7333\n",
            "  eval_f1                 =     0.7877\n",
            "  eval_loss               =     0.5858\n",
            "  eval_runtime            = 0:00:01.19\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    342.446\n",
            "  eval_steps_per_second   =     42.806\n"
          ]
        }
      ],
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bart-0.99 \\\n",
        "  --tokenizer_name \"facebook/bart-base\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bart-0.99/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dSc6PJ5lvaj",
        "outputId": "fe53c3bb-f5c0-4bf4-8bb9-8baf4d35dd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert/runs/Nov09_04-51-01_251896dc902d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 900.39it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 04:51:06,372 >> loading configuration file /content/models/bert/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:51:06,375 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 04:51:07,129 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:51:07,129 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:51:07,130 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:51:07,130 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:51:07,130 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:51:07,130 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:51:07,130 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 04:51:07,130 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:51:07,131 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 04:51:07,176 >> loading weights file /content/models/bert/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 04:51:08,374 >> Some weights of the model checkpoint at /content/models/bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 04:51:08,374 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90351dc3512198ae.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-412bbf189ac157de.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 04:51:11,838 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 04:51:11,849 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 04:51:11,849 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 04:51:11,849 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 04:51:11,849 >>   Instantaneous batch size per device = 128\n",
            "[INFO|trainer.py:1626] 2022-11-09 04:51:11,849 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "[INFO|trainer.py:1627] 2022-11-09 04:51:11,849 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 04:51:11,849 >>   Total optimization steps = 87\n",
            "[INFO|trainer.py:1630] 2022-11-09 04:51:11,850 >>   Number of trainable parameters = 109483778\n",
            "100% 87/87 [03:47<00:00,  2.38s/it][INFO|trainer.py:1873] 2022-11-09 04:54:58,871 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 227.0245, 'train_samples_per_second': 48.471, 'train_steps_per_second': 0.383, 'train_loss': 0.5164566040039062, 'epoch': 3.0}\n",
            "100% 87/87 [03:47<00:00,  2.61s/it]\n",
            "[INFO|trainer.py:2692] 2022-11-09 04:54:58,876 >> Saving model checkpoint to tmp/bert/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 04:54:58,877 >> Configuration saved in tmp/bert/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 04:54:59,906 >> Model weights saved in tmp/bert/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 04:54:59,907 >> tokenizer config file saved in tmp/bert/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 04:54:59,907 >> Special tokens file saved in tmp/bert/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5165\n",
            "  train_runtime            = 0:03:47.02\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     48.471\n",
            "  train_steps_per_second   =      0.383\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 04:54:59,943 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 04:54:59,944 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 04:54:59,944 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 04:54:59,945 >>   Batch size = 8\n",
            "100% 51/51 [00:03<00:00, 14.31it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.7868\n",
            "  eval_combined_score     =     0.8222\n",
            "  eval_f1                 =     0.8576\n",
            "  eval_loss               =     0.4789\n",
            "  eval_runtime            = 0:00:03.60\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    113.038\n",
            "  eval_steps_per_second   =      14.13\n"
          ]
        }
      ],
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert-0.1 \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert-0.1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idR1QDu8uL1O",
        "outputId": "9bcdafb3-a344-4e27-ba14-03746ba8d895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert-0.1/runs/Nov09_04-55-28_251896dc902d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert-0.1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert-0.1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 970.60it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 04:55:32,754 >> loading configuration file /content/models/bert-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:55:32,757 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 04:55:33,507 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:55:33,507 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:55:33,508 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:55:33,508 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:55:33,508 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:55:33,508 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 04:55:33,508 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 04:55:33,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 04:55:33,509 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 04:55:33,554 >> loading weights file /content/models/bert-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 04:55:35,514 >> Some weights of the model checkpoint at /content/models/bert-0.1 were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'cls.predictions.bias', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'cls.predictions.decoder.bias', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.6.output.dense.weight_mask', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_mask']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 04:55:35,514 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert-0.1 and are newly initialized: ['bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.weight', 'classifier.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90351dc3512198ae.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-412bbf189ac157de.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 04:55:39,129 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 04:55:39,140 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 04:55:39,140 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 04:55:39,141 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 04:55:39,141 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1626] 2022-11-09 04:55:39,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1627] 2022-11-09 04:55:39,141 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 04:55:39,141 >>   Total optimization steps = 174\n",
            "[INFO|trainer.py:1630] 2022-11-09 04:55:39,141 >>   Number of trainable parameters = 109483778\n",
            "100% 174/174 [03:53<00:00,  1.08s/it][INFO|trainer.py:1873] 2022-11-09 04:59:33,061 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 233.9236, 'train_samples_per_second': 47.041, 'train_steps_per_second': 0.744, 'train_loss': 0.624261044907844, 'epoch': 3.0}\n",
            "100% 174/174 [03:53<00:00,  1.34s/it]\n",
            "[INFO|trainer.py:2692] 2022-11-09 04:59:33,067 >> Saving model checkpoint to tmp/bert-0.1/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 04:59:33,068 >> Configuration saved in tmp/bert-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 04:59:34,036 >> Model weights saved in tmp/bert-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 04:59:34,037 >> tokenizer config file saved in tmp/bert-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 04:59:34,037 >> Special tokens file saved in tmp/bert-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6243\n",
            "  train_runtime            = 0:03:53.92\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     47.041\n",
            "  train_steps_per_second   =      0.744\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 04:59:34,070 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 04:59:34,072 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 04:59:34,072 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 04:59:34,072 >>   Batch size = 8\n",
            "100% 51/51 [00:03<00:00, 14.48it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.674\n",
            "  eval_combined_score     =     0.7323\n",
            "  eval_f1                 =     0.7906\n",
            "  eval_loss               =     0.6089\n",
            "  eval_runtime            = 0:00:03.56\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    114.343\n",
            "  eval_steps_per_second   =     14.293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert-0.5 \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert-0.5/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xzSxOkTuQSZ",
        "outputId": "d9ee0666-b67e-417f-f4fe-2cdbdd4e1c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert-0.5/runs/Nov09_05-11-57_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert-0.5/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert-0.5/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/glue/resolve/main/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpypr2hkhw\n",
            "Downloading builder script: 100% 28.8k/28.8k [00:00<00:00, 127kB/s] \n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/glue/resolve/main/glue.py in cache at /root/.cache/huggingface/datasets/downloads/03ff78d4e3b8dc3726741609e0bc0e266acd3efa19ccc1c835f7b0fbf8384d86.4ad6b10d4037f376902901afcaf7c8fd58cbf839548cd585189cb35d344e9e0f.py\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/03ff78d4e3b8dc3726741609e0bc0e266acd3efa19ccc1c835f7b0fbf8384d86.4ad6b10d4037f376902901afcaf7c8fd58cbf839548cd585189cb35d344e9e0f.py\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/glue/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpenzwrhq9\n",
            "Downloading metadata: 100% 28.7k/28.7k [00:00<00:00, 128kB/s] \n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/glue/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/b3da855f86bd64f2a90382d5b68f6a50a890fa9ce3ef4a702eef51fd4e479522.1e6f4f37bbc9505420d0d89bebca59a9b16e9a6cd7ab0db88628ac9a8e40a747\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/b3da855f86bd64f2a90382d5b68f6a50a890fa9ce3ef4a702eef51fd4e479522.1e6f4f37bbc9505420d0d89bebca59a9b16e9a6cd7ab0db88628ac9a8e40a747\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/glue/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpr324oe9q\n",
            "Downloading readme: 100% 27.8k/27.8k [00:00<00:00, 122kB/s] \n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/glue/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/55feec413a92e2bb3525cd297350424529de1d8325cd5eb1aca38d722ac9290a.56cf5c97d222ed79b56fe6c55c226378cd9bc10a17c89aa986a11bbe74306749\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/55feec413a92e2bb3525cd297350424529de1d8325cd5eb1aca38d722ac9290a.56cf5c97d222ed79b56fe6c55c226378cd9bc10a17c89aa986a11bbe74306749\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Generating dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mrpc to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "INFO:datasets.builder:Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]INFO:datasets.utils.file_utils:https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp5dsocud8\n",
            "\n",
            "Downloading data: 6.22kB [00:00, 4.72MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv in cache at /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/393f97e0117f7c5fc1053fba02fb070b221bc0da0eafbd6b57e8cdf7621a4e64\n",
            "Downloading data files:  33% 1/3 [00:01<00:03,  1.94s/it]INFO:datasets.utils.file_utils:https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2fu2ur44\n",
            "\n",
            "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading data: 21.6kB [00:00, 106kB/s]\u001b[A\n",
            "Downloading data: 54.4kB [00:00, 138kB/s]\u001b[A\n",
            "Downloading data: 124kB [00:00, 241kB/s] \u001b[A\n",
            "Downloading data: 298kB [00:00, 492kB/s]\u001b[A\n",
            "Downloading data: 1.05MB [00:01, 1.01MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt in cache at /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/7c6c4f66e416181b62e136ddd5834ec10afe3aac4f7a327b81ca74025ea69529\n",
            "Downloading data files:  67% 2/3 [00:04<00:02,  2.50s/it]INFO:datasets.utils.file_utils:https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpazajffxj\n",
            "\n",
            "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
            "Downloading data: 19.8kB [00:00, 91.2kB/s]\u001b[A\n",
            "Downloading data: 54.7kB [00:00, 131kB/s] \u001b[A\n",
            "Downloading data: 125kB [00:00, 220kB/s] \u001b[A\n",
            "Downloading data: 441kB [00:00, 505kB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt in cache at /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/d0f75e90c732a9847ec38471fddece4ebcaad09dd1958467e2b00c6a3cbd31a9\n",
            "Downloading data files: 100% 3/3 [00:06<00:00,  2.31s/it]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 786.14it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:12:15,853 >> loading configuration file /content/models/bert-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:12:15,856 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.5\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 26.0kB/s]\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:12:17,729 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:12:17,730 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 241kB/s]\n",
            "Downloading: 100% 466k/466k [00:01<00:00, 406kB/s]\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:12:25,508 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:12:25,508 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:12:25,508 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:12:25,508 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:12:25,508 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:12:25,509 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:12:25,509 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:12:25,559 >> loading weights file /content/models/bert-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:12:27,565 >> Some weights of the model checkpoint at /content/models/bert-0.5 were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'cls.predictions.bias', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'cls.predictions.decoder.bias', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_mask']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:12:27,565 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert-0.5 and are newly initialized: ['bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'classifier.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "Running tokenizer on dataset:  75% 3/4 [00:00<00:00,  6.54ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-226e393032315faf.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d6a0e077b7973ba5.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  6.95ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 4.35MB/s]\n",
            "[INFO|trainer.py:726] 2022-11-09 05:12:32,828 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:12:32,840 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:12:32,840 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:12:32,840 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:12:32,840 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:12:32,840 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:12:32,840 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:12:32,840 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:12:32,841 >>   Number of trainable parameters = 109483778\n",
            "100% 345/345 [01:05<00:00,  5.93it/s][INFO|trainer.py:1873] 2022-11-09 05:13:37,877 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 65.0356, 'train_samples_per_second': 169.2, 'train_steps_per_second': 5.305, 'train_loss': 0.6009463545204936, 'epoch': 3.0}\n",
            "100% 345/345 [01:05<00:00,  5.31it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:13:37,879 >> Saving model checkpoint to tmp/bert-0.5/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:13:37,880 >> Configuration saved in tmp/bert-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:13:38,800 >> Model weights saved in tmp/bert-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:13:38,801 >> tokenizer config file saved in tmp/bert-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:13:38,801 >> Special tokens file saved in tmp/bert-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6009\n",
            "  train_runtime            = 0:01:05.03\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =      169.2\n",
            "  train_steps_per_second   =      5.305\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:13:38,842 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:13:38,844 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:13:38,844 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:13:38,844 >>   Batch size = 8\n",
            "100% 51/51 [00:00<00:00, 56.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6667\n",
            "  eval_combined_score     =     0.7226\n",
            "  eval_f1                 =     0.7785\n",
            "  eval_loss               =      0.599\n",
            "  eval_runtime            = 0:00:00.94\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    432.268\n",
            "  eval_steps_per_second   =     54.034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert-0.9 \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert-0.9/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjHoxR9ZuRH7",
        "outputId": "8a21a173-8609-41aa-dbd5-a7ffd9094b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert-0.9/runs/Nov09_05-13-45_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert-0.9/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert-0.9/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 782.76it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:13:51,131 >> loading configuration file /content/models/bert-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:13:51,134 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.9\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:13:52,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:13:52,111 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:13:52,111 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:13:52,111 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:13:52,112 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:13:52,112 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:13:52,112 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:13:52,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:13:52,113 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:13:52,161 >> loading weights file /content/models/bert-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:13:54,190 >> Some weights of the model checkpoint at /content/models/bert-0.9 were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'cls.predictions.bias', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.3.output.dense.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'cls.predictions.decoder.bias', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_orig']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:13:54,190 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert-0.9 and are newly initialized: ['bert.encoder.layer.4.attention.self.query.weight', 'classifier.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'classifier.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90351dc3512198ae.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d6a0e077b7973ba5.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:13:58,126 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:13:58,138 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:13:58,138 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:13:58,138 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:13:58,138 >>   Instantaneous batch size per device = 128\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:13:58,138 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:13:58,138 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:13:58,138 >>   Total optimization steps = 87\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:13:58,139 >>   Number of trainable parameters = 109483778\n",
            "100% 87/87 [00:54<00:00,  1.79it/s][INFO|trainer.py:1873] 2022-11-09 05:14:52,666 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 54.5271, 'train_samples_per_second': 201.808, 'train_steps_per_second': 1.596, 'train_loss': 0.6362354234717358, 'epoch': 3.0}\n",
            "100% 87/87 [00:54<00:00,  1.60it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:14:52,668 >> Saving model checkpoint to tmp/bert-0.9/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:14:52,669 >> Configuration saved in tmp/bert-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:14:53,577 >> Model weights saved in tmp/bert-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:14:53,578 >> tokenizer config file saved in tmp/bert-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:14:53,578 >> Special tokens file saved in tmp/bert-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6362\n",
            "  train_runtime            = 0:00:54.52\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    201.808\n",
            "  train_steps_per_second   =      1.596\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:14:53,617 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:14:53,619 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:14:53,619 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:14:53,619 >>   Batch size = 8\n",
            "100% 51/51 [00:00<00:00, 56.24it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6233\n",
            "  eval_runtime            = 0:00:00.92\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    441.295\n",
            "  eval_steps_per_second   =     55.162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert-0.95 \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert-0.95/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov77vqfGuXRh",
        "outputId": "fae8a83b-aa02-40d3-8a50-fbe6a6ed3e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert-0.95/runs/Nov09_05-15-00_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert-0.95/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert-0.95/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 780.92it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:15:06,977 >> loading configuration file /content/models/bert-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:15:06,980 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.95\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:15:07,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:15:07,918 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:15:07,919 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:15:07,919 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:15:07,919 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:15:07,919 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:15:07,919 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:15:07,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:15:07,920 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:15:07,968 >> loading weights file /content/models/bert-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:15:09,966 >> Some weights of the model checkpoint at /content/models/bert-0.95 were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'cls.predictions.bias', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.3.output.dense.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'bert.encoder.layer.6.output.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'cls.predictions.decoder.bias', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:15:09,966 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert-0.95 and are newly initialized: ['bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'classifier.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90351dc3512198ae.arrow\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-412bbf189ac157de.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  5.09ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:15:14,107 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:15:14,119 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:15:14,119 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:15:14,119 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:15:14,119 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:15:14,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:15:14,119 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:15:14,119 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:15:14,120 >>   Number of trainable parameters = 109483778\n",
            "100% 345/345 [01:05<00:00,  5.94it/s][INFO|trainer.py:1873] 2022-11-09 05:16:19,370 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 65.2498, 'train_samples_per_second': 168.644, 'train_steps_per_second': 5.287, 'train_loss': 0.6009463545204936, 'epoch': 3.0}\n",
            "100% 345/345 [01:05<00:00,  5.29it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:16:19,372 >> Saving model checkpoint to tmp/bert-0.95/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:16:19,373 >> Configuration saved in tmp/bert-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:16:20,204 >> Model weights saved in tmp/bert-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:16:20,204 >> tokenizer config file saved in tmp/bert-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:16:20,205 >> Special tokens file saved in tmp/bert-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6009\n",
            "  train_runtime            = 0:01:05.24\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    168.644\n",
            "  train_steps_per_second   =      5.287\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:16:20,241 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:16:20,243 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:16:20,244 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:16:20,244 >>   Batch size = 8\n",
            "100% 51/51 [00:00<00:00, 56.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6667\n",
            "  eval_combined_score     =     0.7226\n",
            "  eval_f1                 =     0.7785\n",
            "  eval_loss               =      0.599\n",
            "  eval_runtime            = 0:00:00.91\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    444.461\n",
            "  eval_steps_per_second   =     55.558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/bert-0.99 \\\n",
        "  --tokenizer_name \"bert-base-uncased\" \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/bert-0.99/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUcpvyafuYcv",
        "outputId": "a8ab4c81-68b4-4383-cd94-dc4288446a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/bert-0.99/runs/Nov09_05-16-26_ef7be8754f60,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/bert-0.99/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/bert-0.99/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 799.83it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 05:16:32,370 >> loading configuration file /content/models/bert-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:16:32,373 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/bert-0.99\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:16:33,302 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:16:33,303 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:16:33,304 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:16:33,304 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:16:33,304 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:16:33,304 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-09 05:16:33,304 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-09 05:16:33,305 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 05:16:33,305 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 05:16:33,354 >> loading weights file /content/models/bert-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 05:16:35,389 >> Some weights of the model checkpoint at /content/models/bert-0.99 were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.4.attention.self.key.weight_mask', 'bert.encoder.layer.7.output.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_mask', 'bert.encoder.layer.9.intermediate.dense.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.self.key.weight_mask', 'bert.encoder.layer.4.output.dense.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_orig', 'bert.encoder.layer.4.attention.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_mask', 'bert.encoder.layer.4.intermediate.dense.weight_mask', 'bert.encoder.layer.3.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.6.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.self.query.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_mask', 'bert.encoder.layer.9.output.dense.weight_orig', 'bert.encoder.layer.8.attention.self.key.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_mask', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.7.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.query.weight_orig', 'bert.encoder.layer.5.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_mask', 'cls.predictions.decoder.weight_mask', 'bert.encoder.layer.5.intermediate.dense.weight_mask', 'bert.encoder.layer.6.output.dense.weight_orig', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.6.attention.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_orig', 'bert.encoder.layer.4.attention.self.query.weight_mask', 'bert.encoder.layer.6.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_orig', 'bert.encoder.layer.6.attention.self.value.weight_mask', 'bert.encoder.layer.5.output.dense.weight_mask', 'bert.encoder.layer.4.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_orig', 'bert.encoder.layer.11.attention.self.query.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.3.attention.self.query.weight_mask', 'bert.encoder.layer.4.output.dense.weight_orig', 'bert.encoder.layer.9.output.dense.weight_mask', 'cls.predictions.transform.dense.weight_orig', 'bert.encoder.layer.7.attention.self.value.weight_mask', 'bert.encoder.layer.7.attention.output.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.10.attention.self.query.weight_orig', 'bert.encoder.layer.6.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'bert.encoder.layer.5.attention.output.dense.weight_orig', 'bert.encoder.layer.2.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'cls.predictions.transform.dense.weight_mask', 'bert.encoder.layer.2.attention.output.dense.weight_mask', 'bert.encoder.layer.4.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.key.weight_orig', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.3.intermediate.dense.weight_mask', 'bert.encoder.layer.8.output.dense.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_orig', 'bert.encoder.layer.4.intermediate.dense.weight_orig', 'bert.encoder.layer.10.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_orig', 'bert.encoder.layer.11.attention.self.value.weight_orig', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_mask', 'bert.encoder.layer.2.output.dense.weight_orig', 'bert.encoder.layer.8.attention.output.dense.weight_mask', 'bert.encoder.layer.10.intermediate.dense.weight_mask', 'bert.encoder.layer.4.attention.self.value.weight_mask', 'cls.predictions.bias', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.9.attention.self.value.weight_mask', 'bert.encoder.layer.11.intermediate.dense.weight_mask', 'cls.predictions.decoder.bias', 'bert.encoder.layer.10.attention.self.key.weight_orig', 'bert.encoder.layer.4.attention.self.key.weight_orig', 'bert.encoder.layer.3.attention.self.query.weight_orig', 'bert.encoder.layer.7.attention.self.query.weight_mask', 'bert.encoder.layer.2.attention.self.key.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_orig', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.7.output.dense.weight_orig', 'bert.encoder.layer.10.attention.self.value.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_orig', 'bert.encoder.layer.8.attention.self.query.weight_mask', 'bert.encoder.layer.11.output.dense.weight_mask', 'bert.encoder.layer.8.intermediate.dense.weight_mask', 'bert.encoder.layer.3.attention.self.key.weight_orig', 'bert.encoder.layer.2.output.dense.weight_mask', 'bert.encoder.layer.11.attention.self.value.weight_mask', 'bert.encoder.layer.7.intermediate.dense.weight_orig', 'bert.encoder.layer.10.intermediate.dense.weight_orig', 'bert.encoder.layer.5.output.dense.weight_orig', 'bert.encoder.layer.9.attention.self.key.weight_orig', 'bert.encoder.layer.9.intermediate.dense.weight_mask', 'bert.encoder.layer.11.output.dense.weight_orig', 'bert.encoder.layer.7.attention.self.key.weight_mask', 'bert.encoder.layer.9.attention.output.dense.weight_mask', 'cls.predictions.decoder.weight_orig', 'bert.encoder.layer.5.attention.self.query.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_mask', 'bert.encoder.layer.5.attention.self.value.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_orig', 'bert.encoder.layer.3.attention.output.dense.weight_orig', 'bert.encoder.layer.3.output.dense.weight_orig', 'bert.encoder.layer.5.intermediate.dense.weight_orig', 'bert.encoder.layer.5.attention.self.value.weight_orig', 'bert.encoder.layer.10.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'bert.encoder.layer.6.attention.self.value.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.2.attention.self.value.weight_orig', 'bert.encoder.layer.11.attention.self.key.weight_mask', 'bert.encoder.layer.10.attention.self.key.weight_mask', 'bert.encoder.layer.3.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.8.attention.self.value.weight_mask', 'bert.encoder.layer.11.attention.output.dense.weight_orig', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.10.attention.output.dense.weight_orig', 'bert.encoder.layer.5.attention.self.key.weight_mask', 'bert.encoder.layer.10.attention.self.query.weight_mask', 'bert.encoder.layer.2.intermediate.dense.weight_mask', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.2.attention.self.query.weight_orig', 'bert.encoder.layer.7.intermediate.dense.weight_mask', 'bert.encoder.layer.8.output.dense.weight_orig', 'bert.encoder.layer.8.intermediate.dense.weight_orig', 'bert.encoder.layer.11.attention.output.dense.weight_mask', 'bert.encoder.layer.1.attention.self.key.weight_mask']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 05:16:35,389 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/models/bert-0.99 and are newly initialized: ['bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'classifier.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ad404bae13e47401.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-90351dc3512198ae.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-412bbf189ac157de.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 05:16:39,416 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 05:16:39,428 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 05:16:39,428 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 05:16:39,428 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 05:16:39,428 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 05:16:39,428 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 05:16:39,428 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 05:16:39,428 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 05:16:39,429 >>   Number of trainable parameters = 109483778\n",
            "100% 345/345 [01:05<00:00,  5.95it/s][INFO|trainer.py:1873] 2022-11-09 05:17:44,521 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 65.0917, 'train_samples_per_second': 169.054, 'train_steps_per_second': 5.3, 'train_loss': 0.6009463545204936, 'epoch': 3.0}\n",
            "100% 345/345 [01:05<00:00,  5.30it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 05:17:44,523 >> Saving model checkpoint to tmp/bert-0.99/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 05:17:44,524 >> Configuration saved in tmp/bert-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 05:17:45,414 >> Model weights saved in tmp/bert-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 05:17:45,415 >> tokenizer config file saved in tmp/bert-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 05:17:45,415 >> Special tokens file saved in tmp/bert-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6009\n",
            "  train_runtime            = 0:01:05.09\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    169.054\n",
            "  train_steps_per_second   =        5.3\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 05:17:45,454 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 05:17:45,456 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 05:17:45,456 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 05:17:45,456 >>   Batch size = 8\n",
            "100% 51/51 [00:00<00:00, 56.55it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6667\n",
            "  eval_combined_score     =     0.7226\n",
            "  eval_f1                 =     0.7785\n",
            "  eval_loss               =      0.599\n",
            "  eval_runtime            = 0:00:00.91\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    443.623\n",
            "  eval_steps_per_second   =     55.453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt2 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR-qucZFtEl_",
        "outputId": "216d3aa6-4a85-499e-b41d-bb657660bb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt2/runs/Nov09_06-03-16_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt2/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt2/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 780.38it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:03:22,694 >> loading configuration file /content/models/gpt2/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:03:22,697 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,698 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,699 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,699 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,699 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,699 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:03:22,699 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:03:22,791 >> loading weights file /content/models/gpt2/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:03:25,475 >> Some weights of the model checkpoint at /content/models/gpt2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:03:25,476 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b1fe3ae3f93c4e4f.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-38e0a2fe492a7867.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 06:03:29,397 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:03:29,408 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:03:29,408 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:03:29,408 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:03:29,408 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:03:29,408 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:03:29,408 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:03:29,408 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:03:29,409 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:07<00:00,  5.60it/s][INFO|trainer.py:1873] 2022-11-09 06:04:37,290 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 67.8812, 'train_samples_per_second': 162.107, 'train_steps_per_second': 5.082, 'train_loss': 0.612242237035779, 'epoch': 3.0}\n",
            "100% 345/345 [01:07<00:00,  5.08it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:04:37,292 >> Saving model checkpoint to tmp/gpt2/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:04:37,293 >> Configuration saved in tmp/gpt2/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:04:38,459 >> Model weights saved in tmp/gpt2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:04:38,460 >> tokenizer config file saved in tmp/gpt2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:04:38,460 >> Special tokens file saved in tmp/gpt2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6122\n",
            "  train_runtime            = 0:01:07.88\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    162.107\n",
            "  train_steps_per_second   =      5.082\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:04:38,573 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:04:38,575 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:04:38,575 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:04:38,575 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 48.84it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.777\n",
            "  eval_combined_score     =      0.814\n",
            "  eval_f1                 =     0.8511\n",
            "  eval_loss               =     0.4894\n",
            "  eval_runtime            = 0:00:01.06\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    383.125\n",
            "  eval_steps_per_second   =     47.891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#50257\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt-2-0.1 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt-2-0.1/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-0ZFCbT78H6",
        "outputId": "41c117b4-5b5b-4f69-ea0b-f344af019c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt-2-0.1/runs/Nov09_06-01-43_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt-2-0.1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt-2-0.1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 763.76it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:01:49,818 >> loading configuration file /content/models/gpt-2-0.1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:01:49,822 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.1\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:01:49,823 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:01:49,917 >> loading weights file /content/models/gpt-2-0.1/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:01:52,068 >> Some weights of the model checkpoint at /content/models/gpt-2-0.1 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'lm_head.weight', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:01:52,068 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt-2-0.1 and are newly initialized: ['transformer.h.9.attn.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.8.mlp.c_fc.weight', 'score.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "Running tokenizer on dataset:  75% 3/4 [00:00<00:00, 11.37ba/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1f51a60bd7614e38.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-38e0a2fe492a7867.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  8.61ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 4.27MB/s]\n",
            "[INFO|trainer.py:726] 2022-11-09 06:01:59,123 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:01:59,134 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:01:59,135 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:01:59,135 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:01:59,135 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:01:59,135 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:01:59,135 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:01:59,135 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:01:59,135 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:08<00:00,  5.75it/s][INFO|trainer.py:1873] 2022-11-09 06:03:08,093 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 68.9573, 'train_samples_per_second': 159.577, 'train_steps_per_second': 5.003, 'train_loss': 0.6400960728742074, 'epoch': 3.0}\n",
            "100% 345/345 [01:08<00:00,  5.00it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:03:08,095 >> Saving model checkpoint to tmp/gpt-2-0.1/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:03:08,096 >> Configuration saved in tmp/gpt-2-0.1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:03:09,213 >> Model weights saved in tmp/gpt-2-0.1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:03:09,214 >> tokenizer config file saved in tmp/gpt-2-0.1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:03:09,214 >> Special tokens file saved in tmp/gpt-2-0.1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6401\n",
            "  train_runtime            = 0:01:08.95\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    159.577\n",
            "  train_steps_per_second   =      5.003\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:03:09,318 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:03:09,320 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:03:09,320 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:03:09,320 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 50.47it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6036\n",
            "  eval_runtime            = 0:00:01.03\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    395.751\n",
            "  eval_steps_per_second   =     49.469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt-2-0.5 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt-2-0.5/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv_ujIu6-36c",
        "outputId": "4142ab70-8bbd-4f7d-9ab4-64551d00b9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt-2-0.5/runs/Nov09_06-04-54_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt-2-0.5/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt-2-0.5/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 752.70it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:05:00,296 >> loading configuration file /content/models/gpt-2-0.5/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:05:00,299 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:05:00,300 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:05:00,395 >> loading weights file /content/models/gpt-2-0.5/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:05:02,620 >> Some weights of the model checkpoint at /content/models/gpt-2-0.5 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'lm_head.weight', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_fc.weight_mask']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:05:02,620 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt-2-0.5 and are newly initialized: ['transformer.h.1.mlp.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b1fe3ae3f93c4e4f.arrow\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1506a218f10f0920.arrow\n",
            "Running tokenizer on dataset:  50% 1/2 [00:00<00:00,  6.96ba/s]\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 06:05:06,772 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:05:06,783 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:05:06,784 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:05:06,784 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:05:06,784 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:05:06,784 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:05:06,784 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:05:06,784 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:05:06,784 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:07<00:00,  5.72it/s][INFO|trainer.py:1873] 2022-11-09 06:06:14,556 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 67.7716, 'train_samples_per_second': 162.369, 'train_steps_per_second': 5.091, 'train_loss': 0.6400960728742074, 'epoch': 3.0}\n",
            "100% 345/345 [01:07<00:00,  5.09it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:06:14,558 >> Saving model checkpoint to tmp/gpt-2-0.5/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:06:14,559 >> Configuration saved in tmp/gpt-2-0.5/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:06:15,676 >> Model weights saved in tmp/gpt-2-0.5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:06:15,676 >> tokenizer config file saved in tmp/gpt-2-0.5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:06:15,677 >> Special tokens file saved in tmp/gpt-2-0.5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6401\n",
            "  train_runtime            = 0:01:07.77\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    162.369\n",
            "  train_steps_per_second   =      5.091\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:06:15,778 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:06:15,780 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:06:15,780 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:06:15,780 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 49.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6036\n",
            "  eval_runtime            = 0:00:01.05\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    385.134\n",
            "  eval_steps_per_second   =     48.142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt-2-0.9 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt-2-0.9/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MmanCxT--rV",
        "outputId": "b294b5e5-2005-4de4-bebd-eab68932d352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt-2-0.9/runs/Nov09_06-06-22_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt-2-0.9/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt-2-0.9/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 880.29it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:06:28,014 >> loading configuration file /content/models/gpt-2-0.9/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:06:28,018 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.9\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:06:28,019 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:06:28,109 >> loading weights file /content/models/gpt-2-0.9/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:06:30,280 >> Some weights of the model checkpoint at /content/models/gpt-2-0.9 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.0.attn.c_attn.weight_mask', 'lm_head.weight', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:06:30,280 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt-2-0.9 and are newly initialized: ['transformer.h.10.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'score.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b1fe3ae3f93c4e4f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1506a218f10f0920.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 06:06:34,199 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:06:34,210 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:06:34,210 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:06:34,210 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:06:34,210 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:06:34,210 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:06:34,210 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:06:34,210 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:06:34,211 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:07<00:00,  5.70it/s][INFO|trainer.py:1873] 2022-11-09 06:07:42,037 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 67.8267, 'train_samples_per_second': 162.237, 'train_steps_per_second': 5.086, 'train_loss': 0.6400960728742074, 'epoch': 3.0}\n",
            "100% 345/345 [01:07<00:00,  5.09it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:07:42,040 >> Saving model checkpoint to tmp/gpt-2-0.9/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:07:42,041 >> Configuration saved in tmp/gpt-2-0.9/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:07:43,176 >> Model weights saved in tmp/gpt-2-0.9/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:07:43,177 >> tokenizer config file saved in tmp/gpt-2-0.9/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:07:43,177 >> Special tokens file saved in tmp/gpt-2-0.9/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6401\n",
            "  train_runtime            = 0:01:07.82\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    162.237\n",
            "  train_steps_per_second   =      5.086\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:07:43,282 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:07:43,284 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:07:43,284 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:07:43,284 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 48.85it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6036\n",
            "  eval_runtime            = 0:00:01.06\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    383.289\n",
            "  eval_steps_per_second   =     47.911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt-2-0.95 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt-2-0.95/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_0ILtCl_dqY",
        "outputId": "a10c9371-75d0-4e60-b807-fd04c84e6500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt-2-0.95/runs/Nov09_06-08-10_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt-2-0.95/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt-2-0.95/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 800.19it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:08:16,206 >> loading configuration file /content/models/gpt-2-0.95/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:08:16,209 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.95\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:08:16,211 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:08:16,306 >> loading weights file /content/models/gpt-2-0.95/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:08:18,483 >> Some weights of the model checkpoint at /content/models/gpt-2-0.95 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.8.mlp.c_proj.weight_orig', 'lm_head.weight', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_orig']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:08:18,483 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt-2-0.95 and are newly initialized: ['transformer.h.8.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'score.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.4.attn.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b1fe3ae3f93c4e4f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1506a218f10f0920.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 06:08:22,347 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:08:22,358 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:08:22,358 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:08:22,358 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:08:22,358 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:08:22,358 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:08:22,358 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:08:22,358 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:08:22,359 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:07<00:00,  5.74it/s][INFO|trainer.py:1873] 2022-11-09 06:09:29,986 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 67.6274, 'train_samples_per_second': 162.715, 'train_steps_per_second': 5.101, 'train_loss': 0.6400960728742074, 'epoch': 3.0}\n",
            "100% 345/345 [01:07<00:00,  5.10it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:09:29,988 >> Saving model checkpoint to tmp/gpt-2-0.95/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:09:29,989 >> Configuration saved in tmp/gpt-2-0.95/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:09:31,105 >> Model weights saved in tmp/gpt-2-0.95/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:09:31,105 >> tokenizer config file saved in tmp/gpt-2-0.95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:09:31,106 >> Special tokens file saved in tmp/gpt-2-0.95/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6401\n",
            "  train_runtime            = 0:01:07.62\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =    162.715\n",
            "  train_steps_per_second   =      5.101\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:09:31,210 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:09:31,212 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:09:31,212 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:09:31,212 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 48.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6036\n",
            "  eval_runtime            = 0:00:01.07\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    380.583\n",
            "  eval_steps_per_second   =     47.573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python run_glue.py \\\n",
        "  --model_name_or_path /content/models/gpt-2-0.99 \\\n",
        "  --tokenizer_name /content/models/gpt2-tokenizer \\\n",
        "  --task_name mrpc \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir tmp/gpt-2-0.99/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E2kfh-c_fb5",
        "outputId": "5a5b644b-118d-46cb-8586-667888950cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tmp/gpt-2-0.99/runs/Nov09_06-09-37_41c4cd82e816,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=tmp/gpt-2-0.99/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=tmp/gpt-2-0.99/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.info:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 863.80it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-09 06:09:43,482 >> loading configuration file /content/models/gpt-2-0.99/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-09 06:09:43,485 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/content/models/gpt-2-0.99\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-09 06:09:43,487 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-09 06:09:43,577 >> loading weights file /content/models/gpt-2-0.99/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2600] 2022-11-09 06:09:45,737 >> Some weights of the model checkpoint at /content/models/gpt-2-0.99 were not used when initializing GPT2ForSequenceClassification: ['transformer.h.10.mlp.c_fc.weight_mask', 'transformer.h.10.mlp.c_proj.weight_orig', 'transformer.h.1.attn.c_attn.weight_mask', 'transformer.h.2.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_orig', 'transformer.h.0.attn.c_proj.weight_mask', 'transformer.h.4.attn.c_attn.weight_mask', 'transformer.h.1.attn.c_attn.weight_orig', 'transformer.h.6.attn.c_proj.weight_orig', 'transformer.h.1.attn.c_proj.weight_orig', 'transformer.h.11.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_orig', 'transformer.h.8.mlp.c_fc.weight_mask', 'transformer.h.2.mlp.c_proj.weight_orig', 'transformer.h.11.mlp.c_proj.weight_mask', 'transformer.h.10.attn.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_mask', 'transformer.h.4.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_fc.weight_mask', 'transformer.h.7.attn.c_proj.weight_orig', 'transformer.h.3.attn.c_attn.weight_mask', 'transformer.h.8.mlp.c_proj.weight_mask', 'transformer.h.11.attn.c_proj.weight_orig', 'transformer.h.11.mlp.c_fc.weight_mask', 'transformer.h.6.mlp.c_fc.weight_mask', 'transformer.h.5.attn.c_proj.weight_orig', 'transformer.h.2.attn.c_attn.weight_orig', 'transformer.h.10.attn.c_proj.weight_orig', 'transformer.h.1.mlp.c_fc.weight_orig', 'transformer.h.0.mlp.c_fc.weight_mask', 'transformer.h.3.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_orig', 'transformer.h.11.mlp.c_proj.weight_orig', 'transformer.h.2.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_mask', 'transformer.h.4.attn.c_proj.weight_mask', 'transformer.h.1.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_mask', 'transformer.h.10.mlp.c_proj.weight_mask', 'transformer.h.6.attn.c_attn.weight_orig', 'transformer.h.5.attn.c_proj.weight_mask', 'transformer.h.1.mlp.c_proj.weight_mask', 'transformer.h.0.attn.c_attn.weight_orig', 'transformer.h.11.mlp.c_fc.weight_orig', 'transformer.h.9.attn.c_attn.weight_orig', 'transformer.h.8.attn.c_proj.weight_mask', 'transformer.h.9.attn.c_attn.weight_mask', 'transformer.h.3.mlp.c_fc.weight_mask', 'transformer.h.5.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_orig', 'transformer.h.2.mlp.c_fc.weight_mask', 'transformer.h.8.mlp.c_fc.weight_orig', 'transformer.h.3.mlp.c_fc.weight_orig', 'transformer.h.10.mlp.c_fc.weight_orig', 'lm_head.weight', 'transformer.h.2.mlp.c_proj.weight_mask', 'transformer.h.7.attn.c_proj.weight_mask', 'transformer.h.7.mlp.c_fc.weight_mask', 'transformer.h.10.attn.c_attn.weight_mask', 'transformer.h.0.mlp.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_mask', 'transformer.h.6.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_fc.weight_orig', 'transformer.h.3.attn.c_proj.weight_orig', 'transformer.h.5.mlp.c_proj.weight_orig', 'transformer.h.4.attn.c_attn.weight_orig', 'transformer.h.9.mlp.c_fc.weight_orig', 'transformer.h.0.attn.c_attn.weight_mask', 'transformer.h.4.mlp.c_proj.weight_orig', 'transformer.h.5.mlp.c_fc.weight_orig', 'transformer.h.7.attn.c_attn.weight_mask', 'transformer.h.6.attn.c_proj.weight_mask', 'transformer.h.6.mlp.c_proj.weight_orig', 'transformer.h.0.mlp.c_fc.weight_orig', 'transformer.h.1.mlp.c_fc.weight_mask', 'transformer.h.9.attn.c_proj.weight_mask', 'transformer.h.0.mlp.c_proj.weight_mask', 'transformer.h.7.mlp.c_proj.weight_mask', 'transformer.h.3.mlp.c_proj.weight_mask', 'transformer.h.5.attn.c_attn.weight_orig', 'transformer.h.4.mlp.c_fc.weight_mask', 'transformer.h.4.mlp.c_proj.weight_mask', 'transformer.h.8.attn.c_attn.weight_mask', 'transformer.h.3.attn.c_attn.weight_orig', 'transformer.h.7.attn.c_attn.weight_orig', 'transformer.h.3.mlp.c_proj.weight_orig', 'transformer.h.11.attn.c_attn.weight_mask', 'transformer.h.8.attn.c_proj.weight_orig', 'transformer.h.10.attn.c_attn.weight_orig', 'transformer.h.2.attn.c_attn.weight_mask', 'transformer.h.2.attn.c_proj.weight_orig', 'transformer.h.8.mlp.c_proj.weight_orig', 'transformer.h.0.attn.c_proj.weight_orig', 'transformer.h.9.mlp.c_proj.weight_orig', 'transformer.h.6.mlp.c_fc.weight_orig']\n",
            "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2612] 2022-11-09 06:09:45,737 >> Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /content/models/gpt-2-0.99 and are newly initialized: ['transformer.h.10.attn.c_proj.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.4.attn.c_proj.weight', 'score.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f34f0d0e13d9e479.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b1fe3ae3f93c4e4f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1506a218f10f0920.arrow\n",
            "INFO:__main__:Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [464, 13106, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 4747, 5468, 286, 4963, 36995, 7636, 1028, 683, 764, 2964, 2707, 654, 547, 2077, 510, 351, 11947, 36995, 511, 1339, 1028, 1703, 305, 17027, 837, 3555, 257, 4747, 12, 7700, 25984, 3850, 284, 262, 2184, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [7376, 6607, 2828, 1762, 329, 262, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 290, 12097, 318, 2491, 1029, 4058, 286, 1306, 3502, 705, 82, 4787, 3071, 287, 1175, 12, 45910, 2580, 1349, 3972, 764, 25883, 287, 2580, 1349, 3972, 705, 82, 9070, 12, 17078, 1230, 389, 257, 10792, 2496, 329, 12669, 837, 290, 12097, 318, 2491, 1029, 4058, 286, 3502, 705, 82, 4787, 3071, 287, 262, 1175, 12, 4108, 1886, 3814, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [23615, 1222, 23676, 705, 82, 5323, 4283, 6376, 25650, 7392, 604, 13, 1821, 2173, 284, 860, 5999, 13, 1120, 837, 981, 22767, 48539, 25650, 3214, 718, 13, 20, 2173, 284, 352, 11, 22136, 13, 1120, 764, 464, 8997, 1222, 23676, 705, 82, 5323, 12901, 373, 510, 352, 13, 2425, 2173, 837, 393, 657, 13, 1507, 1411, 837, 284, 860, 3324, 13, 3104, 764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-09 06:09:49,644 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1622] 2022-11-09 06:09:49,655 >> ***** Running training *****\n",
            "[INFO|trainer.py:1623] 2022-11-09 06:09:49,655 >>   Num examples = 3668\n",
            "[INFO|trainer.py:1624] 2022-11-09 06:09:49,655 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1625] 2022-11-09 06:09:49,655 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1626] 2022-11-09 06:09:49,655 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1627] 2022-11-09 06:09:49,655 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1628] 2022-11-09 06:09:49,655 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:1630] 2022-11-09 06:09:49,655 >>   Number of trainable parameters = 124441344\n",
            "100% 345/345 [01:07<00:00,  5.73it/s][INFO|trainer.py:1873] 2022-11-09 06:10:57,173 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 67.5174, 'train_samples_per_second': 162.98, 'train_steps_per_second': 5.11, 'train_loss': 0.6400960728742074, 'epoch': 3.0}\n",
            "100% 345/345 [01:07<00:00,  5.11it/s]\n",
            "[INFO|trainer.py:2692] 2022-11-09 06:10:57,175 >> Saving model checkpoint to tmp/gpt-2-0.99/\n",
            "[INFO|configuration_utils.py:447] 2022-11-09 06:10:57,176 >> Configuration saved in tmp/gpt-2-0.99/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-09 06:10:58,303 >> Model weights saved in tmp/gpt-2-0.99/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-09 06:10:58,304 >> tokenizer config file saved in tmp/gpt-2-0.99/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-09 06:10:58,304 >> Special tokens file saved in tmp/gpt-2-0.99/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6401\n",
            "  train_runtime            = 0:01:07.51\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     162.98\n",
            "  train_steps_per_second   =       5.11\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-09 06:10:58,412 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2943] 2022-11-09 06:10:58,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2945] 2022-11-09 06:10:58,414 >>   Num examples = 408\n",
            "[INFO|trainer.py:2948] 2022-11-09 06:10:58,414 >>   Batch size = 8\n",
            "100% 51/51 [00:01<00:00, 49.62it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6838\n",
            "  eval_combined_score     =      0.748\n",
            "  eval_f1                 =     0.8122\n",
            "  eval_loss               =     0.6036\n",
            "  eval_runtime            = 0:00:01.04\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =    389.274\n",
            "  eval_steps_per_second   =     48.659\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c733a3ee0d0447b6aafbb68bf188cebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61fa1f80f90242079689838e4a334b90",
              "IPY_MODEL_142b7c89aa7b45ecba885d95d29e5612",
              "IPY_MODEL_17c46008314e4f379ae293ead10248a4"
            ],
            "layout": "IPY_MODEL_babe0ba1d3224e8eba317b3737d9f886"
          }
        },
        "61fa1f80f90242079689838e4a334b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1851fb59d6f4ccaa0d8d7f51ca4b043",
            "placeholder": "​",
            "style": "IPY_MODEL_ec9d472931294b43a1a9aca055f7e093",
            "value": "Downloading: 100%"
          }
        },
        "142b7c89aa7b45ecba885d95d29e5612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17ccb7f663c1451daf6203775d7ec859",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e80217cb56c460eb78911da58809896",
            "value": 665
          }
        },
        "17c46008314e4f379ae293ead10248a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a2cc723b864d9d8ab715e663ce8f1a",
            "placeholder": "​",
            "style": "IPY_MODEL_8217f118a8364c6f927ce6983809c35b",
            "value": " 665/665 [00:00&lt;00:00, 23.9kB/s]"
          }
        },
        "babe0ba1d3224e8eba317b3737d9f886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1851fb59d6f4ccaa0d8d7f51ca4b043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9d472931294b43a1a9aca055f7e093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17ccb7f663c1451daf6203775d7ec859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e80217cb56c460eb78911da58809896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19a2cc723b864d9d8ab715e663ce8f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8217f118a8364c6f927ce6983809c35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80bc60933f3b4884b66155aecb924fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68925db5119b4597ae6f488f28dcc1e1",
              "IPY_MODEL_8666b7b4104042748b580e1f8f25544c",
              "IPY_MODEL_fec785c9b7e74082a53f3ebe77a3ce1c"
            ],
            "layout": "IPY_MODEL_5396319d1c98460c9b37d76c9e1e0ef0"
          }
        },
        "68925db5119b4597ae6f488f28dcc1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64356aeb50e442e2956e2a09e57e0906",
            "placeholder": "​",
            "style": "IPY_MODEL_02e4b938ec304577831b6ce7f2f1c7c1",
            "value": "Downloading: 100%"
          }
        },
        "8666b7b4104042748b580e1f8f25544c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e8fca200bd3456ca7281d0d06462669",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae2eb252d005457eaaff09383269fcc1",
            "value": 1042301
          }
        },
        "fec785c9b7e74082a53f3ebe77a3ce1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034f0fefab3247a7801ff930e7e74c0e",
            "placeholder": "​",
            "style": "IPY_MODEL_234ebcbdcf8643339538d30d5e5d3a73",
            "value": " 1.04M/1.04M [00:01&lt;00:00, 985kB/s]"
          }
        },
        "5396319d1c98460c9b37d76c9e1e0ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64356aeb50e442e2956e2a09e57e0906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e4b938ec304577831b6ce7f2f1c7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e8fca200bd3456ca7281d0d06462669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2eb252d005457eaaff09383269fcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "034f0fefab3247a7801ff930e7e74c0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234ebcbdcf8643339538d30d5e5d3a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a24a8694e6741fca1b76e7aa3138a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_debc664fdabd4cb6a4c664c80eaa80cd",
              "IPY_MODEL_b89b035d5507480bbba0a208ba15d6d1",
              "IPY_MODEL_1ae9b0658c38486187b7d6211afbb355"
            ],
            "layout": "IPY_MODEL_1f13f85de2224c57b23a1383f8cfb7d5"
          }
        },
        "debc664fdabd4cb6a4c664c80eaa80cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87db0caa7e4746139f537d16e3179ac6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0b744d75a2242faa4d413884fc7395a",
            "value": "Downloading: 100%"
          }
        },
        "b89b035d5507480bbba0a208ba15d6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f472dda6becb45618bb2df1e64124de7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c47b8cd32e014a4086ffa1f4ef1fe7f1",
            "value": 456318
          }
        },
        "1ae9b0658c38486187b7d6211afbb355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d433d17ba034293b804b673cee21c13",
            "placeholder": "​",
            "style": "IPY_MODEL_48a911a833c0481a8aba876e5b2af8e7",
            "value": " 456k/456k [00:01&lt;00:00, 484kB/s]"
          }
        },
        "1f13f85de2224c57b23a1383f8cfb7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87db0caa7e4746139f537d16e3179ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0b744d75a2242faa4d413884fc7395a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f472dda6becb45618bb2df1e64124de7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c47b8cd32e014a4086ffa1f4ef1fe7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d433d17ba034293b804b673cee21c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a911a833c0481a8aba876e5b2af8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51afc871129c4b35a86d17e1e89a7f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bcb7cb4cd7a404bb8b646288680aec5",
              "IPY_MODEL_d2f5f9848bb1401aa830bf8b156e9ea5",
              "IPY_MODEL_e8f5d8b51ddd4cc2919380551066158d"
            ],
            "layout": "IPY_MODEL_3d5516da3e93476ea31f065c9d3353b3"
          }
        },
        "4bcb7cb4cd7a404bb8b646288680aec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_964080f00c0f4b2a89c45fab38afc192",
            "placeholder": "​",
            "style": "IPY_MODEL_5767de72a0f94bd0b3f42260616b0e55",
            "value": "Downloading: 100%"
          }
        },
        "d2f5f9848bb1401aa830bf8b156e9ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0e35bbfac44d74af77997f55dbd69d",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c01803dcf8b4767a6b7c023286a7333",
            "value": 1355256
          }
        },
        "e8f5d8b51ddd4cc2919380551066158d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d09bbfd552534eceaf3a6c039d1b462b",
            "placeholder": "​",
            "style": "IPY_MODEL_958427faadc34474b684abf79cb97b07",
            "value": " 1.36M/1.36M [00:01&lt;00:00, 1.01MB/s]"
          }
        },
        "3d5516da3e93476ea31f065c9d3353b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "964080f00c0f4b2a89c45fab38afc192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5767de72a0f94bd0b3f42260616b0e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0e35bbfac44d74af77997f55dbd69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c01803dcf8b4767a6b7c023286a7333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d09bbfd552534eceaf3a6c039d1b462b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958427faadc34474b684abf79cb97b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11de0efc60af4b83bfda904de7779922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b5b9ad285e14d60a4d3f94e2882ee10",
              "IPY_MODEL_e47c98427c1b41378934d90b55db6d34",
              "IPY_MODEL_80f8eeb5644b4ae39ea9ffe119bdb1d1"
            ],
            "layout": "IPY_MODEL_d998acaf91cb4814b9c505ee8bbe7271"
          }
        },
        "0b5b9ad285e14d60a4d3f94e2882ee10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_377611ef050a4c71a97d3c2eff18f8a7",
            "placeholder": "​",
            "style": "IPY_MODEL_ac21d11d105749a7a33743ad6ea4cd08",
            "value": "Downloading: 100%"
          }
        },
        "e47c98427c1b41378934d90b55db6d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c640405f1de452882d261b5e4f05d25",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f30d5733275488588f41981e8d2a3a7",
            "value": 570
          }
        },
        "80f8eeb5644b4ae39ea9ffe119bdb1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d46cc180894bc493653b50949da153",
            "placeholder": "​",
            "style": "IPY_MODEL_60e8a7296bb243f0812ebc8fe01c31ea",
            "value": " 570/570 [00:00&lt;00:00, 18.9kB/s]"
          }
        },
        "d998acaf91cb4814b9c505ee8bbe7271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "377611ef050a4c71a97d3c2eff18f8a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac21d11d105749a7a33743ad6ea4cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c640405f1de452882d261b5e4f05d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f30d5733275488588f41981e8d2a3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d46cc180894bc493653b50949da153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e8a7296bb243f0812ebc8fe01c31ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d23c295c159b4e5fa0a408b69866f202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2937569da6448af9d7f899f794e1d9a",
              "IPY_MODEL_111b5a60de2b4e6eb08c2f82d2e22565",
              "IPY_MODEL_5eb6ade1d2404cb2ad480b0860302091"
            ],
            "layout": "IPY_MODEL_1f9bd36e8dc04038b83824479db305f8"
          }
        },
        "e2937569da6448af9d7f899f794e1d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bc1116e46a432f9aac2569c1c046de",
            "placeholder": "​",
            "style": "IPY_MODEL_734fd7fd1ca8436f801e2e241d64d947",
            "value": "Downloading: 100%"
          }
        },
        "111b5a60de2b4e6eb08c2f82d2e22565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d17785e81247068066e1150809bf6d",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77646cd36f034f74a01925dfdd0c29e3",
            "value": 440473133
          }
        },
        "5eb6ade1d2404cb2ad480b0860302091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ced300b4fb4379b37a1eb7b45f00ab",
            "placeholder": "​",
            "style": "IPY_MODEL_24f5141e70e04316ac417ee4692b24d9",
            "value": " 440M/440M [00:07&lt;00:00, 63.6MB/s]"
          }
        },
        "1f9bd36e8dc04038b83824479db305f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bc1116e46a432f9aac2569c1c046de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734fd7fd1ca8436f801e2e241d64d947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63d17785e81247068066e1150809bf6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77646cd36f034f74a01925dfdd0c29e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45ced300b4fb4379b37a1eb7b45f00ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f5141e70e04316ac417ee4692b24d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291ab20d40904f53922440c6d59d1763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e1ee79253074a66ae0225c46602805f",
              "IPY_MODEL_b57851a40b6a4b6a992770973cfee1ce",
              "IPY_MODEL_3add678ddab34a01a3efa5d199cc8ceb"
            ],
            "layout": "IPY_MODEL_6a833b668c784a08aed2fd7f84e98a01"
          }
        },
        "0e1ee79253074a66ae0225c46602805f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7746a3fc10c446e8e728265313af63d",
            "placeholder": "​",
            "style": "IPY_MODEL_1fb95a199b99407780ad0d5dfc3c43c6",
            "value": "Downloading: 100%"
          }
        },
        "b57851a40b6a4b6a992770973cfee1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581ef67b704b47a99ca65acf3c48b97c",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7808f4c441214a3bbd9c79581ba65f29",
            "value": 548118077
          }
        },
        "3add678ddab34a01a3efa5d199cc8ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1412e213c5dc4717ae3cf51068089077",
            "placeholder": "​",
            "style": "IPY_MODEL_34ebcf3f72a3490293bfa12ed270a0bc",
            "value": " 548M/548M [00:08&lt;00:00, 60.3MB/s]"
          }
        },
        "6a833b668c784a08aed2fd7f84e98a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7746a3fc10c446e8e728265313af63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb95a199b99407780ad0d5dfc3c43c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581ef67b704b47a99ca65acf3c48b97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7808f4c441214a3bbd9c79581ba65f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1412e213c5dc4717ae3cf51068089077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ebcf3f72a3490293bfa12ed270a0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a7644deae44cdab8048df0811431f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ac39cdea4054d1ab66f9deba0826a5f",
              "IPY_MODEL_f50d3fccd7cb4da38cc57dc2e7701a58",
              "IPY_MODEL_a6ab345e8d414ed183f4bef624a1eeb2"
            ],
            "layout": "IPY_MODEL_daa343a441c5460bae71f1a2b05587de"
          }
        },
        "4ac39cdea4054d1ab66f9deba0826a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fa10e0622242828f93e170eae34f6c",
            "placeholder": "​",
            "style": "IPY_MODEL_0ab68eef02124b518efdc05dc4efeda1",
            "value": "Downloading: 100%"
          }
        },
        "f50d3fccd7cb4da38cc57dc2e7701a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89723cbdc4134d33aaa7b4b0205baebc",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f91fa000fa242028b664380e89d138d",
            "value": 1716
          }
        },
        "a6ab345e8d414ed183f4bef624a1eeb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9fe88ed2514b0a8f9b87d2b3810bff",
            "placeholder": "​",
            "style": "IPY_MODEL_bc71a8b306e64ceaa1d29042882b223e",
            "value": " 1.72k/1.72k [00:00&lt;00:00, 58.1kB/s]"
          }
        },
        "daa343a441c5460bae71f1a2b05587de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fa10e0622242828f93e170eae34f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab68eef02124b518efdc05dc4efeda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89723cbdc4134d33aaa7b4b0205baebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f91fa000fa242028b664380e89d138d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a9fe88ed2514b0a8f9b87d2b3810bff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc71a8b306e64ceaa1d29042882b223e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd8e21af53c84135b76beff614819442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0d2e62226ba4403a745225e3f2637cb",
              "IPY_MODEL_1ac85a8d95cf419cbc0de21ed882c303",
              "IPY_MODEL_0bd41b1d6cd14d818a1eab5bcad44e36"
            ],
            "layout": "IPY_MODEL_e4d969f3d2de4cefb83b87ee78541bb5"
          }
        },
        "e0d2e62226ba4403a745225e3f2637cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6225fed41874333adbcf8ae8b5d1a07",
            "placeholder": "​",
            "style": "IPY_MODEL_95d59897ddb44aaaa55817908489e2b7",
            "value": "Downloading: 100%"
          }
        },
        "1ac85a8d95cf419cbc0de21ed882c303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac409454e0f5436882adcd50905ae418",
            "max": 557771387,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd7baf5254884ddc872e81462c409717",
            "value": 557771387
          }
        },
        "0bd41b1d6cd14d818a1eab5bcad44e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44ee232d312048a99cb112626b5674c3",
            "placeholder": "​",
            "style": "IPY_MODEL_2cfc71304cab4821a7028910fe229231",
            "value": " 558M/558M [00:09&lt;00:00, 59.0MB/s]"
          }
        },
        "e4d969f3d2de4cefb83b87ee78541bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6225fed41874333adbcf8ae8b5d1a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95d59897ddb44aaaa55817908489e2b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac409454e0f5436882adcd50905ae418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7baf5254884ddc872e81462c409717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44ee232d312048a99cb112626b5674c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cfc71304cab4821a7028910fe229231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}